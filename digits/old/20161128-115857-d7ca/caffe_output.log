salloc: Granted job allocation 3875172
srun: Job step created
I1128 11:58:59.418944   778 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/digits/jobs/20161128-115857-d7ca/solver.prototxt
I1128 11:58:59.419363   778 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1128 11:58:59.419373   778 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1128 11:58:59.419539   778 caffe.cpp:217] Using GPUs 0
I1128 11:58:59.464521   778 caffe.cpp:222] GPU 0: Tesla K20m
I1128 11:58:59.840899   778 solver.cpp:48] Initializing solver from parameters:
test_iter: 469
test_interval: 704
base_lr: 0.01
display: 79
max_iter: 21120
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 6970
snapshot: 704
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
net: "train_val.prototxt"
train_state {
level: 0
stage: ""
}
type: "SGD"
I1128 11:58:59.841630   778 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1128 11:58:59.842667   778 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val-data
I1128 11:58:59.842705   778 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1128 11:58:59.842726   778 net.cpp:58] Initializing net from parameters:
state {
phase: TRAIN
level: 0
stage: ""
}
layer {
name: "train-data"
type: "Data"
top: "data"
top: "label"
include {
phase: TRAIN
}
transform_param {
mean_file: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto"
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/train_db"
batch_size: 64
backend: LMDB
}
}
layer {
name: "scale"
type: "Power"
bottom: "data"
top: "scaled"
power_param {
scale: 0.0125
}
}
layer {
name: "conv1"
type: "Convolution"
bottom: "scaled"
top: "conv1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 20
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool1"
type: "Pooling"
bottom: "conv1"
top: "pool1"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "conv2"
type: "Convolution"
bottom: "pool1"
top: "conv2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 50
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool2"
type: "Pooling"
bottom: "conv2"
top: "pool2"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "ip1"
type: "InnerProduct"
bottom: "pool2"
top: "ip1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 500
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "relu1"
type: "ReLU"
bottom: "ip1"
top: "ip1"
}
layer {
name: "ip2"
type: "InnerProduct"
bottom: "ip1"
top: "ip2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 10
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "loss"
type: "SoftmaxWithLoss"
bottom: "ip2"
bottom: "label"
top: "loss"
}
I1128 11:58:59.843142   778 layer_factory.hpp:77] Creating layer train-data
I1128 11:58:59.843900   778 net.cpp:100] Creating Layer train-data
I1128 11:58:59.843921   778 net.cpp:408] train-data -> data
I1128 11:58:59.843973   778 net.cpp:408] train-data -> label
I1128 11:58:59.844009   778 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto
I1128 11:58:59.846243   783 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/train_db
I1128 11:58:59.863873   778 data_layer.cpp:41] output data size: 64,3,28,28
I1128 11:58:59.868369   778 net.cpp:150] Setting up train-data
I1128 11:58:59.868440   778 net.cpp:157] Top shape: 64 3 28 28 (150528)
I1128 11:58:59.868464   778 net.cpp:157] Top shape: 64 (64)
I1128 11:58:59.868472   778 net.cpp:165] Memory required for data: 602368
I1128 11:58:59.868496   778 layer_factory.hpp:77] Creating layer scale
I1128 11:58:59.868515   778 net.cpp:100] Creating Layer scale
I1128 11:58:59.868523   778 net.cpp:434] scale <- data
I1128 11:58:59.868542   778 net.cpp:408] scale -> scaled
I1128 11:58:59.868615   778 net.cpp:150] Setting up scale
I1128 11:58:59.868631   778 net.cpp:157] Top shape: 64 3 28 28 (150528)
I1128 11:58:59.868646   778 net.cpp:165] Memory required for data: 1204480
I1128 11:58:59.868654   778 layer_factory.hpp:77] Creating layer conv1
I1128 11:58:59.868693   778 net.cpp:100] Creating Layer conv1
I1128 11:58:59.868700   778 net.cpp:434] conv1 <- scaled
I1128 11:58:59.868710   778 net.cpp:408] conv1 -> conv1
I1128 11:59:00.128161   778 net.cpp:150] Setting up conv1
I1128 11:59:00.128202   778 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1128 11:59:00.128221   778 net.cpp:165] Memory required for data: 4153600
I1128 11:59:00.128273   778 layer_factory.hpp:77] Creating layer pool1
I1128 11:59:00.128303   778 net.cpp:100] Creating Layer pool1
I1128 11:59:00.128317   778 net.cpp:434] pool1 <- conv1
I1128 11:59:00.128332   778 net.cpp:408] pool1 -> pool1
I1128 11:59:00.128442   778 net.cpp:150] Setting up pool1
I1128 11:59:00.128450   778 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1128 11:59:00.128458   778 net.cpp:165] Memory required for data: 4890880
I1128 11:59:00.128461   778 layer_factory.hpp:77] Creating layer conv2
I1128 11:59:00.128506   778 net.cpp:100] Creating Layer conv2
I1128 11:59:00.128514   778 net.cpp:434] conv2 <- pool1
I1128 11:59:00.128525   778 net.cpp:408] conv2 -> conv2
I1128 11:59:00.131714   778 net.cpp:150] Setting up conv2
I1128 11:59:00.131731   778 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1128 11:59:00.131742   778 net.cpp:165] Memory required for data: 5710080
I1128 11:59:00.131764   778 layer_factory.hpp:77] Creating layer pool2
I1128 11:59:00.131778   778 net.cpp:100] Creating Layer pool2
I1128 11:59:00.131784   778 net.cpp:434] pool2 <- conv2
I1128 11:59:00.131794   778 net.cpp:408] pool2 -> pool2
I1128 11:59:00.131868   778 net.cpp:150] Setting up pool2
I1128 11:59:00.131880   778 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1128 11:59:00.131888   778 net.cpp:165] Memory required for data: 5914880
I1128 11:59:00.131896   778 layer_factory.hpp:77] Creating layer ip1
I1128 11:59:00.131916   778 net.cpp:100] Creating Layer ip1
I1128 11:59:00.131927   778 net.cpp:434] ip1 <- pool2
I1128 11:59:00.131937   778 net.cpp:408] ip1 -> ip1
I1128 11:59:00.137606   778 net.cpp:150] Setting up ip1
I1128 11:59:00.137622   778 net.cpp:157] Top shape: 64 500 (32000)
I1128 11:59:00.137634   778 net.cpp:165] Memory required for data: 6042880
I1128 11:59:00.137650   778 layer_factory.hpp:77] Creating layer relu1
I1128 11:59:00.137670   778 net.cpp:100] Creating Layer relu1
I1128 11:59:00.137676   778 net.cpp:434] relu1 <- ip1
I1128 11:59:00.137686   778 net.cpp:395] relu1 -> ip1 (in-place)
I1128 11:59:00.138005   778 net.cpp:150] Setting up relu1
I1128 11:59:00.138018   778 net.cpp:157] Top shape: 64 500 (32000)
I1128 11:59:00.138027   778 net.cpp:165] Memory required for data: 6170880
I1128 11:59:00.138034   778 layer_factory.hpp:77] Creating layer ip2
I1128 11:59:00.138051   778 net.cpp:100] Creating Layer ip2
I1128 11:59:00.138065   778 net.cpp:434] ip2 <- ip1
I1128 11:59:00.138078   778 net.cpp:408] ip2 -> ip2
I1128 11:59:00.139308   778 net.cpp:150] Setting up ip2
I1128 11:59:00.139323   778 net.cpp:157] Top shape: 64 10 (640)
I1128 11:59:00.139333   778 net.cpp:165] Memory required for data: 6173440
I1128 11:59:00.139345   778 layer_factory.hpp:77] Creating layer loss
I1128 11:59:00.139360   778 net.cpp:100] Creating Layer loss
I1128 11:59:00.139367   778 net.cpp:434] loss <- ip2
I1128 11:59:00.139374   778 net.cpp:434] loss <- label
I1128 11:59:00.139385   778 net.cpp:408] loss -> loss
I1128 11:59:00.139406   778 layer_factory.hpp:77] Creating layer loss
I1128 11:59:00.140138   778 net.cpp:150] Setting up loss
I1128 11:59:00.140154   778 net.cpp:157] Top shape: (1)
I1128 11:59:00.140167   778 net.cpp:160]     with loss weight 1
I1128 11:59:00.140208   778 net.cpp:165] Memory required for data: 6173444
I1128 11:59:00.140218   778 net.cpp:226] loss needs backward computation.
I1128 11:59:00.140226   778 net.cpp:226] ip2 needs backward computation.
I1128 11:59:00.140231   778 net.cpp:226] relu1 needs backward computation.
I1128 11:59:00.140238   778 net.cpp:226] ip1 needs backward computation.
I1128 11:59:00.140242   778 net.cpp:226] pool2 needs backward computation.
I1128 11:59:00.140247   778 net.cpp:226] conv2 needs backward computation.
I1128 11:59:00.140254   778 net.cpp:226] pool1 needs backward computation.
I1128 11:59:00.140259   778 net.cpp:226] conv1 needs backward computation.
I1128 11:59:00.140264   778 net.cpp:228] scale does not need backward computation.
I1128 11:59:00.140269   778 net.cpp:228] train-data does not need backward computation.
I1128 11:59:00.140275   778 net.cpp:270] This network produces output loss
I1128 11:59:00.140288   778 net.cpp:283] Network initialization done.
I1128 11:59:00.141068   778 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1128 11:59:00.141124   778 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train-data
I1128 11:59:00.141149   778 net.cpp:58] Initializing net from parameters:
state {
phase: TEST
}
layer {
name: "val-data"
type: "Data"
top: "data"
top: "label"
include {
phase: TEST
}
transform_param {
mean_file: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto"
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/val_db"
batch_size: 32
backend: LMDB
}
}
layer {
name: "scale"
type: "Power"
bottom: "data"
top: "scaled"
power_param {
scale: 0.0125
}
}
layer {
name: "conv1"
type: "Convolution"
bottom: "scaled"
top: "conv1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 20
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool1"
type: "Pooling"
bottom: "conv1"
top: "pool1"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "conv2"
type: "Convolution"
bottom: "pool1"
top: "conv2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 50
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool2"
type: "Pooling"
bottom: "conv2"
top: "pool2"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "ip1"
type: "InnerProduct"
bottom: "pool2"
top: "ip1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 500
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "relu1"
type: "ReLU"
bottom: "ip1"
top: "ip1"
}
layer {
name: "ip2"
type: "InnerProduct"
bottom: "ip1"
top: "ip2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 10
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "accuracy"
type: "Accuracy"
bottom: "ip2"
bottom: "label"
top: "accuracy"
include {
phase: TEST
}
}
layer {
name: "loss"
type: "SoftmaxWithLoss"
bottom: "ip2"
bottom: "label"
top: "loss"
}
I1128 11:59:00.141523   778 layer_factory.hpp:77] Creating layer val-data
I1128 11:59:00.141669   778 net.cpp:100] Creating Layer val-data
I1128 11:59:00.141685   778 net.cpp:408] val-data -> data
I1128 11:59:00.141700   778 net.cpp:408] val-data -> label
I1128 11:59:00.141715   778 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto
I1128 11:59:00.162559   785 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/val_db
I1128 11:59:00.181670   778 data_layer.cpp:41] output data size: 32,3,28,28
I1128 11:59:00.182487   778 net.cpp:150] Setting up val-data
I1128 11:59:00.182502   778 net.cpp:157] Top shape: 32 3 28 28 (75264)
I1128 11:59:00.182514   778 net.cpp:157] Top shape: 32 (32)
I1128 11:59:00.182523   778 net.cpp:165] Memory required for data: 301184
I1128 11:59:00.182529   778 layer_factory.hpp:77] Creating layer label_val-data_1_split
I1128 11:59:00.182557   778 net.cpp:100] Creating Layer label_val-data_1_split
I1128 11:59:00.182564   778 net.cpp:434] label_val-data_1_split <- label
I1128 11:59:00.182579   778 net.cpp:408] label_val-data_1_split -> label_val-data_1_split_0
I1128 11:59:00.182595   778 net.cpp:408] label_val-data_1_split -> label_val-data_1_split_1
I1128 11:59:00.182703   778 net.cpp:150] Setting up label_val-data_1_split
I1128 11:59:00.182715   778 net.cpp:157] Top shape: 32 (32)
I1128 11:59:00.182730   778 net.cpp:157] Top shape: 32 (32)
I1128 11:59:00.182737   778 net.cpp:165] Memory required for data: 301440
I1128 11:59:00.182744   778 layer_factory.hpp:77] Creating layer scale
I1128 11:59:00.182754   778 net.cpp:100] Creating Layer scale
I1128 11:59:00.182760   778 net.cpp:434] scale <- data
I1128 11:59:00.182768   778 net.cpp:408] scale -> scaled
I1128 11:59:00.182922   778 net.cpp:150] Setting up scale
I1128 11:59:00.182934   778 net.cpp:157] Top shape: 32 3 28 28 (75264)
I1128 11:59:00.182943   778 net.cpp:165] Memory required for data: 602496
I1128 11:59:00.182950   778 layer_factory.hpp:77] Creating layer conv1
I1128 11:59:00.182971   778 net.cpp:100] Creating Layer conv1
I1128 11:59:00.182984   778 net.cpp:434] conv1 <- scaled
I1128 11:59:00.182996   778 net.cpp:408] conv1 -> conv1
I1128 11:59:00.184926   778 net.cpp:150] Setting up conv1
I1128 11:59:00.184944   778 net.cpp:157] Top shape: 32 20 24 24 (368640)
I1128 11:59:00.184955   778 net.cpp:165] Memory required for data: 2077056
I1128 11:59:00.184974   778 layer_factory.hpp:77] Creating layer pool1
I1128 11:59:00.184991   778 net.cpp:100] Creating Layer pool1
I1128 11:59:00.184998   778 net.cpp:434] pool1 <- conv1
I1128 11:59:00.185011   778 net.cpp:408] pool1 -> pool1
I1128 11:59:00.185091   778 net.cpp:150] Setting up pool1
I1128 11:59:00.185101   778 net.cpp:157] Top shape: 32 20 12 12 (92160)
I1128 11:59:00.185111   778 net.cpp:165] Memory required for data: 2445696
I1128 11:59:00.185117   778 layer_factory.hpp:77] Creating layer conv2
I1128 11:59:00.185135   778 net.cpp:100] Creating Layer conv2
I1128 11:59:00.185142   778 net.cpp:434] conv2 <- pool1
I1128 11:59:00.185154   778 net.cpp:408] conv2 -> conv2
I1128 11:59:00.187300   778 net.cpp:150] Setting up conv2
I1128 11:59:00.187316   778 net.cpp:157] Top shape: 32 50 8 8 (102400)
I1128 11:59:00.187327   778 net.cpp:165] Memory required for data: 2855296
I1128 11:59:00.187347   778 layer_factory.hpp:77] Creating layer pool2
I1128 11:59:00.187360   778 net.cpp:100] Creating Layer pool2
I1128 11:59:00.187366   778 net.cpp:434] pool2 <- conv2
I1128 11:59:00.187376   778 net.cpp:408] pool2 -> pool2
I1128 11:59:00.187468   778 net.cpp:150] Setting up pool2
I1128 11:59:00.187484   778 net.cpp:157] Top shape: 32 50 4 4 (25600)
I1128 11:59:00.187494   778 net.cpp:165] Memory required for data: 2957696
I1128 11:59:00.187501   778 layer_factory.hpp:77] Creating layer ip1
I1128 11:59:00.187516   778 net.cpp:100] Creating Layer ip1
I1128 11:59:00.188468   778 net.cpp:434] ip1 <- pool2
I1128 11:59:00.188494   778 net.cpp:408] ip1 -> ip1
I1128 11:59:00.195096   778 net.cpp:150] Setting up ip1
I1128 11:59:00.195114   778 net.cpp:157] Top shape: 32 500 (16000)
I1128 11:59:00.195125   778 net.cpp:165] Memory required for data: 3021696
I1128 11:59:00.195143   778 layer_factory.hpp:77] Creating layer relu1
I1128 11:59:00.195157   778 net.cpp:100] Creating Layer relu1
I1128 11:59:00.195163   778 net.cpp:434] relu1 <- ip1
I1128 11:59:00.195201   778 net.cpp:395] relu1 -> ip1 (in-place)
I1128 11:59:00.195524   778 net.cpp:150] Setting up relu1
I1128 11:59:00.195538   778 net.cpp:157] Top shape: 32 500 (16000)
I1128 11:59:00.195547   778 net.cpp:165] Memory required for data: 3085696
I1128 11:59:00.195554   778 layer_factory.hpp:77] Creating layer ip2
I1128 11:59:00.195570   778 net.cpp:100] Creating Layer ip2
I1128 11:59:00.195576   778 net.cpp:434] ip2 <- ip1
I1128 11:59:00.195586   778 net.cpp:408] ip2 -> ip2
I1128 11:59:00.195827   778 net.cpp:150] Setting up ip2
I1128 11:59:00.195839   778 net.cpp:157] Top shape: 32 10 (320)
I1128 11:59:00.195848   778 net.cpp:165] Memory required for data: 3086976
I1128 11:59:00.195865   778 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I1128 11:59:00.195875   778 net.cpp:100] Creating Layer ip2_ip2_0_split
I1128 11:59:00.195880   778 net.cpp:434] ip2_ip2_0_split <- ip2
I1128 11:59:00.195893   778 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1128 11:59:00.195904   778 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1128 11:59:00.195969   778 net.cpp:150] Setting up ip2_ip2_0_split
I1128 11:59:00.195979   778 net.cpp:157] Top shape: 32 10 (320)
I1128 11:59:00.195988   778 net.cpp:157] Top shape: 32 10 (320)
I1128 11:59:00.195994   778 net.cpp:165] Memory required for data: 3089536
I1128 11:59:00.196000   778 layer_factory.hpp:77] Creating layer accuracy
I1128 11:59:00.196017   778 net.cpp:100] Creating Layer accuracy
I1128 11:59:00.196022   778 net.cpp:434] accuracy <- ip2_ip2_0_split_0
I1128 11:59:00.196029   778 net.cpp:434] accuracy <- label_val-data_1_split_0
I1128 11:59:00.196043   778 net.cpp:408] accuracy -> accuracy
I1128 11:59:00.196058   778 net.cpp:150] Setting up accuracy
I1128 11:59:00.196069   778 net.cpp:157] Top shape: (1)
I1128 11:59:00.196076   778 net.cpp:165] Memory required for data: 3089540
I1128 11:59:00.196082   778 layer_factory.hpp:77] Creating layer loss
I1128 11:59:00.196091   778 net.cpp:100] Creating Layer loss
I1128 11:59:00.196096   778 net.cpp:434] loss <- ip2_ip2_0_split_1
I1128 11:59:00.196104   778 net.cpp:434] loss <- label_val-data_1_split_1
I1128 11:59:00.196115   778 net.cpp:408] loss -> loss
I1128 11:59:00.196127   778 layer_factory.hpp:77] Creating layer loss
I1128 11:59:00.196801   778 net.cpp:150] Setting up loss
I1128 11:59:00.196821   778 net.cpp:157] Top shape: (1)
I1128 11:59:00.196830   778 net.cpp:160]     with loss weight 1
I1128 11:59:00.196841   778 net.cpp:165] Memory required for data: 3089544
I1128 11:59:00.196848   778 net.cpp:226] loss needs backward computation.
I1128 11:59:00.196856   778 net.cpp:228] accuracy does not need backward computation.
I1128 11:59:00.196861   778 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1128 11:59:00.196866   778 net.cpp:226] ip2 needs backward computation.
I1128 11:59:00.196872   778 net.cpp:226] relu1 needs backward computation.
I1128 11:59:00.196877   778 net.cpp:226] ip1 needs backward computation.
I1128 11:59:00.196882   778 net.cpp:226] pool2 needs backward computation.
I1128 11:59:00.196887   778 net.cpp:226] conv2 needs backward computation.
I1128 11:59:00.196892   778 net.cpp:226] pool1 needs backward computation.
I1128 11:59:00.196897   778 net.cpp:226] conv1 needs backward computation.
I1128 11:59:00.196903   778 net.cpp:228] scale does not need backward computation.
I1128 11:59:00.196908   778 net.cpp:228] label_val-data_1_split does not need backward computation.
I1128 11:59:00.196915   778 net.cpp:228] val-data does not need backward computation.
I1128 11:59:00.196919   778 net.cpp:270] This network produces output accuracy
I1128 11:59:00.196925   778 net.cpp:270] This network produces output loss
I1128 11:59:00.196949   778 net.cpp:283] Network initialization done.
I1128 11:59:00.197031   778 solver.cpp:60] Solver scaffolding done.
I1128 11:59:00.197477   778 caffe.cpp:251] Starting Optimization
I1128 11:59:00.197491   778 solver.cpp:279] Solving
I1128 11:59:00.197496   778 solver.cpp:280] Learning Rate Policy: step
I1128 11:59:00.198590   778 solver.cpp:337] Iteration 0, Testing net (#0)
I1128 11:59:00.198628   778 net.cpp:693] Ignoring source layer train-data
I1128 11:59:00.208411   778 blocking_queue.cpp:50] Data layer prefetch queue empty
I1128 11:59:01.098253   778 solver.cpp:404]     Test net output #0: accuracy = 0.0702958
I1128 11:59:01.098294   778 solver.cpp:404]     Test net output #1: loss = 2.84675 (* 1 = 2.84675 loss)
I1128 11:59:01.103574   778 solver.cpp:228] Iteration 0, loss = 2.82056
I1128 11:59:01.103606   778 solver.cpp:244]     Train net output #0: loss = 2.82056 (* 1 = 2.82056 loss)
I1128 11:59:01.103657   778 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1128 11:59:01.428133   778 solver.cpp:228] Iteration 79, loss = 0.373732
I1128 11:59:01.428179   778 solver.cpp:244]     Train net output #0: loss = 0.373731 (* 1 = 0.373731 loss)
I1128 11:59:01.428200   778 sgd_solver.cpp:106] Iteration 79, lr = 0.01
I1128 11:59:01.750669   778 solver.cpp:228] Iteration 158, loss = 0.308313
I1128 11:59:01.750700   778 solver.cpp:244]     Train net output #0: loss = 0.308313 (* 1 = 0.308313 loss)
I1128 11:59:01.750712   778 sgd_solver.cpp:106] Iteration 158, lr = 0.01
I1128 11:59:02.073081   778 solver.cpp:228] Iteration 237, loss = 0.228539
I1128 11:59:02.073110   778 solver.cpp:244]     Train net output #0: loss = 0.228539 (* 1 = 0.228539 loss)
I1128 11:59:02.073122   778 sgd_solver.cpp:106] Iteration 237, lr = 0.01
I1128 11:59:02.395992   778 solver.cpp:228] Iteration 316, loss = 0.299355
I1128 11:59:02.396019   778 solver.cpp:244]     Train net output #0: loss = 0.299355 (* 1 = 0.299355 loss)
I1128 11:59:02.396031   778 sgd_solver.cpp:106] Iteration 316, lr = 0.01
