salloc: Granted job allocation 3875173
srun: Job step created
I1128 11:59:22.825548  1760 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/digits/jobs/20161128-115920-aa49/solver.prototxt
I1128 11:59:22.825949  1760 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1128 11:59:22.825959  1760 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1128 11:59:22.826122  1760 caffe.cpp:217] Using GPUs 0
I1128 11:59:22.874331  1760 caffe.cpp:222] GPU 0: Tesla K20m
I1128 11:59:23.302692  1760 solver.cpp:48] Initializing solver from parameters:
test_iter: 469
test_interval: 704
base_lr: 0.01
display: 79
max_iter: 21120
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 6970
snapshot: 704
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
net: "train_val.prototxt"
train_state {
level: 0
stage: ""
}
type: "SGD"
I1128 11:59:23.303473  1760 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1128 11:59:23.304471  1760 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val-data
I1128 11:59:23.304510  1760 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1128 11:59:23.304533  1760 net.cpp:58] Initializing net from parameters:
state {
phase: TRAIN
level: 0
stage: ""
}
layer {
name: "train-data"
type: "Data"
top: "data"
top: "label"
include {
phase: TRAIN
}
transform_param {
mean_file: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto"
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/train_db"
batch_size: 64
backend: LMDB
}
}
layer {
name: "scale"
type: "Power"
bottom: "data"
top: "scaled"
power_param {
scale: 0.0125
}
}
layer {
name: "conv1"
type: "Convolution"
bottom: "scaled"
top: "conv1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 20
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool1"
type: "Pooling"
bottom: "conv1"
top: "pool1"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "conv2"
type: "Convolution"
bottom: "pool1"
top: "conv2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 50
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool2"
type: "Pooling"
bottom: "conv2"
top: "pool2"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "ip1"
type: "InnerProduct"
bottom: "pool2"
top: "ip1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 500
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "relu1"
type: "ReLU"
bottom: "ip1"
top: "ip1"
}
layer {
name: "ip2"
type: "InnerProduct"
bottom: "ip1"
top: "ip2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 10
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "loss"
type: "SoftmaxWithLoss"
bottom: "ip2"
bottom: "label"
top: "loss"
}
I1128 11:59:23.304949  1760 layer_factory.hpp:77] Creating layer train-data
I1128 11:59:23.305713  1760 net.cpp:100] Creating Layer train-data
I1128 11:59:23.305733  1760 net.cpp:408] train-data -> data
I1128 11:59:23.305780  1760 net.cpp:408] train-data -> label
I1128 11:59:23.305894  1760 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto
I1128 11:59:23.308715  1765 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/train_db
I1128 11:59:23.331614  1760 data_layer.cpp:41] output data size: 64,3,28,28
I1128 11:59:23.336110  1760 net.cpp:150] Setting up train-data
I1128 11:59:23.336194  1760 net.cpp:157] Top shape: 64 3 28 28 (150528)
I1128 11:59:23.336215  1760 net.cpp:157] Top shape: 64 (64)
I1128 11:59:23.336223  1760 net.cpp:165] Memory required for data: 602368
I1128 11:59:23.336258  1760 layer_factory.hpp:77] Creating layer scale
I1128 11:59:23.336288  1760 net.cpp:100] Creating Layer scale
I1128 11:59:23.336297  1760 net.cpp:434] scale <- data
I1128 11:59:23.336311  1760 net.cpp:408] scale -> scaled
I1128 11:59:23.336407  1760 net.cpp:150] Setting up scale
I1128 11:59:23.336419  1760 net.cpp:157] Top shape: 64 3 28 28 (150528)
I1128 11:59:23.336434  1760 net.cpp:165] Memory required for data: 1204480
I1128 11:59:23.336441  1760 layer_factory.hpp:77] Creating layer conv1
I1128 11:59:23.336472  1760 net.cpp:100] Creating Layer conv1
I1128 11:59:23.336483  1760 net.cpp:434] conv1 <- scaled
I1128 11:59:23.336494  1760 net.cpp:408] conv1 -> conv1
I1128 11:59:23.616999  1760 net.cpp:150] Setting up conv1
I1128 11:59:23.617040  1760 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1128 11:59:23.617058  1760 net.cpp:165] Memory required for data: 4153600
I1128 11:59:23.617100  1760 layer_factory.hpp:77] Creating layer pool1
I1128 11:59:23.617126  1760 net.cpp:100] Creating Layer pool1
I1128 11:59:23.617135  1760 net.cpp:434] pool1 <- conv1
I1128 11:59:23.617146  1760 net.cpp:408] pool1 -> pool1
I1128 11:59:23.617233  1760 net.cpp:150] Setting up pool1
I1128 11:59:23.617250  1760 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1128 11:59:23.617259  1760 net.cpp:165] Memory required for data: 4890880
I1128 11:59:23.617266  1760 layer_factory.hpp:77] Creating layer conv2
I1128 11:59:23.617292  1760 net.cpp:100] Creating Layer conv2
I1128 11:59:23.617307  1760 net.cpp:434] conv2 <- pool1
I1128 11:59:23.617317  1760 net.cpp:408] conv2 -> conv2
I1128 11:59:23.620203  1760 net.cpp:150] Setting up conv2
I1128 11:59:23.620220  1760 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1128 11:59:23.620240  1760 net.cpp:165] Memory required for data: 5710080
I1128 11:59:23.620262  1760 layer_factory.hpp:77] Creating layer pool2
I1128 11:59:23.620275  1760 net.cpp:100] Creating Layer pool2
I1128 11:59:23.620281  1760 net.cpp:434] pool2 <- conv2
I1128 11:59:23.620290  1760 net.cpp:408] pool2 -> pool2
I1128 11:59:23.620362  1760 net.cpp:150] Setting up pool2
I1128 11:59:23.620373  1760 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1128 11:59:23.620383  1760 net.cpp:165] Memory required for data: 5914880
I1128 11:59:23.620388  1760 layer_factory.hpp:77] Creating layer ip1
I1128 11:59:23.620415  1760 net.cpp:100] Creating Layer ip1
I1128 11:59:23.620422  1760 net.cpp:434] ip1 <- pool2
I1128 11:59:23.620431  1760 net.cpp:408] ip1 -> ip1
I1128 11:59:23.625723  1760 net.cpp:150] Setting up ip1
I1128 11:59:23.625740  1760 net.cpp:157] Top shape: 64 500 (32000)
I1128 11:59:23.625749  1760 net.cpp:165] Memory required for data: 6042880
I1128 11:59:23.625766  1760 layer_factory.hpp:77] Creating layer relu1
I1128 11:59:23.625787  1760 net.cpp:100] Creating Layer relu1
I1128 11:59:23.625793  1760 net.cpp:434] relu1 <- ip1
I1128 11:59:23.625802  1760 net.cpp:395] relu1 -> ip1 (in-place)
I1128 11:59:23.626111  1760 net.cpp:150] Setting up relu1
I1128 11:59:23.626126  1760 net.cpp:157] Top shape: 64 500 (32000)
I1128 11:59:23.626135  1760 net.cpp:165] Memory required for data: 6170880
I1128 11:59:23.626142  1760 layer_factory.hpp:77] Creating layer ip2
I1128 11:59:23.626158  1760 net.cpp:100] Creating Layer ip2
I1128 11:59:23.626164  1760 net.cpp:434] ip2 <- ip1
I1128 11:59:23.626178  1760 net.cpp:408] ip2 -> ip2
I1128 11:59:23.627147  1760 net.cpp:150] Setting up ip2
I1128 11:59:23.627166  1760 net.cpp:157] Top shape: 64 10 (640)
I1128 11:59:23.627176  1760 net.cpp:165] Memory required for data: 6173440
I1128 11:59:23.627187  1760 layer_factory.hpp:77] Creating layer loss
I1128 11:59:23.627202  1760 net.cpp:100] Creating Layer loss
I1128 11:59:23.627209  1760 net.cpp:434] loss <- ip2
I1128 11:59:23.627216  1760 net.cpp:434] loss <- label
I1128 11:59:23.627226  1760 net.cpp:408] loss -> loss
I1128 11:59:23.627246  1760 layer_factory.hpp:77] Creating layer loss
I1128 11:59:23.627954  1760 net.cpp:150] Setting up loss
I1128 11:59:23.627969  1760 net.cpp:157] Top shape: (1)
I1128 11:59:23.627979  1760 net.cpp:160]     with loss weight 1
I1128 11:59:23.628018  1760 net.cpp:165] Memory required for data: 6173444
I1128 11:59:23.628027  1760 net.cpp:226] loss needs backward computation.
I1128 11:59:23.628036  1760 net.cpp:226] ip2 needs backward computation.
I1128 11:59:23.628041  1760 net.cpp:226] relu1 needs backward computation.
I1128 11:59:23.628046  1760 net.cpp:226] ip1 needs backward computation.
I1128 11:59:23.628051  1760 net.cpp:226] pool2 needs backward computation.
I1128 11:59:23.628057  1760 net.cpp:226] conv2 needs backward computation.
I1128 11:59:23.628062  1760 net.cpp:226] pool1 needs backward computation.
I1128 11:59:23.628067  1760 net.cpp:226] conv1 needs backward computation.
I1128 11:59:23.628073  1760 net.cpp:228] scale does not need backward computation.
I1128 11:59:23.628079  1760 net.cpp:228] train-data does not need backward computation.
I1128 11:59:23.628083  1760 net.cpp:270] This network produces output loss
I1128 11:59:23.628106  1760 net.cpp:283] Network initialization done.
I1128 11:59:23.628883  1760 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1128 11:59:23.628937  1760 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train-data
I1128 11:59:23.628960  1760 net.cpp:58] Initializing net from parameters:
state {
phase: TEST
}
layer {
name: "val-data"
type: "Data"
top: "data"
top: "label"
include {
phase: TEST
}
transform_param {
mean_file: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto"
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/val_db"
batch_size: 32
backend: LMDB
}
}
layer {
name: "scale"
type: "Power"
bottom: "data"
top: "scaled"
power_param {
scale: 0.0125
}
}
layer {
name: "conv1"
type: "Convolution"
bottom: "scaled"
top: "conv1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 20
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool1"
type: "Pooling"
bottom: "conv1"
top: "pool1"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "conv2"
type: "Convolution"
bottom: "pool1"
top: "conv2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 50
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool2"
type: "Pooling"
bottom: "conv2"
top: "pool2"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "ip1"
type: "InnerProduct"
bottom: "pool2"
top: "ip1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 500
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "relu1"
type: "ReLU"
bottom: "ip1"
top: "ip1"
}
layer {
name: "ip2"
type: "InnerProduct"
bottom: "ip1"
top: "ip2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 10
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "accuracy"
type: "Accuracy"
bottom: "ip2"
bottom: "label"
top: "accuracy"
include {
phase: TEST
}
}
layer {
name: "loss"
type: "SoftmaxWithLoss"
bottom: "ip2"
bottom: "label"
top: "loss"
}
I1128 11:59:23.629323  1760 layer_factory.hpp:77] Creating layer val-data
I1128 11:59:23.629474  1760 net.cpp:100] Creating Layer val-data
I1128 11:59:23.629489  1760 net.cpp:408] val-data -> data
I1128 11:59:23.629504  1760 net.cpp:408] val-data -> label
I1128 11:59:23.629518  1760 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto
I1128 11:59:23.632336  1767 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/val_db
I1128 11:59:23.635368  1760 data_layer.cpp:41] output data size: 32,3,28,28
I1128 11:59:23.636792  1760 net.cpp:150] Setting up val-data
I1128 11:59:23.636808  1760 net.cpp:157] Top shape: 32 3 28 28 (75264)
I1128 11:59:23.636821  1760 net.cpp:157] Top shape: 32 (32)
I1128 11:59:23.636827  1760 net.cpp:165] Memory required for data: 301184
I1128 11:59:23.636836  1760 layer_factory.hpp:77] Creating layer label_val-data_1_split
I1128 11:59:23.636863  1760 net.cpp:100] Creating Layer label_val-data_1_split
I1128 11:59:23.636870  1760 net.cpp:434] label_val-data_1_split <- label
I1128 11:59:23.636884  1760 net.cpp:408] label_val-data_1_split -> label_val-data_1_split_0
I1128 11:59:23.636899  1760 net.cpp:408] label_val-data_1_split -> label_val-data_1_split_1
I1128 11:59:23.637015  1760 net.cpp:150] Setting up label_val-data_1_split
I1128 11:59:23.637027  1760 net.cpp:157] Top shape: 32 (32)
I1128 11:59:23.637042  1760 net.cpp:157] Top shape: 32 (32)
I1128 11:59:23.637048  1760 net.cpp:165] Memory required for data: 301440
I1128 11:59:23.637054  1760 layer_factory.hpp:77] Creating layer scale
I1128 11:59:23.637073  1760 net.cpp:100] Creating Layer scale
I1128 11:59:23.637079  1760 net.cpp:434] scale <- data
I1128 11:59:23.637087  1760 net.cpp:408] scale -> scaled
I1128 11:59:23.637145  1760 net.cpp:150] Setting up scale
I1128 11:59:23.637156  1760 net.cpp:157] Top shape: 32 3 28 28 (75264)
I1128 11:59:23.637164  1760 net.cpp:165] Memory required for data: 602496
I1128 11:59:23.637171  1760 layer_factory.hpp:77] Creating layer conv1
I1128 11:59:23.637190  1760 net.cpp:100] Creating Layer conv1
I1128 11:59:23.637197  1760 net.cpp:434] conv1 <- scaled
I1128 11:59:23.637207  1760 net.cpp:408] conv1 -> conv1
I1128 11:59:23.639540  1760 net.cpp:150] Setting up conv1
I1128 11:59:23.639556  1760 net.cpp:157] Top shape: 32 20 24 24 (368640)
I1128 11:59:23.639567  1760 net.cpp:165] Memory required for data: 2077056
I1128 11:59:23.639585  1760 layer_factory.hpp:77] Creating layer pool1
I1128 11:59:23.639602  1760 net.cpp:100] Creating Layer pool1
I1128 11:59:23.639611  1760 net.cpp:434] pool1 <- conv1
I1128 11:59:23.639621  1760 net.cpp:408] pool1 -> pool1
I1128 11:59:23.639690  1760 net.cpp:150] Setting up pool1
I1128 11:59:23.639701  1760 net.cpp:157] Top shape: 32 20 12 12 (92160)
I1128 11:59:23.639710  1760 net.cpp:165] Memory required for data: 2445696
I1128 11:59:23.639716  1760 layer_factory.hpp:77] Creating layer conv2
I1128 11:59:23.639735  1760 net.cpp:100] Creating Layer conv2
I1128 11:59:23.639741  1760 net.cpp:434] conv2 <- pool1
I1128 11:59:23.639755  1760 net.cpp:408] conv2 -> conv2
I1128 11:59:23.641809  1760 net.cpp:150] Setting up conv2
I1128 11:59:23.641825  1760 net.cpp:157] Top shape: 32 50 8 8 (102400)
I1128 11:59:23.641835  1760 net.cpp:165] Memory required for data: 2855296
I1128 11:59:23.641855  1760 layer_factory.hpp:77] Creating layer pool2
I1128 11:59:23.641867  1760 net.cpp:100] Creating Layer pool2
I1128 11:59:23.641873  1760 net.cpp:434] pool2 <- conv2
I1128 11:59:23.641882  1760 net.cpp:408] pool2 -> pool2
I1128 11:59:23.641997  1760 net.cpp:150] Setting up pool2
I1128 11:59:23.642009  1760 net.cpp:157] Top shape: 32 50 4 4 (25600)
I1128 11:59:23.642019  1760 net.cpp:165] Memory required for data: 2957696
I1128 11:59:23.642025  1760 layer_factory.hpp:77] Creating layer ip1
I1128 11:59:23.642040  1760 net.cpp:100] Creating Layer ip1
I1128 11:59:23.642047  1760 net.cpp:434] ip1 <- pool2
I1128 11:59:23.642056  1760 net.cpp:408] ip1 -> ip1
I1128 11:59:23.647228  1760 net.cpp:150] Setting up ip1
I1128 11:59:23.647243  1760 net.cpp:157] Top shape: 32 500 (16000)
I1128 11:59:23.647253  1760 net.cpp:165] Memory required for data: 3021696
I1128 11:59:23.647269  1760 layer_factory.hpp:77] Creating layer relu1
I1128 11:59:23.647280  1760 net.cpp:100] Creating Layer relu1
I1128 11:59:23.647287  1760 net.cpp:434] relu1 <- ip1
I1128 11:59:23.647321  1760 net.cpp:395] relu1 -> ip1 (in-place)
I1128 11:59:23.647620  1760 net.cpp:150] Setting up relu1
I1128 11:59:23.647634  1760 net.cpp:157] Top shape: 32 500 (16000)
I1128 11:59:23.647644  1760 net.cpp:165] Memory required for data: 3085696
I1128 11:59:23.647650  1760 layer_factory.hpp:77] Creating layer ip2
I1128 11:59:23.647661  1760 net.cpp:100] Creating Layer ip2
I1128 11:59:23.647667  1760 net.cpp:434] ip2 <- ip1
I1128 11:59:23.647680  1760 net.cpp:408] ip2 -> ip2
I1128 11:59:23.647920  1760 net.cpp:150] Setting up ip2
I1128 11:59:23.647933  1760 net.cpp:157] Top shape: 32 10 (320)
I1128 11:59:23.647941  1760 net.cpp:165] Memory required for data: 3086976
I1128 11:59:23.647953  1760 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I1128 11:59:23.647964  1760 net.cpp:100] Creating Layer ip2_ip2_0_split
I1128 11:59:23.647969  1760 net.cpp:434] ip2_ip2_0_split <- ip2
I1128 11:59:23.647980  1760 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1128 11:59:23.647992  1760 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1128 11:59:23.648052  1760 net.cpp:150] Setting up ip2_ip2_0_split
I1128 11:59:23.648063  1760 net.cpp:157] Top shape: 32 10 (320)
I1128 11:59:23.648072  1760 net.cpp:157] Top shape: 32 10 (320)
I1128 11:59:23.648078  1760 net.cpp:165] Memory required for data: 3089536
I1128 11:59:23.648084  1760 layer_factory.hpp:77] Creating layer accuracy
I1128 11:59:23.648097  1760 net.cpp:100] Creating Layer accuracy
I1128 11:59:23.648104  1760 net.cpp:434] accuracy <- ip2_ip2_0_split_0
I1128 11:59:23.648111  1760 net.cpp:434] accuracy <- label_val-data_1_split_0
I1128 11:59:23.648123  1760 net.cpp:408] accuracy -> accuracy
I1128 11:59:23.648139  1760 net.cpp:150] Setting up accuracy
I1128 11:59:23.648149  1760 net.cpp:157] Top shape: (1)
I1128 11:59:23.648155  1760 net.cpp:165] Memory required for data: 3089540
I1128 11:59:23.648161  1760 layer_factory.hpp:77] Creating layer loss
I1128 11:59:23.648175  1760 net.cpp:100] Creating Layer loss
I1128 11:59:23.648180  1760 net.cpp:434] loss <- ip2_ip2_0_split_1
I1128 11:59:23.648186  1760 net.cpp:434] loss <- label_val-data_1_split_1
I1128 11:59:23.648196  1760 net.cpp:408] loss -> loss
I1128 11:59:23.648210  1760 layer_factory.hpp:77] Creating layer loss
I1128 11:59:23.648860  1760 net.cpp:150] Setting up loss
I1128 11:59:23.648876  1760 net.cpp:157] Top shape: (1)
I1128 11:59:23.648885  1760 net.cpp:160]     with loss weight 1
I1128 11:59:23.648896  1760 net.cpp:165] Memory required for data: 3089544
I1128 11:59:23.648902  1760 net.cpp:226] loss needs backward computation.
I1128 11:59:23.648908  1760 net.cpp:228] accuracy does not need backward computation.
I1128 11:59:23.648921  1760 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1128 11:59:23.648926  1760 net.cpp:226] ip2 needs backward computation.
I1128 11:59:23.648931  1760 net.cpp:226] relu1 needs backward computation.
I1128 11:59:23.648936  1760 net.cpp:226] ip1 needs backward computation.
I1128 11:59:23.648941  1760 net.cpp:226] pool2 needs backward computation.
I1128 11:59:23.648947  1760 net.cpp:226] conv2 needs backward computation.
I1128 11:59:23.648952  1760 net.cpp:226] pool1 needs backward computation.
I1128 11:59:23.648957  1760 net.cpp:226] conv1 needs backward computation.
I1128 11:59:23.648962  1760 net.cpp:228] scale does not need backward computation.
I1128 11:59:23.648968  1760 net.cpp:228] label_val-data_1_split does not need backward computation.
I1128 11:59:23.648974  1760 net.cpp:228] val-data does not need backward computation.
I1128 11:59:23.648979  1760 net.cpp:270] This network produces output accuracy
I1128 11:59:23.648985  1760 net.cpp:270] This network produces output loss
I1128 11:59:23.649008  1760 net.cpp:283] Network initialization done.
I1128 11:59:23.649094  1760 solver.cpp:60] Solver scaffolding done.
I1128 11:59:23.649531  1760 caffe.cpp:251] Starting Optimization
I1128 11:59:23.649547  1760 solver.cpp:279] Solving
I1128 11:59:23.649552  1760 solver.cpp:280] Learning Rate Policy: step
I1128 11:59:23.650590  1760 solver.cpp:337] Iteration 0, Testing net (#0)
I1128 11:59:23.650627  1760 net.cpp:693] Ignoring source layer train-data
I1128 11:59:23.663182  1760 blocking_queue.cpp:50] Data layer prefetch queue empty
I1128 11:59:24.563922  1760 solver.cpp:404]     Test net output #0: accuracy = 0.134795
I1128 11:59:24.563963  1760 solver.cpp:404]     Test net output #1: loss = 2.62217 (* 1 = 2.62217 loss)
I1128 11:59:24.573024  1760 solver.cpp:228] Iteration 0, loss = 2.6976
I1128 11:59:24.573065  1760 solver.cpp:244]     Train net output #0: loss = 2.6976 (* 1 = 2.6976 loss)
I1128 11:59:24.573101  1760 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1128 11:59:24.908006  1760 solver.cpp:228] Iteration 79, loss = 0.259404
I1128 11:59:24.908134  1760 solver.cpp:244]     Train net output #0: loss = 0.259404 (* 1 = 0.259404 loss)
I1128 11:59:24.908184  1760 sgd_solver.cpp:106] Iteration 79, lr = 0.01
I1128 11:59:25.238343  1760 solver.cpp:228] Iteration 158, loss = 0.278189
I1128 11:59:25.238433  1760 solver.cpp:244]     Train net output #0: loss = 0.278189 (* 1 = 0.278189 loss)
I1128 11:59:25.238484  1760 sgd_solver.cpp:106] Iteration 158, lr = 0.01
I1128 11:59:25.569046  1760 solver.cpp:228] Iteration 237, loss = 0.16293
I1128 11:59:25.569136  1760 solver.cpp:244]     Train net output #0: loss = 0.16293 (* 1 = 0.16293 loss)
I1128 11:59:25.569185  1760 sgd_solver.cpp:106] Iteration 237, lr = 0.01
I1128 11:59:25.899803  1760 solver.cpp:228] Iteration 316, loss = 0.214891
I1128 11:59:25.899893  1760 solver.cpp:244]     Train net output #0: loss = 0.214891 (* 1 = 0.214891 loss)
I1128 11:59:25.899942  1760 sgd_solver.cpp:106] Iteration 316, lr = 0.01
I1128 11:59:26.229957  1760 solver.cpp:228] Iteration 395, loss = 0.030236
I1128 11:59:26.230051  1760 solver.cpp:244]     Train net output #0: loss = 0.0302359 (* 1 = 0.0302359 loss)
I1128 11:59:26.230100  1760 sgd_solver.cpp:106] Iteration 395, lr = 0.01
I1128 11:59:26.560804  1760 solver.cpp:228] Iteration 474, loss = 0.0831995
I1128 11:59:26.560896  1760 solver.cpp:244]     Train net output #0: loss = 0.0831994 (* 1 = 0.0831994 loss)
I1128 11:59:26.560948  1760 sgd_solver.cpp:106] Iteration 474, lr = 0.01
I1128 11:59:26.890883  1760 solver.cpp:228] Iteration 553, loss = 0.0837035
I1128 11:59:26.890995  1760 solver.cpp:244]     Train net output #0: loss = 0.0837035 (* 1 = 0.0837035 loss)
I1128 11:59:26.891044  1760 sgd_solver.cpp:106] Iteration 553, lr = 0.01
I1128 11:59:27.215811  1760 solver.cpp:228] Iteration 632, loss = 0.0616661
I1128 11:59:27.215884  1760 solver.cpp:244]     Train net output #0: loss = 0.061666 (* 1 = 0.061666 loss)
I1128 11:59:27.215899  1760 sgd_solver.cpp:106] Iteration 632, lr = 0.01
I1128 11:59:27.507414  1760 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_704.caffemodel
I1128 11:59:27.525055  1760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_704.solverstate
I1128 11:59:27.531972  1760 solver.cpp:337] Iteration 704, Testing net (#0)
I1128 11:59:27.531994  1760 net.cpp:693] Ignoring source layer train-data
I1128 11:59:28.425518  1760 solver.cpp:404]     Test net output #0: accuracy = 0.979544
I1128 11:59:28.425645  1760 solver.cpp:404]     Test net output #1: loss = 0.0640542 (* 1 = 0.0640542 loss)
I1128 11:59:28.456298  1760 solver.cpp:228] Iteration 711, loss = 0.0914695
I1128 11:59:28.456327  1760 solver.cpp:244]     Train net output #0: loss = 0.0914694 (* 1 = 0.0914694 loss)
I1128 11:59:28.456338  1760 sgd_solver.cpp:106] Iteration 711, lr = 0.01
I1128 11:59:28.780158  1760 solver.cpp:228] Iteration 790, loss = 0.0420571
I1128 11:59:28.780186  1760 solver.cpp:244]     Train net output #0: loss = 0.042057 (* 1 = 0.042057 loss)
I1128 11:59:28.780197  1760 sgd_solver.cpp:106] Iteration 790, lr = 0.01
I1128 11:59:29.103960  1760 solver.cpp:228] Iteration 869, loss = 0.0173216
I1128 11:59:29.103988  1760 solver.cpp:244]     Train net output #0: loss = 0.0173215 (* 1 = 0.0173215 loss)
I1128 11:59:29.104001  1760 sgd_solver.cpp:106] Iteration 869, lr = 0.01
I1128 11:59:29.428464  1760 solver.cpp:228] Iteration 948, loss = 0.0052083
I1128 11:59:29.428541  1760 solver.cpp:244]     Train net output #0: loss = 0.00520827 (* 1 = 0.00520827 loss)
I1128 11:59:29.428553  1760 sgd_solver.cpp:106] Iteration 948, lr = 0.01
I1128 11:59:29.752835  1760 solver.cpp:228] Iteration 1027, loss = 0.0368996
I1128 11:59:29.752864  1760 solver.cpp:244]     Train net output #0: loss = 0.0368996 (* 1 = 0.0368996 loss)
I1128 11:59:29.752876  1760 sgd_solver.cpp:106] Iteration 1027, lr = 0.01
I1128 11:59:30.077870  1760 solver.cpp:228] Iteration 1106, loss = 0.00296281
I1128 11:59:30.077903  1760 solver.cpp:244]     Train net output #0: loss = 0.00296274 (* 1 = 0.00296274 loss)
I1128 11:59:30.077914  1760 sgd_solver.cpp:106] Iteration 1106, lr = 0.01
I1128 11:59:30.403609  1760 solver.cpp:228] Iteration 1185, loss = 0.00859121
I1128 11:59:30.403637  1760 solver.cpp:244]     Train net output #0: loss = 0.00859115 (* 1 = 0.00859115 loss)
I1128 11:59:30.403648  1760 sgd_solver.cpp:106] Iteration 1185, lr = 0.01
I1128 11:59:30.729436  1760 solver.cpp:228] Iteration 1264, loss = 0.0368105
I1128 11:59:30.729465  1760 solver.cpp:244]     Train net output #0: loss = 0.0368104 (* 1 = 0.0368104 loss)
I1128 11:59:30.729476  1760 sgd_solver.cpp:106] Iteration 1264, lr = 0.01
I1128 11:59:31.055455  1760 solver.cpp:228] Iteration 1343, loss = 0.00272629
I1128 11:59:31.055483  1760 solver.cpp:244]     Train net output #0: loss = 0.00272621 (* 1 = 0.00272621 loss)
I1128 11:59:31.055495  1760 sgd_solver.cpp:106] Iteration 1343, lr = 0.01
I1128 11:59:31.319679  1760 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_1408.caffemodel
I1128 11:59:31.335852  1760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_1408.solverstate
I1128 11:59:31.342507  1760 solver.cpp:337] Iteration 1408, Testing net (#0)
I1128 11:59:31.342536  1760 net.cpp:693] Ignoring source layer train-data
I1128 11:59:31.489771  1760 blocking_queue.cpp:50] Data layer prefetch queue empty
