salloc: Granted job allocation 3888297
srun: Job step created
I1129 09:33:26.332195 10257 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/digits/jobs/20161129-093324-e0ee/solver.prototxt
I1129 09:33:26.332612 10257 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1129 09:33:26.332623 10257 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1129 09:33:26.332752 10257 caffe.cpp:217] Using GPUs 0
I1129 09:33:26.377930 10257 caffe.cpp:222] GPU 0: Tesla K20m
I1129 09:33:26.769230 10257 solver.cpp:48] Initializing solver from parameters:
test_iter: 469
test_interval: 704
base_lr: 0.01
display: 79
max_iter: 21120
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 6970
snapshot: 704
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
net: "train_val.prototxt"
train_state {
level: 0
stage: ""
}
type: "SGD"
I1129 09:33:26.769992 10257 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1129 09:33:26.770989 10257 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val-data
I1129 09:33:26.771028 10257 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1129 09:33:26.771049 10257 net.cpp:58] Initializing net from parameters:
state {
phase: TRAIN
level: 0
stage: ""
}
layer {
name: "train-data"
type: "Data"
top: "data"
top: "label"
include {
phase: TRAIN
}
transform_param {
mean_file: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto"
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/train_db"
batch_size: 64
backend: LMDB
}
}
layer {
name: "scale"
type: "Power"
bottom: "data"
top: "scaled"
power_param {
scale: 0.0125
}
}
layer {
name: "conv1"
type: "Convolution"
bottom: "scaled"
top: "conv1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 20
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool1"
type: "Pooling"
bottom: "conv1"
top: "pool1"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "conv2"
type: "Convolution"
bottom: "pool1"
top: "conv2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 50
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool2"
type: "Pooling"
bottom: "conv2"
top: "pool2"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "ip1"
type: "InnerProduct"
bottom: "pool2"
top: "ip1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 500
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "relu1"
type: "ReLU"
bottom: "ip1"
top: "ip1"
}
layer {
name: "ip2"
type: "InnerProduct"
bottom: "ip1"
top: "ip2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 10
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "loss"
type: "SoftmaxWithLoss"
bottom: "ip2"
bottom: "label"
top: "loss"
}
I1129 09:33:26.771437 10257 layer_factory.hpp:77] Creating layer train-data
I1129 09:33:26.772197 10257 net.cpp:100] Creating Layer train-data
I1129 09:33:26.772215 10257 net.cpp:408] train-data -> data
I1129 09:33:26.772270 10257 net.cpp:408] train-data -> label
I1129 09:33:26.772436 10257 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto
I1129 09:33:26.775918 10263 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/train_db
I1129 09:33:26.792623 10257 data_layer.cpp:41] output data size: 64,3,28,28
I1129 09:33:26.796463 10257 net.cpp:150] Setting up train-data
I1129 09:33:26.796537 10257 net.cpp:157] Top shape: 64 3 28 28 (150528)
I1129 09:33:26.796600 10257 net.cpp:157] Top shape: 64 (64)
I1129 09:33:26.796609 10257 net.cpp:165] Memory required for data: 602368
I1129 09:33:26.796640 10257 layer_factory.hpp:77] Creating layer scale
I1129 09:33:26.796663 10257 net.cpp:100] Creating Layer scale
I1129 09:33:26.796670 10257 net.cpp:434] scale <- data
I1129 09:33:26.796689 10257 net.cpp:408] scale -> scaled
I1129 09:33:26.796803 10257 net.cpp:150] Setting up scale
I1129 09:33:26.796815 10257 net.cpp:157] Top shape: 64 3 28 28 (150528)
I1129 09:33:26.796830 10257 net.cpp:165] Memory required for data: 1204480
I1129 09:33:26.796838 10257 layer_factory.hpp:77] Creating layer conv1
I1129 09:33:26.796875 10257 net.cpp:100] Creating Layer conv1
I1129 09:33:26.796888 10257 net.cpp:434] conv1 <- scaled
I1129 09:33:26.796900 10257 net.cpp:408] conv1 -> conv1
I1129 09:33:27.056385 10257 net.cpp:150] Setting up conv1
I1129 09:33:27.056424 10257 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1129 09:33:27.056442 10257 net.cpp:165] Memory required for data: 4153600
I1129 09:33:27.056485 10257 layer_factory.hpp:77] Creating layer pool1
I1129 09:33:27.056510 10257 net.cpp:100] Creating Layer pool1
I1129 09:33:27.056519 10257 net.cpp:434] pool1 <- conv1
I1129 09:33:27.056530 10257 net.cpp:408] pool1 -> pool1
I1129 09:33:27.056632 10257 net.cpp:150] Setting up pool1
I1129 09:33:27.056649 10257 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1129 09:33:27.056659 10257 net.cpp:165] Memory required for data: 4890880
I1129 09:33:27.056665 10257 layer_factory.hpp:77] Creating layer conv2
I1129 09:33:27.056690 10257 net.cpp:100] Creating Layer conv2
I1129 09:33:27.056700 10257 net.cpp:434] conv2 <- pool1
I1129 09:33:27.056710 10257 net.cpp:408] conv2 -> conv2
I1129 09:33:27.059659 10257 net.cpp:150] Setting up conv2
I1129 09:33:27.059676 10257 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1129 09:33:27.059687 10257 net.cpp:165] Memory required for data: 5710080
I1129 09:33:27.059710 10257 layer_factory.hpp:77] Creating layer pool2
I1129 09:33:27.059723 10257 net.cpp:100] Creating Layer pool2
I1129 09:33:27.059729 10257 net.cpp:434] pool2 <- conv2
I1129 09:33:27.059738 10257 net.cpp:408] pool2 -> pool2
I1129 09:33:27.059806 10257 net.cpp:150] Setting up pool2
I1129 09:33:27.059818 10257 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1129 09:33:27.059826 10257 net.cpp:165] Memory required for data: 5914880
I1129 09:33:27.059840 10257 layer_factory.hpp:77] Creating layer ip1
I1129 09:33:27.059861 10257 net.cpp:100] Creating Layer ip1
I1129 09:33:27.059867 10257 net.cpp:434] ip1 <- pool2
I1129 09:33:27.059877 10257 net.cpp:408] ip1 -> ip1
I1129 09:33:27.065228 10257 net.cpp:150] Setting up ip1
I1129 09:33:27.065243 10257 net.cpp:157] Top shape: 64 500 (32000)
I1129 09:33:27.065253 10257 net.cpp:165] Memory required for data: 6042880
I1129 09:33:27.065270 10257 layer_factory.hpp:77] Creating layer relu1
I1129 09:33:27.065289 10257 net.cpp:100] Creating Layer relu1
I1129 09:33:27.065296 10257 net.cpp:434] relu1 <- ip1
I1129 09:33:27.065305 10257 net.cpp:395] relu1 -> ip1 (in-place)
I1129 09:33:27.065618 10257 net.cpp:150] Setting up relu1
I1129 09:33:27.065630 10257 net.cpp:157] Top shape: 64 500 (32000)
I1129 09:33:27.065640 10257 net.cpp:165] Memory required for data: 6170880
I1129 09:33:27.065647 10257 layer_factory.hpp:77] Creating layer ip2
I1129 09:33:27.065663 10257 net.cpp:100] Creating Layer ip2
I1129 09:33:27.065670 10257 net.cpp:434] ip2 <- ip1
I1129 09:33:27.065682 10257 net.cpp:408] ip2 -> ip2
I1129 09:33:27.066686 10257 net.cpp:150] Setting up ip2
I1129 09:33:27.066702 10257 net.cpp:157] Top shape: 64 10 (640)
I1129 09:33:27.066711 10257 net.cpp:165] Memory required for data: 6173440
I1129 09:33:27.066727 10257 layer_factory.hpp:77] Creating layer loss
I1129 09:33:27.066742 10257 net.cpp:100] Creating Layer loss
I1129 09:33:27.066748 10257 net.cpp:434] loss <- ip2
I1129 09:33:27.066756 10257 net.cpp:434] loss <- label
I1129 09:33:27.066766 10257 net.cpp:408] loss -> loss
I1129 09:33:27.066786 10257 layer_factory.hpp:77] Creating layer loss
I1129 09:33:27.067503 10257 net.cpp:150] Setting up loss
I1129 09:33:27.067518 10257 net.cpp:157] Top shape: (1)
I1129 09:33:27.067528 10257 net.cpp:160]     with loss weight 1
I1129 09:33:27.067569 10257 net.cpp:165] Memory required for data: 6173444
I1129 09:33:27.067576 10257 net.cpp:226] loss needs backward computation.
I1129 09:33:27.067584 10257 net.cpp:226] ip2 needs backward computation.
I1129 09:33:27.067590 10257 net.cpp:226] relu1 needs backward computation.
I1129 09:33:27.067595 10257 net.cpp:226] ip1 needs backward computation.
I1129 09:33:27.067600 10257 net.cpp:226] pool2 needs backward computation.
I1129 09:33:27.067605 10257 net.cpp:226] conv2 needs backward computation.
I1129 09:33:27.067611 10257 net.cpp:226] pool1 needs backward computation.
I1129 09:33:27.067616 10257 net.cpp:226] conv1 needs backward computation.
I1129 09:33:27.067621 10257 net.cpp:228] scale does not need backward computation.
I1129 09:33:27.067628 10257 net.cpp:228] train-data does not need backward computation.
I1129 09:33:27.067632 10257 net.cpp:270] This network produces output loss
I1129 09:33:27.067651 10257 net.cpp:283] Network initialization done.
I1129 09:33:27.068437 10257 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1129 09:33:27.068490 10257 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train-data
I1129 09:33:27.068512 10257 net.cpp:58] Initializing net from parameters:
state {
phase: TEST
}
layer {
name: "val-data"
type: "Data"
top: "data"
top: "label"
include {
phase: TEST
}
transform_param {
mean_file: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto"
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/val_db"
batch_size: 32
backend: LMDB
}
}
layer {
name: "scale"
type: "Power"
bottom: "data"
top: "scaled"
power_param {
scale: 0.0125
}
}
layer {
name: "conv1"
type: "Convolution"
bottom: "scaled"
top: "conv1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 20
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool1"
type: "Pooling"
bottom: "conv1"
top: "pool1"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "conv2"
type: "Convolution"
bottom: "pool1"
top: "conv2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
convolution_param {
num_output: 50
kernel_size: 5
stride: 1
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "pool2"
type: "Pooling"
bottom: "conv2"
top: "pool2"
pooling_param {
pool: MAX
kernel_size: 2
stride: 2
}
}
layer {
name: "ip1"
type: "InnerProduct"
bottom: "pool2"
top: "ip1"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 500
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "relu1"
type: "ReLU"
bottom: "ip1"
top: "ip1"
}
layer {
name: "ip2"
type: "InnerProduct"
bottom: "ip1"
top: "ip2"
param {
lr_mult: 1
}
param {
lr_mult: 2
}
inner_product_param {
num_output: 10
weight_filler {
type: "xavier"
}
bias_filler {
type: "constant"
}
}
}
layer {
name: "accuracy"
type: "Accuracy"
bottom: "ip2"
bottom: "label"
top: "accuracy"
include {
phase: TEST
}
}
layer {
name: "loss"
type: "SoftmaxWithLoss"
bottom: "ip2"
bottom: "label"
top: "loss"
}
I1129 09:33:27.068869 10257 layer_factory.hpp:77] Creating layer val-data
I1129 09:33:27.069028 10257 net.cpp:100] Creating Layer val-data
I1129 09:33:27.069046 10257 net.cpp:408] val-data -> data
I1129 09:33:27.069061 10257 net.cpp:408] val-data -> label
I1129 09:33:27.069074 10257 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/mean.binaryproto
I1129 09:33:27.071445 10265 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/digits/jobs/20161125-161231-08c0/val_db
I1129 09:33:27.071727 10257 data_layer.cpp:41] output data size: 32,3,28,28
I1129 09:33:27.072484 10257 net.cpp:150] Setting up val-data
I1129 09:33:27.072497 10257 net.cpp:157] Top shape: 32 3 28 28 (75264)
I1129 09:33:27.072510 10257 net.cpp:157] Top shape: 32 (32)
I1129 09:33:27.072525 10257 net.cpp:165] Memory required for data: 301184
I1129 09:33:27.072531 10257 layer_factory.hpp:77] Creating layer label_val-data_1_split
I1129 09:33:27.072547 10257 net.cpp:100] Creating Layer label_val-data_1_split
I1129 09:33:27.072553 10257 net.cpp:434] label_val-data_1_split <- label
I1129 09:33:27.072566 10257 net.cpp:408] label_val-data_1_split -> label_val-data_1_split_0
I1129 09:33:27.072580 10257 net.cpp:408] label_val-data_1_split -> label_val-data_1_split_1
I1129 09:33:27.072695 10257 net.cpp:150] Setting up label_val-data_1_split
I1129 09:33:27.072706 10257 net.cpp:157] Top shape: 32 (32)
I1129 09:33:27.072732 10257 net.cpp:157] Top shape: 32 (32)
I1129 09:33:27.072739 10257 net.cpp:165] Memory required for data: 301440
I1129 09:33:27.072746 10257 layer_factory.hpp:77] Creating layer scale
I1129 09:33:27.072757 10257 net.cpp:100] Creating Layer scale
I1129 09:33:27.072763 10257 net.cpp:434] scale <- data
I1129 09:33:27.072772 10257 net.cpp:408] scale -> scaled
I1129 09:33:27.072844 10257 net.cpp:150] Setting up scale
I1129 09:33:27.072855 10257 net.cpp:157] Top shape: 32 3 28 28 (75264)
I1129 09:33:27.072865 10257 net.cpp:165] Memory required for data: 602496
I1129 09:33:27.072870 10257 layer_factory.hpp:77] Creating layer conv1
I1129 09:33:27.072888 10257 net.cpp:100] Creating Layer conv1
I1129 09:33:27.072896 10257 net.cpp:434] conv1 <- scaled
I1129 09:33:27.072906 10257 net.cpp:408] conv1 -> conv1
I1129 09:33:27.074937 10257 net.cpp:150] Setting up conv1
I1129 09:33:27.074954 10257 net.cpp:157] Top shape: 32 20 24 24 (368640)
I1129 09:33:27.074970 10257 net.cpp:165] Memory required for data: 2077056
I1129 09:33:27.074988 10257 layer_factory.hpp:77] Creating layer pool1
I1129 09:33:27.075002 10257 net.cpp:100] Creating Layer pool1
I1129 09:33:27.075008 10257 net.cpp:434] pool1 <- conv1
I1129 09:33:27.075023 10257 net.cpp:408] pool1 -> pool1
I1129 09:33:27.075089 10257 net.cpp:150] Setting up pool1
I1129 09:33:27.075100 10257 net.cpp:157] Top shape: 32 20 12 12 (92160)
I1129 09:33:27.075109 10257 net.cpp:165] Memory required for data: 2445696
I1129 09:33:27.075115 10257 layer_factory.hpp:77] Creating layer conv2
I1129 09:33:27.075135 10257 net.cpp:100] Creating Layer conv2
I1129 09:33:27.075141 10257 net.cpp:434] conv2 <- pool1
I1129 09:33:27.075151 10257 net.cpp:408] conv2 -> conv2
I1129 09:33:27.077198 10257 net.cpp:150] Setting up conv2
I1129 09:33:27.077214 10257 net.cpp:157] Top shape: 32 50 8 8 (102400)
I1129 09:33:27.077224 10257 net.cpp:165] Memory required for data: 2855296
I1129 09:33:27.077244 10257 layer_factory.hpp:77] Creating layer pool2
I1129 09:33:27.077256 10257 net.cpp:100] Creating Layer pool2
I1129 09:33:27.077262 10257 net.cpp:434] pool2 <- conv2
I1129 09:33:27.077272 10257 net.cpp:408] pool2 -> pool2
I1129 09:33:27.077400 10257 net.cpp:150] Setting up pool2
I1129 09:33:27.077412 10257 net.cpp:157] Top shape: 32 50 4 4 (25600)
I1129 09:33:27.077421 10257 net.cpp:165] Memory required for data: 2957696
I1129 09:33:27.077427 10257 layer_factory.hpp:77] Creating layer ip1
I1129 09:33:27.077441 10257 net.cpp:100] Creating Layer ip1
I1129 09:33:27.077447 10257 net.cpp:434] ip1 <- pool2
I1129 09:33:27.077461 10257 net.cpp:408] ip1 -> ip1
I1129 09:33:27.082664 10257 net.cpp:150] Setting up ip1
I1129 09:33:27.082679 10257 net.cpp:157] Top shape: 32 500 (16000)
I1129 09:33:27.082689 10257 net.cpp:165] Memory required for data: 3021696
I1129 09:33:27.082707 10257 layer_factory.hpp:77] Creating layer relu1
I1129 09:33:27.082720 10257 net.cpp:100] Creating Layer relu1
I1129 09:33:27.082726 10257 net.cpp:434] relu1 <- ip1
I1129 09:33:27.082756 10257 net.cpp:395] relu1 -> ip1 (in-place)
I1129 09:33:27.083048 10257 net.cpp:150] Setting up relu1
I1129 09:33:27.083062 10257 net.cpp:157] Top shape: 32 500 (16000)
I1129 09:33:27.083071 10257 net.cpp:165] Memory required for data: 3085696
I1129 09:33:27.083078 10257 layer_factory.hpp:77] Creating layer ip2
I1129 09:33:27.083089 10257 net.cpp:100] Creating Layer ip2
I1129 09:33:27.083096 10257 net.cpp:434] ip2 <- ip1
I1129 09:33:27.083108 10257 net.cpp:408] ip2 -> ip2
I1129 09:33:27.083345 10257 net.cpp:150] Setting up ip2
I1129 09:33:27.083359 10257 net.cpp:157] Top shape: 32 10 (320)
I1129 09:33:27.083367 10257 net.cpp:165] Memory required for data: 3086976
I1129 09:33:27.083379 10257 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I1129 09:33:27.083389 10257 net.cpp:100] Creating Layer ip2_ip2_0_split
I1129 09:33:27.083395 10257 net.cpp:434] ip2_ip2_0_split <- ip2
I1129 09:33:27.083405 10257 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1129 09:33:27.083416 10257 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1129 09:33:27.083475 10257 net.cpp:150] Setting up ip2_ip2_0_split
I1129 09:33:27.083487 10257 net.cpp:157] Top shape: 32 10 (320)
I1129 09:33:27.083494 10257 net.cpp:157] Top shape: 32 10 (320)
I1129 09:33:27.083501 10257 net.cpp:165] Memory required for data: 3089536
I1129 09:33:27.083508 10257 layer_factory.hpp:77] Creating layer accuracy
I1129 09:33:27.083523 10257 net.cpp:100] Creating Layer accuracy
I1129 09:33:27.083530 10257 net.cpp:434] accuracy <- ip2_ip2_0_split_0
I1129 09:33:27.083537 10257 net.cpp:434] accuracy <- label_val-data_1_split_0
I1129 09:33:27.083546 10257 net.cpp:408] accuracy -> accuracy
I1129 09:33:27.083562 10257 net.cpp:150] Setting up accuracy
I1129 09:33:27.083577 10257 net.cpp:157] Top shape: (1)
I1129 09:33:27.083585 10257 net.cpp:165] Memory required for data: 3089540
I1129 09:33:27.083590 10257 layer_factory.hpp:77] Creating layer loss
I1129 09:33:27.083602 10257 net.cpp:100] Creating Layer loss
I1129 09:33:27.083608 10257 net.cpp:434] loss <- ip2_ip2_0_split_1
I1129 09:33:27.083616 10257 net.cpp:434] loss <- label_val-data_1_split_1
I1129 09:33:27.083623 10257 net.cpp:408] loss -> loss
I1129 09:33:27.083636 10257 layer_factory.hpp:77] Creating layer loss
I1129 09:33:27.084280 10257 net.cpp:150] Setting up loss
I1129 09:33:27.084295 10257 net.cpp:157] Top shape: (1)
I1129 09:33:27.084303 10257 net.cpp:160]     with loss weight 1
I1129 09:33:27.084313 10257 net.cpp:165] Memory required for data: 3089544
I1129 09:33:27.084321 10257 net.cpp:226] loss needs backward computation.
I1129 09:33:27.084326 10257 net.cpp:228] accuracy does not need backward computation.
I1129 09:33:27.084332 10257 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1129 09:33:27.084338 10257 net.cpp:226] ip2 needs backward computation.
I1129 09:33:27.084343 10257 net.cpp:226] relu1 needs backward computation.
I1129 09:33:27.084348 10257 net.cpp:226] ip1 needs backward computation.
I1129 09:33:27.084353 10257 net.cpp:226] pool2 needs backward computation.
I1129 09:33:27.084358 10257 net.cpp:226] conv2 needs backward computation.
I1129 09:33:27.084364 10257 net.cpp:226] pool1 needs backward computation.
I1129 09:33:27.084369 10257 net.cpp:226] conv1 needs backward computation.
I1129 09:33:27.084375 10257 net.cpp:228] scale does not need backward computation.
I1129 09:33:27.084381 10257 net.cpp:228] label_val-data_1_split does not need backward computation.
I1129 09:33:27.084388 10257 net.cpp:228] val-data does not need backward computation.
I1129 09:33:27.084396 10257 net.cpp:270] This network produces output accuracy
I1129 09:33:27.084403 10257 net.cpp:270] This network produces output loss
I1129 09:33:27.084419 10257 net.cpp:283] Network initialization done.
I1129 09:33:27.084499 10257 solver.cpp:60] Solver scaffolding done.
I1129 09:33:27.084930 10257 caffe.cpp:251] Starting Optimization
I1129 09:33:27.084944 10257 solver.cpp:279] Solving
I1129 09:33:27.084949 10257 solver.cpp:280] Learning Rate Policy: step
I1129 09:33:27.085999 10257 solver.cpp:337] Iteration 0, Testing net (#0)
I1129 09:33:27.086036 10257 net.cpp:693] Ignoring source layer train-data
I1129 09:33:27.095649 10257 blocking_queue.cpp:50] Data layer prefetch queue empty
I1129 09:33:28.000180 10257 solver.cpp:404]     Test net output #0: accuracy = 0.10621
I1129 09:33:28.000219 10257 solver.cpp:404]     Test net output #1: loss = 2.59559 (* 1 = 2.59559 loss)
I1129 09:33:28.004755 10257 solver.cpp:228] Iteration 0, loss = 2.49821
I1129 09:33:28.004784 10257 solver.cpp:244]     Train net output #0: loss = 2.49821 (* 1 = 2.49821 loss)
I1129 09:33:28.004822 10257 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1129 09:33:28.325785 10257 solver.cpp:228] Iteration 79, loss = 0.256778
I1129 09:33:28.325820 10257 solver.cpp:244]     Train net output #0: loss = 0.256778 (* 1 = 0.256778 loss)
I1129 09:33:28.325829 10257 sgd_solver.cpp:106] Iteration 79, lr = 0.01
I1129 09:33:28.647740 10257 solver.cpp:228] Iteration 158, loss = 0.36331
I1129 09:33:28.647773 10257 solver.cpp:244]     Train net output #0: loss = 0.36331 (* 1 = 0.36331 loss)
I1129 09:33:28.647780 10257 sgd_solver.cpp:106] Iteration 158, lr = 0.01
I1129 09:33:28.969548 10257 solver.cpp:228] Iteration 237, loss = 0.259146
I1129 09:33:28.969580 10257 solver.cpp:244]     Train net output #0: loss = 0.259146 (* 1 = 0.259146 loss)
I1129 09:33:28.969588 10257 sgd_solver.cpp:106] Iteration 237, lr = 0.01
I1129 09:33:29.290887 10257 solver.cpp:228] Iteration 316, loss = 0.338048
I1129 09:33:29.290917 10257 solver.cpp:244]     Train net output #0: loss = 0.338048 (* 1 = 0.338048 loss)
I1129 09:33:29.290926 10257 sgd_solver.cpp:106] Iteration 316, lr = 0.01
I1129 09:33:29.613286 10257 solver.cpp:228] Iteration 395, loss = 0.0325984
I1129 09:33:29.613306 10257 solver.cpp:244]     Train net output #0: loss = 0.0325985 (* 1 = 0.0325985 loss)
I1129 09:33:29.613314 10257 sgd_solver.cpp:106] Iteration 395, lr = 0.01
I1129 09:33:29.935858 10257 solver.cpp:228] Iteration 474, loss = 0.126076
I1129 09:33:29.935889 10257 solver.cpp:244]     Train net output #0: loss = 0.126076 (* 1 = 0.126076 loss)
I1129 09:33:29.935896 10257 sgd_solver.cpp:106] Iteration 474, lr = 0.01
