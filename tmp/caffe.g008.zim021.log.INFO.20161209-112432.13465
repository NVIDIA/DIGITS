Log file created at: 2016/12/09 11:24:32
Running on machine: g008
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:24:32.574671 13465 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112430-868a/solver.prototxt
I1209 11:24:32.576454 13465 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:24:32.576465 13465 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:24:32.577560 13465 caffe.cpp:217] Using GPUs 0
I1209 11:24:32.626732 13465 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:24:33.051177 13465 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1
test_interval: 9
base_lr: 0.01
display: 1
max_iter: 27
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 9
snapshot: 9
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:24:33.054020 13465 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:24:33.056479 13465 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 11:24:33.056499 13465 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I1209 11:24:33.056520 13465 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 127.61719
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112422-97b8/train_db/features"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112422-97b8/train_db/labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "identity"
  type: "Power"
  bottom: "data"
  top: "output"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:24:33.056736 13465 layer_factory.hpp:77] Creating layer data
I1209 11:24:33.056912 13465 net.cpp:100] Creating Layer data
I1209 11:24:33.056931 13465 net.cpp:408] data -> data
I1209 11:24:33.070029 13470 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112422-97b8/train_db/features
I1209 11:24:33.096101 13465 data_layer.cpp:41] output data size: 10,1,32,32
I1209 11:24:33.097339 13465 net.cpp:150] Setting up data
I1209 11:24:33.097383 13465 net.cpp:157] Top shape: 10 1 32 32 (10240)
I1209 11:24:33.097426 13465 net.cpp:165] Memory required for data: 40960
I1209 11:24:33.097456 13465 layer_factory.hpp:77] Creating layer label
I1209 11:24:33.097558 13465 net.cpp:100] Creating Layer label
I1209 11:24:33.097573 13465 net.cpp:408] label -> label
I1209 11:24:33.109520 13472 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112422-97b8/train_db/labels
I1209 11:24:33.110723 13465 data_layer.cpp:41] output data size: 10,1,32,32
I1209 11:24:33.110944 13465 net.cpp:150] Setting up label
I1209 11:24:33.110958 13465 net.cpp:157] Top shape: 10 1 32 32 (10240)
I1209 11:24:33.110970 13465 net.cpp:165] Memory required for data: 81920
I1209 11:24:33.110976 13465 layer_factory.hpp:77] Creating layer identity
I1209 11:24:33.110996 13465 net.cpp:100] Creating Layer identity
I1209 11:24:33.111004 13465 net.cpp:434] identity <- data
I1209 11:24:33.111026 13465 net.cpp:408] identity -> output
I1209 11:24:33.111158 13465 net.cpp:150] Setting up identity
I1209 11:24:33.111173 13465 net.cpp:157] Top shape: 10 1 32 32 (10240)
I1209 11:24:33.111182 13465 net.cpp:165] Memory required for data: 122880
I1209 11:24:33.111189 13465 layer_factory.hpp:77] Creating layer loss
I1209 11:24:33.111199 13465 net.cpp:100] Creating Layer loss
I1209 11:24:33.111209 13465 net.cpp:434] loss <- output
I1209 11:24:33.111217 13465 net.cpp:434] loss <- label
I1209 11:24:33.111227 13465 net.cpp:408] loss -> loss
I1209 11:24:33.111291 13465 net.cpp:150] Setting up loss
I1209 11:24:33.111304 13465 net.cpp:157] Top shape: (1)
I1209 11:24:33.111312 13465 net.cpp:160]     with loss weight 1
I1209 11:24:33.111349 13465 net.cpp:165] Memory required for data: 122884
I1209 11:24:33.111358 13465 net.cpp:228] loss does not need backward computation.
I1209 11:24:33.111366 13465 net.cpp:228] identity does not need backward computation.
I1209 11:24:33.111372 13465 net.cpp:228] label does not need backward computation.
I1209 11:24:33.111377 13465 net.cpp:228] data does not need backward computation.
I1209 11:24:33.111383 13465 net.cpp:270] This network produces output loss
I1209 11:24:33.111393 13465 net.cpp:283] Network initialization done.
I1209 11:24:33.112138 13465 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:24:33.112177 13465 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 11:24:33.112185 13465 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer label
I1209 11:24:33.112197 13465 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 127.61719
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112422-97b8/val_db/features"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112422-97b8/val_db/labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "identity"
  type: "Power"
  bottom: "data"
  top: "output"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:24:33.112321 13465 layer_factory.hpp:77] Creating layer data
I1209 11:24:33.112408 13465 net.cpp:100] Creating Layer data
I1209 11:24:33.112427 13465 net.cpp:408] data -> data
I1209 11:24:33.124158 13474 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112422-97b8/val_db/features
I1209 11:24:33.124399 13465 data_layer.cpp:41] output data size: 10,1,32,32
I1209 11:24:33.124610 13465 net.cpp:150] Setting up data
I1209 11:24:33.124624 13465 net.cpp:157] Top shape: 10 1 32 32 (10240)
I1209 11:24:33.124634 13465 net.cpp:165] Memory required for data: 40960
I1209 11:24:33.124641 13465 layer_factory.hpp:77] Creating layer label
I1209 11:24:33.124737 13465 net.cpp:100] Creating Layer label
I1209 11:24:33.124752 13465 net.cpp:408] label -> label
I1209 11:24:33.133612 13476 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112422-97b8/val_db/labels
I1209 11:24:33.133816 13465 data_layer.cpp:41] output data size: 10,1,32,32
I1209 11:24:33.134054 13465 net.cpp:150] Setting up label
I1209 11:24:33.134069 13465 net.cpp:157] Top shape: 10 1 32 32 (10240)
I1209 11:24:33.134080 13465 net.cpp:165] Memory required for data: 81920
I1209 11:24:33.134088 13465 layer_factory.hpp:77] Creating layer identity
I1209 11:24:33.134100 13465 net.cpp:100] Creating Layer identity
I1209 11:24:33.134109 13465 net.cpp:434] identity <- data
I1209 11:24:33.134117 13465 net.cpp:408] identity -> output
I1209 11:24:33.134176 13465 net.cpp:150] Setting up identity
I1209 11:24:33.134186 13465 net.cpp:157] Top shape: 10 1 32 32 (10240)
I1209 11:24:33.134199 13465 net.cpp:165] Memory required for data: 122880
I1209 11:24:33.134205 13465 layer_factory.hpp:77] Creating layer loss
I1209 11:24:33.134217 13465 net.cpp:100] Creating Layer loss
I1209 11:24:33.134222 13465 net.cpp:434] loss <- output
I1209 11:24:33.134228 13465 net.cpp:434] loss <- label
I1209 11:24:33.134237 13465 net.cpp:408] loss -> loss
I1209 11:24:33.134311 13465 net.cpp:150] Setting up loss
I1209 11:24:33.134322 13465 net.cpp:157] Top shape: (1)
I1209 11:24:33.134330 13465 net.cpp:160]     with loss weight 1
I1209 11:24:33.134341 13465 net.cpp:165] Memory required for data: 122884
I1209 11:24:33.134347 13465 net.cpp:228] loss does not need backward computation.
I1209 11:24:33.134354 13465 net.cpp:228] identity does not need backward computation.
I1209 11:24:33.134363 13465 net.cpp:228] label does not need backward computation.
I1209 11:24:33.134368 13465 net.cpp:228] data does not need backward computation.
I1209 11:24:33.134373 13465 net.cpp:270] This network produces output loss
I1209 11:24:33.134380 13465 net.cpp:283] Network initialization done.
I1209 11:24:33.134413 13465 solver.cpp:60] Solver scaffolding done.
I1209 11:24:33.134433 13465 caffe.cpp:251] Starting Optimization
I1209 11:24:33.134441 13465 solver.cpp:279] Solving 
I1209 11:24:33.134446 13465 solver.cpp:280] Learning Rate Policy: step
I1209 11:24:33.134459 13465 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:24:33.134572 13465 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:24:33.136417 13465 solver.cpp:404]     Test net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.137691 13465 solver.cpp:228] Iteration 0, loss = 8.33851e+06
I1209 11:24:33.137720 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.137750 13465 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:24:33.138353 13465 solver.cpp:228] Iteration 1, loss = 8.33851e+06
I1209 11:24:33.138376 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.138387 13465 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I1209 11:24:33.138619 13465 solver.cpp:228] Iteration 2, loss = 8.33851e+06
I1209 11:24:33.138640 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.138651 13465 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I1209 11:24:33.138902 13465 solver.cpp:228] Iteration 3, loss = 8.33851e+06
I1209 11:24:33.138923 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.138936 13465 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I1209 11:24:33.139168 13465 solver.cpp:228] Iteration 4, loss = 8.33851e+06
I1209 11:24:33.139189 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.139199 13465 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:24:33.139426 13465 solver.cpp:228] Iteration 5, loss = 8.33851e+06
I1209 11:24:33.139447 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.139458 13465 sgd_solver.cpp:106] Iteration 5, lr = 0.01
I1209 11:24:33.139710 13465 solver.cpp:228] Iteration 6, loss = 8.33851e+06
I1209 11:24:33.139732 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.139742 13465 sgd_solver.cpp:106] Iteration 6, lr = 0.01
I1209 11:24:33.139958 13465 solver.cpp:228] Iteration 7, loss = 8.33851e+06
I1209 11:24:33.139979 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.139989 13465 sgd_solver.cpp:106] Iteration 7, lr = 0.01
I1209 11:24:33.140601 13465 solver.cpp:228] Iteration 8, loss = 8.33851e+06
I1209 11:24:33.140622 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.140632 13465 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:24:33.140646 13465 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_9.caffemodel
I1209 11:24:33.143532 13465 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_9.solverstate
I1209 11:24:33.144299 13465 solver.cpp:337] Iteration 9, Testing net (#0)
I1209 11:24:33.144850 13465 solver.cpp:404]     Test net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.145141 13465 solver.cpp:228] Iteration 9, loss = 8.33851e+06
I1209 11:24:33.145164 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.145177 13465 sgd_solver.cpp:106] Iteration 9, lr = 0.001
I1209 11:24:33.145452 13465 solver.cpp:228] Iteration 10, loss = 8.33851e+06
I1209 11:24:33.145474 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.145496 13465 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1209 11:24:33.145730 13465 solver.cpp:228] Iteration 11, loss = 8.33851e+06
I1209 11:24:33.145762 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.145773 13465 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1209 11:24:33.145988 13465 solver.cpp:228] Iteration 12, loss = 8.33851e+06
I1209 11:24:33.146009 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.146019 13465 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I1209 11:24:33.146289 13465 solver.cpp:228] Iteration 13, loss = 8.33851e+06
I1209 11:24:33.146311 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.146320 13465 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1209 11:24:33.146549 13465 solver.cpp:228] Iteration 14, loss = 8.33851e+06
I1209 11:24:33.146572 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.146582 13465 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1209 11:24:33.147193 13465 solver.cpp:228] Iteration 15, loss = 8.33851e+06
I1209 11:24:33.147214 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.147225 13465 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I1209 11:24:33.147480 13465 solver.cpp:228] Iteration 16, loss = 8.33851e+06
I1209 11:24:33.147502 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.147512 13465 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I1209 11:24:33.147730 13465 solver.cpp:228] Iteration 17, loss = 8.33851e+06
I1209 11:24:33.147752 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.147761 13465 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I1209 11:24:33.147773 13465 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_18.caffemodel
I1209 11:24:33.148161 13465 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_18.solverstate
I1209 11:24:33.148505 13465 solver.cpp:337] Iteration 18, Testing net (#0)
I1209 11:24:33.148983 13465 solver.cpp:404]     Test net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.149256 13465 solver.cpp:228] Iteration 18, loss = 8.33851e+06
I1209 11:24:33.149278 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.149289 13465 sgd_solver.cpp:106] Iteration 18, lr = 0.0001
I1209 11:24:33.149539 13465 solver.cpp:228] Iteration 19, loss = 8.33851e+06
I1209 11:24:33.149565 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.149575 13465 sgd_solver.cpp:106] Iteration 19, lr = 0.0001
I1209 11:24:33.149816 13465 solver.cpp:228] Iteration 20, loss = 8.33851e+06
I1209 11:24:33.149838 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.149848 13465 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I1209 11:24:33.150090 13465 solver.cpp:228] Iteration 21, loss = 8.33851e+06
I1209 11:24:33.150112 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.150122 13465 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I1209 11:24:33.150369 13465 solver.cpp:228] Iteration 22, loss = 8.33851e+06
I1209 11:24:33.150391 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.150401 13465 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I1209 11:24:33.150645 13465 solver.cpp:228] Iteration 23, loss = 8.33851e+06
I1209 11:24:33.150665 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.150676 13465 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I1209 11:24:33.150890 13465 solver.cpp:228] Iteration 24, loss = 8.33851e+06
I1209 11:24:33.150913 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.150929 13465 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I1209 11:24:33.151610 13465 solver.cpp:228] Iteration 25, loss = 8.33851e+06
I1209 11:24:33.151633 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.151643 13465 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I1209 11:24:33.151901 13465 solver.cpp:228] Iteration 26, loss = 8.33851e+06
I1209 11:24:33.151922 13465 solver.cpp:244]     Train net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.151932 13465 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I1209 11:24:33.151944 13465 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_27.caffemodel
I1209 11:24:33.152436 13465 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_27.solverstate
I1209 11:24:33.153465 13465 solver.cpp:317] Iteration 27, loss = 8.33851e+06
I1209 11:24:33.153484 13465 solver.cpp:337] Iteration 27, Testing net (#0)
I1209 11:24:33.153766 13465 solver.cpp:404]     Test net output #0: loss = 8.33851e+06 (* 1 = 8.33851e+06 loss)
I1209 11:24:33.153790 13465 solver.cpp:322] Optimization Done.
I1209 11:24:33.153795 13465 caffe.cpp:254] Optimization Done.
