Log file created at: 2016/12/09 11:25:33
Running on machine: g008
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:25:33.599982 14150 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112532-1a58/solver.prototxt
I1209 11:25:33.601423 14150 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:25:33.601434 14150 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:25:33.602495 14150 caffe.cpp:217] Using GPUs 0
I1209 11:25:33.645215 14150 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:25:34.025131 14150 solver.cpp:48] Initializing solver from parameters: 
test_iter: 2
test_interval: 10
base_lr: 0.01
display: 1
max_iter: 30
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 10
snapshot: 10
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:25:34.026650 14150 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:25:34.027837 14150 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 11:25:34.027854 14150 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I1209 11:25:34.027878 14150 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:25:34.028107 14150 layer_factory.hpp:77] Creating layer data
I1209 11:25:34.028286 14150 net.cpp:100] Creating Layer data
I1209 11:25:34.028314 14150 net.cpp:408] data -> data
I1209 11:25:34.028384 14150 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:25:34.030764 14155 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:25:34.047240 14150 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:25:34.048498 14150 net.cpp:150] Setting up data
I1209 11:25:34.048519 14150 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:34.048568 14150 net.cpp:165] Memory required for data: 4000
I1209 11:25:34.048595 14150 layer_factory.hpp:77] Creating layer label
I1209 11:25:34.048691 14150 net.cpp:100] Creating Layer label
I1209 11:25:34.048709 14150 net.cpp:408] label -> label
I1209 11:25:34.052798 14157 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:25:34.052956 14150 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:25:34.053151 14150 net.cpp:150] Setting up label
I1209 11:25:34.053165 14150 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:25:34.053179 14150 net.cpp:165] Memory required for data: 4080
I1209 11:25:34.053186 14150 layer_factory.hpp:77] Creating layer scale
I1209 11:25:34.053202 14150 net.cpp:100] Creating Layer scale
I1209 11:25:34.053211 14150 net.cpp:434] scale <- data
I1209 11:25:34.053231 14150 net.cpp:408] scale -> scale
I1209 11:25:34.053333 14150 net.cpp:150] Setting up scale
I1209 11:25:34.053345 14150 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:34.053355 14150 net.cpp:165] Memory required for data: 8080
I1209 11:25:34.053361 14150 layer_factory.hpp:77] Creating layer hidden
I1209 11:25:34.053376 14150 net.cpp:100] Creating Layer hidden
I1209 11:25:34.053383 14150 net.cpp:434] hidden <- scale
I1209 11:25:34.053400 14150 net.cpp:408] hidden -> output
I1209 11:25:34.053582 14150 net.cpp:150] Setting up hidden
I1209 11:25:34.053594 14150 net.cpp:157] Top shape: 10 2 (20)
I1209 11:25:34.053603 14150 net.cpp:165] Memory required for data: 8160
I1209 11:25:34.053633 14150 layer_factory.hpp:77] Creating layer loss
I1209 11:25:34.053645 14150 net.cpp:100] Creating Layer loss
I1209 11:25:34.053652 14150 net.cpp:434] loss <- output
I1209 11:25:34.053658 14150 net.cpp:434] loss <- label
I1209 11:25:34.053668 14150 net.cpp:408] loss -> loss
I1209 11:25:34.053735 14150 net.cpp:150] Setting up loss
I1209 11:25:34.053747 14150 net.cpp:157] Top shape: (1)
I1209 11:25:34.053762 14150 net.cpp:160]     with loss weight 1
I1209 11:25:34.053802 14150 net.cpp:165] Memory required for data: 8164
I1209 11:25:34.053810 14150 net.cpp:226] loss needs backward computation.
I1209 11:25:34.053818 14150 net.cpp:226] hidden needs backward computation.
I1209 11:25:34.053824 14150 net.cpp:228] scale does not need backward computation.
I1209 11:25:34.053830 14150 net.cpp:228] label does not need backward computation.
I1209 11:25:34.053834 14150 net.cpp:228] data does not need backward computation.
I1209 11:25:34.053843 14150 net.cpp:270] This network produces output loss
I1209 11:25:34.053853 14150 net.cpp:283] Network initialization done.
I1209 11:25:34.054405 14150 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:25:34.054447 14150 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 11:25:34.054456 14150 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer label
I1209 11:25:34.054468 14150 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:25:34.054620 14150 layer_factory.hpp:77] Creating layer data
I1209 11:25:34.054719 14150 net.cpp:100] Creating Layer data
I1209 11:25:34.054736 14150 net.cpp:408] data -> data
I1209 11:25:34.054754 14150 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:25:34.056907 14159 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:25:34.057101 14150 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:25:34.057339 14150 net.cpp:150] Setting up data
I1209 11:25:34.057353 14150 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:34.057363 14150 net.cpp:165] Memory required for data: 4000
I1209 11:25:34.057371 14150 layer_factory.hpp:77] Creating layer label
I1209 11:25:34.057454 14150 net.cpp:100] Creating Layer label
I1209 11:25:34.057472 14150 net.cpp:408] label -> label
I1209 11:25:34.060027 14161 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:25:34.060269 14150 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:25:34.060485 14150 net.cpp:150] Setting up label
I1209 11:25:34.060500 14150 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:25:34.060511 14150 net.cpp:165] Memory required for data: 4080
I1209 11:25:34.060520 14150 layer_factory.hpp:77] Creating layer scale
I1209 11:25:34.060533 14150 net.cpp:100] Creating Layer scale
I1209 11:25:34.060544 14150 net.cpp:434] scale <- data
I1209 11:25:34.060552 14150 net.cpp:408] scale -> scale
I1209 11:25:34.060663 14150 net.cpp:150] Setting up scale
I1209 11:25:34.060677 14150 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:34.060686 14150 net.cpp:165] Memory required for data: 8080
I1209 11:25:34.060693 14150 layer_factory.hpp:77] Creating layer hidden
I1209 11:25:34.060705 14150 net.cpp:100] Creating Layer hidden
I1209 11:25:34.060711 14150 net.cpp:434] hidden <- scale
I1209 11:25:34.060720 14150 net.cpp:408] hidden -> output
I1209 11:25:34.060850 14150 net.cpp:150] Setting up hidden
I1209 11:25:34.060861 14150 net.cpp:157] Top shape: 10 2 (20)
I1209 11:25:34.060870 14150 net.cpp:165] Memory required for data: 8160
I1209 11:25:34.060886 14150 layer_factory.hpp:77] Creating layer loss
I1209 11:25:34.060902 14150 net.cpp:100] Creating Layer loss
I1209 11:25:34.060909 14150 net.cpp:434] loss <- output
I1209 11:25:34.060915 14150 net.cpp:434] loss <- label
I1209 11:25:34.060923 14150 net.cpp:408] loss -> loss
I1209 11:25:34.060976 14150 net.cpp:150] Setting up loss
I1209 11:25:34.060986 14150 net.cpp:157] Top shape: (1)
I1209 11:25:34.060994 14150 net.cpp:160]     with loss weight 1
I1209 11:25:34.061004 14150 net.cpp:165] Memory required for data: 8164
I1209 11:25:34.061010 14150 net.cpp:226] loss needs backward computation.
I1209 11:25:34.061017 14150 net.cpp:226] hidden needs backward computation.
I1209 11:25:34.061022 14150 net.cpp:228] scale does not need backward computation.
I1209 11:25:34.061028 14150 net.cpp:228] label does not need backward computation.
I1209 11:25:34.061034 14150 net.cpp:228] data does not need backward computation.
I1209 11:25:34.061038 14150 net.cpp:270] This network produces output loss
I1209 11:25:34.061048 14150 net.cpp:283] Network initialization done.
I1209 11:25:34.061089 14150 solver.cpp:60] Solver scaffolding done.
I1209 11:25:34.061215 14150 caffe.cpp:155] Finetuning from /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112531-36ff/snapshot_iter_30.caffemodel
I1209 11:25:34.061849 14150 caffe.cpp:251] Starting Optimization
I1209 11:25:34.061875 14150 solver.cpp:279] Solving 
I1209 11:25:34.061882 14150 solver.cpp:280] Learning Rate Policy: step
I1209 11:25:34.062049 14150 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:25:34.063237 14150 solver.cpp:404]     Test net output #0: loss = 0.0103879 (* 1 = 0.0103879 loss)
I1209 11:25:34.063907 14150 solver.cpp:228] Iteration 0, loss = 0.00964794
I1209 11:25:34.063932 14150 solver.cpp:244]     Train net output #0: loss = 0.00964794 (* 1 = 0.00964794 loss)
I1209 11:25:34.063963 14150 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:25:34.064499 14150 solver.cpp:228] Iteration 1, loss = 0.00840117
I1209 11:25:34.064522 14150 solver.cpp:244]     Train net output #0: loss = 0.00840117 (* 1 = 0.00840117 loss)
I1209 11:25:34.064532 14150 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I1209 11:25:34.064972 14150 solver.cpp:228] Iteration 2, loss = 0.010222
I1209 11:25:34.064995 14150 solver.cpp:244]     Train net output #0: loss = 0.010222 (* 1 = 0.010222 loss)
I1209 11:25:34.065004 14150 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I1209 11:25:34.065443 14150 solver.cpp:228] Iteration 3, loss = 0.00659591
I1209 11:25:34.065464 14150 solver.cpp:244]     Train net output #0: loss = 0.00659591 (* 1 = 0.00659591 loss)
I1209 11:25:34.065474 14150 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I1209 11:25:34.065928 14150 solver.cpp:228] Iteration 4, loss = 0.00981481
I1209 11:25:34.065951 14150 solver.cpp:244]     Train net output #0: loss = 0.00981481 (* 1 = 0.00981481 loss)
I1209 11:25:34.065961 14150 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:25:34.066395 14150 solver.cpp:228] Iteration 5, loss = 0.00769824
I1209 11:25:34.066416 14150 solver.cpp:244]     Train net output #0: loss = 0.00769824 (* 1 = 0.00769824 loss)
I1209 11:25:34.066426 14150 sgd_solver.cpp:106] Iteration 5, lr = 0.01
I1209 11:25:34.066843 14150 solver.cpp:228] Iteration 6, loss = 0.0069255
I1209 11:25:34.066864 14150 solver.cpp:244]     Train net output #0: loss = 0.0069255 (* 1 = 0.0069255 loss)
I1209 11:25:34.066880 14150 sgd_solver.cpp:106] Iteration 6, lr = 0.01
I1209 11:25:34.067314 14150 solver.cpp:228] Iteration 7, loss = 0.00741194
I1209 11:25:34.067335 14150 solver.cpp:244]     Train net output #0: loss = 0.00741194 (* 1 = 0.00741194 loss)
I1209 11:25:34.067345 14150 sgd_solver.cpp:106] Iteration 7, lr = 0.01
I1209 11:25:34.067770 14150 solver.cpp:228] Iteration 8, loss = 0.00524921
I1209 11:25:34.067791 14150 solver.cpp:244]     Train net output #0: loss = 0.00524921 (* 1 = 0.00524921 loss)
I1209 11:25:34.067800 14150 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:25:34.068218 14150 solver.cpp:228] Iteration 9, loss = 0.00565211
I1209 11:25:34.068240 14150 solver.cpp:244]     Train net output #0: loss = 0.00565211 (* 1 = 0.00565211 loss)
I1209 11:25:34.068249 14150 sgd_solver.cpp:106] Iteration 9, lr = 0.01
I1209 11:25:34.068325 14150 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_10.caffemodel
I1209 11:25:34.069164 14150 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_10.solverstate
I1209 11:25:34.069773 14150 solver.cpp:337] Iteration 10, Testing net (#0)
I1209 11:25:34.070420 14150 solver.cpp:404]     Test net output #0: loss = 0.00531192 (* 1 = 0.00531192 loss)
I1209 11:25:34.070837 14150 solver.cpp:228] Iteration 10, loss = 0.00485462
I1209 11:25:34.070858 14150 solver.cpp:244]     Train net output #0: loss = 0.00485462 (* 1 = 0.00485462 loss)
I1209 11:25:34.070871 14150 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1209 11:25:34.071327 14150 solver.cpp:228] Iteration 11, loss = 0.00370071
I1209 11:25:34.071349 14150 solver.cpp:244]     Train net output #0: loss = 0.00370071 (* 1 = 0.00370071 loss)
I1209 11:25:34.071358 14150 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1209 11:25:34.071802 14150 solver.cpp:228] Iteration 12, loss = 0.00424462
I1209 11:25:34.071825 14150 solver.cpp:244]     Train net output #0: loss = 0.00424462 (* 1 = 0.00424462 loss)
I1209 11:25:34.071835 14150 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I1209 11:25:34.072284 14150 solver.cpp:228] Iteration 13, loss = 0.00251463
I1209 11:25:34.072306 14150 solver.cpp:244]     Train net output #0: loss = 0.00251463 (* 1 = 0.00251463 loss)
I1209 11:25:34.072315 14150 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1209 11:25:34.072748 14150 solver.cpp:228] Iteration 14, loss = 0.00376641
I1209 11:25:34.072769 14150 solver.cpp:244]     Train net output #0: loss = 0.00376641 (* 1 = 0.00376641 loss)
I1209 11:25:34.072778 14150 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1209 11:25:34.073212 14150 solver.cpp:228] Iteration 15, loss = 0.00269244
I1209 11:25:34.073235 14150 solver.cpp:244]     Train net output #0: loss = 0.00269244 (* 1 = 0.00269244 loss)
I1209 11:25:34.073243 14150 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I1209 11:25:34.073680 14150 solver.cpp:228] Iteration 16, loss = 0.00236764
I1209 11:25:34.073701 14150 solver.cpp:244]     Train net output #0: loss = 0.00236764 (* 1 = 0.00236764 loss)
I1209 11:25:34.073711 14150 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I1209 11:25:34.074158 14150 solver.cpp:228] Iteration 17, loss = 0.00268838
I1209 11:25:34.074180 14150 solver.cpp:244]     Train net output #0: loss = 0.00268838 (* 1 = 0.00268838 loss)
I1209 11:25:34.074189 14150 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I1209 11:25:34.074620 14150 solver.cpp:228] Iteration 18, loss = 0.00197448
I1209 11:25:34.074640 14150 solver.cpp:244]     Train net output #0: loss = 0.00197448 (* 1 = 0.00197448 loss)
I1209 11:25:34.074651 14150 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I1209 11:25:34.075086 14150 solver.cpp:228] Iteration 19, loss = 0.00213392
I1209 11:25:34.075109 14150 solver.cpp:244]     Train net output #0: loss = 0.00213392 (* 1 = 0.00213392 loss)
I1209 11:25:34.075117 14150 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I1209 11:25:34.075189 14150 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_20.caffemodel
I1209 11:25:34.075781 14150 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_20.solverstate
I1209 11:25:34.076241 14150 solver.cpp:337] Iteration 20, Testing net (#0)
I1209 11:25:34.076879 14150 solver.cpp:404]     Test net output #0: loss = 0.0020513 (* 1 = 0.0020513 loss)
I1209 11:25:34.077273 14150 solver.cpp:228] Iteration 20, loss = 0.00190715
I1209 11:25:34.077296 14150 solver.cpp:244]     Train net output #0: loss = 0.00190715 (* 1 = 0.00190715 loss)
I1209 11:25:34.077306 14150 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I1209 11:25:34.077739 14150 solver.cpp:228] Iteration 21, loss = 0.00149779
I1209 11:25:34.077775 14150 solver.cpp:244]     Train net output #0: loss = 0.00149779 (* 1 = 0.00149779 loss)
I1209 11:25:34.077787 14150 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I1209 11:25:34.078244 14150 solver.cpp:228] Iteration 22, loss = 0.00178292
I1209 11:25:34.078265 14150 solver.cpp:244]     Train net output #0: loss = 0.00178292 (* 1 = 0.00178292 loss)
I1209 11:25:34.078275 14150 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I1209 11:25:34.078709 14150 solver.cpp:228] Iteration 23, loss = 0.00106488
I1209 11:25:34.078730 14150 solver.cpp:244]     Train net output #0: loss = 0.00106488 (* 1 = 0.00106488 loss)
I1209 11:25:34.078740 14150 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I1209 11:25:34.079166 14150 solver.cpp:228] Iteration 24, loss = 0.00173454
I1209 11:25:34.079188 14150 solver.cpp:244]     Train net output #0: loss = 0.00173454 (* 1 = 0.00173454 loss)
I1209 11:25:34.079197 14150 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I1209 11:25:34.079627 14150 solver.cpp:228] Iteration 25, loss = 0.00125056
I1209 11:25:34.079648 14150 solver.cpp:244]     Train net output #0: loss = 0.00125056 (* 1 = 0.00125056 loss)
I1209 11:25:34.079658 14150 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I1209 11:25:34.080094 14150 solver.cpp:228] Iteration 26, loss = 0.00114636
I1209 11:25:34.080116 14150 solver.cpp:244]     Train net output #0: loss = 0.00114636 (* 1 = 0.00114636 loss)
I1209 11:25:34.080124 14150 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I1209 11:25:34.080559 14150 solver.cpp:228] Iteration 27, loss = 0.00141347
I1209 11:25:34.080579 14150 solver.cpp:244]     Train net output #0: loss = 0.00141347 (* 1 = 0.00141347 loss)
I1209 11:25:34.080590 14150 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I1209 11:25:34.081020 14150 solver.cpp:228] Iteration 28, loss = 0.00108869
I1209 11:25:34.081043 14150 solver.cpp:244]     Train net output #0: loss = 0.00108869 (* 1 = 0.00108869 loss)
I1209 11:25:34.081053 14150 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I1209 11:25:34.081487 14150 solver.cpp:228] Iteration 29, loss = 0.00120544
I1209 11:25:34.081509 14150 solver.cpp:244]     Train net output #0: loss = 0.00120544 (* 1 = 0.00120544 loss)
I1209 11:25:34.081518 14150 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I1209 11:25:34.081591 14150 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_30.caffemodel
I1209 11:25:34.082182 14150 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_30.solverstate
I1209 11:25:34.082901 14150 solver.cpp:317] Iteration 30, loss = 0.00113113
I1209 11:25:34.082919 14150 solver.cpp:337] Iteration 30, Testing net (#0)
I1209 11:25:34.083567 14150 solver.cpp:404]     Test net output #0: loss = 0.00119825 (* 1 = 0.00119825 loss)
I1209 11:25:34.083583 14150 solver.cpp:322] Optimization Done.
I1209 11:25:34.083588 14150 caffe.cpp:254] Optimization Done.
