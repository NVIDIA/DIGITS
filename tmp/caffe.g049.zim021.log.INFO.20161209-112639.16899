Log file created at: 2016/12/09 11:26:39
Running on machine: g049
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:26:39.592289 16899 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112636-ccb2/solver.prototxt
I1209 11:26:39.594163 16899 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:26:39.594174 16899 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:26:39.595434 16899 caffe.cpp:217] Using GPUs 0
I1209 11:26:39.640189 16899 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:26:40.023138 16899 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1
test_interval: 3
base_lr: 0.01
display: 1
max_iter: 3
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 1
snapshot: 0
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:26:40.023389 16899 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:26:40.024451 16899 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 11:26:40.024474 16899 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1209 11:26:40.024492 16899 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112627-3556/mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112627-3556/train_db"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "data"
  top: "output"
  inner_product_param {
    num_output: 3
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:26:40.024662 16899 layer_factory.hpp:77] Creating layer data
I1209 11:26:40.024827 16899 net.cpp:100] Creating Layer data
I1209 11:26:40.024844 16899 net.cpp:408] data -> data
I1209 11:26:40.024880 16899 net.cpp:408] data -> label
I1209 11:26:40.024905 16899 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112627-3556/mean.binaryproto
I1209 11:26:40.027544 16917 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112627-3556/train_db
I1209 11:26:40.044811 16899 data_layer.cpp:41] output data size: 10,3,10,10
I1209 11:26:40.046269 16899 net.cpp:150] Setting up data
I1209 11:26:40.046301 16899 net.cpp:157] Top shape: 10 3 10 10 (3000)
I1209 11:26:40.046358 16899 net.cpp:157] Top shape: 10 (10)
I1209 11:26:40.046367 16899 net.cpp:165] Memory required for data: 12040
I1209 11:26:40.046387 16899 layer_factory.hpp:77] Creating layer hidden
I1209 11:26:40.046412 16899 net.cpp:100] Creating Layer hidden
I1209 11:26:40.046423 16899 net.cpp:434] hidden <- data
I1209 11:26:40.046442 16899 net.cpp:408] hidden -> output
I1209 11:26:40.046627 16899 net.cpp:150] Setting up hidden
I1209 11:26:40.046639 16899 net.cpp:157] Top shape: 10 3 (30)
I1209 11:26:40.046648 16899 net.cpp:165] Memory required for data: 12160
I1209 11:26:40.046679 16899 layer_factory.hpp:77] Creating layer loss
I1209 11:26:40.046694 16899 net.cpp:100] Creating Layer loss
I1209 11:26:40.046700 16899 net.cpp:434] loss <- output
I1209 11:26:40.046706 16899 net.cpp:434] loss <- label
I1209 11:26:40.046716 16899 net.cpp:408] loss -> loss
I1209 11:26:40.046735 16899 layer_factory.hpp:77] Creating layer loss
I1209 11:26:40.376042 16899 net.cpp:150] Setting up loss
I1209 11:26:40.376085 16899 net.cpp:157] Top shape: (1)
I1209 11:26:40.376101 16899 net.cpp:160]     with loss weight 1
I1209 11:26:40.376145 16899 net.cpp:165] Memory required for data: 12164
I1209 11:26:40.376157 16899 net.cpp:226] loss needs backward computation.
I1209 11:26:40.376168 16899 net.cpp:226] hidden needs backward computation.
I1209 11:26:40.376173 16899 net.cpp:228] data does not need backward computation.
I1209 11:26:40.376178 16899 net.cpp:270] This network produces output loss
I1209 11:26:40.376194 16899 net.cpp:283] Network initialization done.
I1209 11:26:40.376791 16899 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:26:40.376832 16899 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 11:26:40.376845 16899 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112627-3556/mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112627-3556/val_db"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "data"
  top: "output"
  inner_product_param {
    num_output: 3
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "output"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
I1209 11:26:40.377025 16899 layer_factory.hpp:77] Creating layer data
I1209 11:26:40.377142 16899 net.cpp:100] Creating Layer data
I1209 11:26:40.377158 16899 net.cpp:408] data -> data
I1209 11:26:40.377177 16899 net.cpp:408] data -> label
I1209 11:26:40.377192 16899 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112627-3556/mean.binaryproto
I1209 11:26:40.381091 16919 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112627-3556/val_db
I1209 11:26:40.381367 16899 data_layer.cpp:41] output data size: 10,3,10,10
I1209 11:26:40.381808 16899 net.cpp:150] Setting up data
I1209 11:26:40.381824 16899 net.cpp:157] Top shape: 10 3 10 10 (3000)
I1209 11:26:40.381836 16899 net.cpp:157] Top shape: 10 (10)
I1209 11:26:40.381844 16899 net.cpp:165] Memory required for data: 12040
I1209 11:26:40.381851 16899 layer_factory.hpp:77] Creating layer label_data_1_split
I1209 11:26:40.381868 16899 net.cpp:100] Creating Layer label_data_1_split
I1209 11:26:40.381875 16899 net.cpp:434] label_data_1_split <- label
I1209 11:26:40.381885 16899 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1209 11:26:40.381898 16899 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1209 11:26:40.382001 16899 net.cpp:150] Setting up label_data_1_split
I1209 11:26:40.382012 16899 net.cpp:157] Top shape: 10 (10)
I1209 11:26:40.382026 16899 net.cpp:157] Top shape: 10 (10)
I1209 11:26:40.382032 16899 net.cpp:165] Memory required for data: 12120
I1209 11:26:40.382040 16899 layer_factory.hpp:77] Creating layer hidden
I1209 11:26:40.382050 16899 net.cpp:100] Creating Layer hidden
I1209 11:26:40.382055 16899 net.cpp:434] hidden <- data
I1209 11:26:40.382064 16899 net.cpp:408] hidden -> output
I1209 11:26:40.382222 16899 net.cpp:150] Setting up hidden
I1209 11:26:40.382235 16899 net.cpp:157] Top shape: 10 3 (30)
I1209 11:26:40.382242 16899 net.cpp:165] Memory required for data: 12240
I1209 11:26:40.382263 16899 layer_factory.hpp:77] Creating layer output_hidden_0_split
I1209 11:26:40.382272 16899 net.cpp:100] Creating Layer output_hidden_0_split
I1209 11:26:40.382278 16899 net.cpp:434] output_hidden_0_split <- output
I1209 11:26:40.382287 16899 net.cpp:408] output_hidden_0_split -> output_hidden_0_split_0
I1209 11:26:40.382300 16899 net.cpp:408] output_hidden_0_split -> output_hidden_0_split_1
I1209 11:26:40.382351 16899 net.cpp:150] Setting up output_hidden_0_split
I1209 11:26:40.382361 16899 net.cpp:157] Top shape: 10 3 (30)
I1209 11:26:40.382370 16899 net.cpp:157] Top shape: 10 3 (30)
I1209 11:26:40.382376 16899 net.cpp:165] Memory required for data: 12480
I1209 11:26:40.382382 16899 layer_factory.hpp:77] Creating layer loss
I1209 11:26:40.382395 16899 net.cpp:100] Creating Layer loss
I1209 11:26:40.382401 16899 net.cpp:434] loss <- output_hidden_0_split_0
I1209 11:26:40.382408 16899 net.cpp:434] loss <- label_data_1_split_0
I1209 11:26:40.382421 16899 net.cpp:408] loss -> loss
I1209 11:26:40.382433 16899 layer_factory.hpp:77] Creating layer loss
I1209 11:26:40.382829 16899 net.cpp:150] Setting up loss
I1209 11:26:40.382843 16899 net.cpp:157] Top shape: (1)
I1209 11:26:40.382860 16899 net.cpp:160]     with loss weight 1
I1209 11:26:40.382869 16899 net.cpp:165] Memory required for data: 12484
I1209 11:26:40.382875 16899 layer_factory.hpp:77] Creating layer accuracy
I1209 11:26:40.382890 16899 net.cpp:100] Creating Layer accuracy
I1209 11:26:40.382896 16899 net.cpp:434] accuracy <- output_hidden_0_split_1
I1209 11:26:40.382903 16899 net.cpp:434] accuracy <- label_data_1_split_1
I1209 11:26:40.382915 16899 net.cpp:408] accuracy -> accuracy
I1209 11:26:40.382933 16899 net.cpp:150] Setting up accuracy
I1209 11:26:40.382942 16899 net.cpp:157] Top shape: (1)
I1209 11:26:40.382951 16899 net.cpp:165] Memory required for data: 12488
I1209 11:26:40.382956 16899 net.cpp:228] accuracy does not need backward computation.
I1209 11:26:40.382962 16899 net.cpp:226] loss needs backward computation.
I1209 11:26:40.382968 16899 net.cpp:226] output_hidden_0_split needs backward computation.
I1209 11:26:40.382974 16899 net.cpp:226] hidden needs backward computation.
I1209 11:26:40.382980 16899 net.cpp:228] label_data_1_split does not need backward computation.
I1209 11:26:40.382987 16899 net.cpp:228] data does not need backward computation.
I1209 11:26:40.382992 16899 net.cpp:270] This network produces output accuracy
I1209 11:26:40.382997 16899 net.cpp:270] This network produces output loss
I1209 11:26:40.383008 16899 net.cpp:283] Network initialization done.
I1209 11:26:40.383044 16899 solver.cpp:60] Solver scaffolding done.
I1209 11:26:40.383177 16899 caffe.cpp:251] Starting Optimization
I1209 11:26:40.383188 16899 solver.cpp:279] Solving 
I1209 11:26:40.383193 16899 solver.cpp:280] Learning Rate Policy: step
I1209 11:26:40.383582 16899 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:26:40.383687 16899 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:26:40.385046 16899 solver.cpp:404]     Test net output #0: accuracy = 0.3
I1209 11:26:40.385076 16899 solver.cpp:404]     Test net output #1: loss = 1.09861 (* 1 = 1.09861 loss)
I1209 11:26:40.385829 16899 solver.cpp:228] Iteration 0, loss = 1.09861
I1209 11:26:40.385856 16899 solver.cpp:244]     Train net output #0: loss = 1.09861 (* 1 = 1.09861 loss)
I1209 11:26:40.385888 16899 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:26:40.386463 16899 solver.cpp:228] Iteration 1, loss = 0
I1209 11:26:40.386484 16899 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:26:40.386497 16899 sgd_solver.cpp:106] Iteration 1, lr = 0.001
I1209 11:26:40.386973 16899 solver.cpp:228] Iteration 2, loss = 0
I1209 11:26:40.386994 16899 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:26:40.387003 16899 sgd_solver.cpp:106] Iteration 2, lr = 0.0001
I1209 11:26:40.387087 16899 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_3.caffemodel
I1209 11:26:40.388522 16899 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_3.solverstate
I1209 11:26:40.389575 16899 solver.cpp:317] Iteration 3, loss = 0
I1209 11:26:40.389593 16899 solver.cpp:337] Iteration 3, Testing net (#0)
I1209 11:26:40.389988 16899 solver.cpp:404]     Test net output #0: accuracy = 1
I1209 11:26:40.390009 16899 solver.cpp:404]     Test net output #1: loss = 0 (* 1 = 0 loss)
I1209 11:26:40.390017 16899 solver.cpp:322] Optimization Done.
I1209 11:26:40.390022 16899 caffe.cpp:254] Optimization Done.
