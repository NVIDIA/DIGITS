Log file created at: 2016/12/09 11:25:53
Running on machine: g008
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:25:53.452749 14823 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112552-30d0/solver.prototxt
I1209 11:25:53.454210 14823 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:25:53.454221 14823 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:25:53.455183 14823 caffe.cpp:217] Using GPUs 0
I1209 11:25:53.497969 14823 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:25:53.879477 14823 solver.cpp:48] Initializing solver from parameters: 
test_iter: 2
test_interval: 10
base_lr: 0.01
display: 1
max_iter: 30
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 10
snapshot: 5
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:25:53.880847 14823 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:25:53.881952 14823 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 11:25:53.881968 14823 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I1209 11:25:53.881991 14823 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:25:53.882217 14823 layer_factory.hpp:77] Creating layer data
I1209 11:25:53.882400 14823 net.cpp:100] Creating Layer data
I1209 11:25:53.882438 14823 net.cpp:408] data -> data
I1209 11:25:53.882506 14823 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:25:53.884901 14829 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:25:53.901262 14823 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:25:53.902534 14823 net.cpp:150] Setting up data
I1209 11:25:53.902568 14823 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:53.902601 14823 net.cpp:165] Memory required for data: 4000
I1209 11:25:53.902629 14823 layer_factory.hpp:77] Creating layer label
I1209 11:25:53.902717 14823 net.cpp:100] Creating Layer label
I1209 11:25:53.902734 14823 net.cpp:408] label -> label
I1209 11:25:53.906576 14831 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:25:53.906735 14823 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:25:53.906936 14823 net.cpp:150] Setting up label
I1209 11:25:53.906950 14823 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:25:53.906961 14823 net.cpp:165] Memory required for data: 4080
I1209 11:25:53.906968 14823 layer_factory.hpp:77] Creating layer scale
I1209 11:25:53.906985 14823 net.cpp:100] Creating Layer scale
I1209 11:25:53.906994 14823 net.cpp:434] scale <- data
I1209 11:25:53.907018 14823 net.cpp:408] scale -> scale
I1209 11:25:53.907094 14823 net.cpp:150] Setting up scale
I1209 11:25:53.907105 14823 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:53.907114 14823 net.cpp:165] Memory required for data: 8080
I1209 11:25:53.907120 14823 layer_factory.hpp:77] Creating layer hidden
I1209 11:25:53.907135 14823 net.cpp:100] Creating Layer hidden
I1209 11:25:53.907141 14823 net.cpp:434] hidden <- scale
I1209 11:25:53.907153 14823 net.cpp:408] hidden -> output
I1209 11:25:53.907332 14823 net.cpp:150] Setting up hidden
I1209 11:25:53.907344 14823 net.cpp:157] Top shape: 10 2 (20)
I1209 11:25:53.907364 14823 net.cpp:165] Memory required for data: 8160
I1209 11:25:53.907394 14823 layer_factory.hpp:77] Creating layer loss
I1209 11:25:53.907409 14823 net.cpp:100] Creating Layer loss
I1209 11:25:53.907414 14823 net.cpp:434] loss <- output
I1209 11:25:53.907421 14823 net.cpp:434] loss <- label
I1209 11:25:53.907431 14823 net.cpp:408] loss -> loss
I1209 11:25:53.907496 14823 net.cpp:150] Setting up loss
I1209 11:25:53.907510 14823 net.cpp:157] Top shape: (1)
I1209 11:25:53.907517 14823 net.cpp:160]     with loss weight 1
I1209 11:25:53.907549 14823 net.cpp:165] Memory required for data: 8164
I1209 11:25:53.907558 14823 net.cpp:226] loss needs backward computation.
I1209 11:25:53.907567 14823 net.cpp:226] hidden needs backward computation.
I1209 11:25:53.907572 14823 net.cpp:228] scale does not need backward computation.
I1209 11:25:53.907577 14823 net.cpp:228] label does not need backward computation.
I1209 11:25:53.907582 14823 net.cpp:228] data does not need backward computation.
I1209 11:25:53.907588 14823 net.cpp:270] This network produces output loss
I1209 11:25:53.907608 14823 net.cpp:283] Network initialization done.
I1209 11:25:53.908170 14823 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:25:53.908206 14823 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 11:25:53.908215 14823 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer label
I1209 11:25:53.908226 14823 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:25:53.908363 14823 layer_factory.hpp:77] Creating layer data
I1209 11:25:53.908445 14823 net.cpp:100] Creating Layer data
I1209 11:25:53.908462 14823 net.cpp:408] data -> data
I1209 11:25:53.908491 14823 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:25:53.910701 14833 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:25:53.910924 14823 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:25:53.911146 14823 net.cpp:150] Setting up data
I1209 11:25:53.911161 14823 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:53.911172 14823 net.cpp:165] Memory required for data: 4000
I1209 11:25:53.911180 14823 layer_factory.hpp:77] Creating layer label
I1209 11:25:53.911274 14823 net.cpp:100] Creating Layer label
I1209 11:25:53.911289 14823 net.cpp:408] label -> label
I1209 11:25:53.914122 14835 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:25:53.914319 14823 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:25:53.914579 14823 net.cpp:150] Setting up label
I1209 11:25:53.914597 14823 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:25:53.914607 14823 net.cpp:165] Memory required for data: 4080
I1209 11:25:53.914616 14823 layer_factory.hpp:77] Creating layer scale
I1209 11:25:53.914633 14823 net.cpp:100] Creating Layer scale
I1209 11:25:53.914645 14823 net.cpp:434] scale <- data
I1209 11:25:53.914655 14823 net.cpp:408] scale -> scale
I1209 11:25:53.914784 14823 net.cpp:150] Setting up scale
I1209 11:25:53.914806 14823 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:53.914818 14823 net.cpp:165] Memory required for data: 8080
I1209 11:25:53.914825 14823 layer_factory.hpp:77] Creating layer hidden
I1209 11:25:53.914836 14823 net.cpp:100] Creating Layer hidden
I1209 11:25:53.914845 14823 net.cpp:434] hidden <- scale
I1209 11:25:53.914858 14823 net.cpp:408] hidden -> output
I1209 11:25:53.915000 14823 net.cpp:150] Setting up hidden
I1209 11:25:53.915012 14823 net.cpp:157] Top shape: 10 2 (20)
I1209 11:25:53.915021 14823 net.cpp:165] Memory required for data: 8160
I1209 11:25:53.915038 14823 layer_factory.hpp:77] Creating layer loss
I1209 11:25:53.915050 14823 net.cpp:100] Creating Layer loss
I1209 11:25:53.915058 14823 net.cpp:434] loss <- output
I1209 11:25:53.915065 14823 net.cpp:434] loss <- label
I1209 11:25:53.915076 14823 net.cpp:408] loss -> loss
I1209 11:25:53.915127 14823 net.cpp:150] Setting up loss
I1209 11:25:53.915136 14823 net.cpp:157] Top shape: (1)
I1209 11:25:53.915148 14823 net.cpp:160]     with loss weight 1
I1209 11:25:53.915161 14823 net.cpp:165] Memory required for data: 8164
I1209 11:25:53.915168 14823 net.cpp:226] loss needs backward computation.
I1209 11:25:53.915174 14823 net.cpp:226] hidden needs backward computation.
I1209 11:25:53.915179 14823 net.cpp:228] scale does not need backward computation.
I1209 11:25:53.915185 14823 net.cpp:228] label does not need backward computation.
I1209 11:25:53.915190 14823 net.cpp:228] data does not need backward computation.
I1209 11:25:53.915194 14823 net.cpp:270] This network produces output loss
I1209 11:25:53.915202 14823 net.cpp:283] Network initialization done.
I1209 11:25:53.915240 14823 solver.cpp:60] Solver scaffolding done.
I1209 11:25:53.915351 14823 caffe.cpp:251] Starting Optimization
I1209 11:25:53.915364 14823 solver.cpp:279] Solving 
I1209 11:25:53.915369 14823 solver.cpp:280] Learning Rate Policy: step
I1209 11:25:53.915485 14823 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:25:53.915644 14823 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:25:53.916963 14823 solver.cpp:404]     Test net output #0: loss = 0.0933352 (* 1 = 0.0933352 loss)
I1209 11:25:53.917639 14823 solver.cpp:228] Iteration 0, loss = 0.0855033
I1209 11:25:53.917666 14823 solver.cpp:244]     Train net output #0: loss = 0.0855033 (* 1 = 0.0855033 loss)
I1209 11:25:53.917695 14823 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:25:53.918226 14823 solver.cpp:228] Iteration 1, loss = 0.0785537
I1209 11:25:53.918251 14823 solver.cpp:244]     Train net output #0: loss = 0.0785537 (* 1 = 0.0785537 loss)
I1209 11:25:53.918261 14823 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I1209 11:25:53.918692 14823 solver.cpp:228] Iteration 2, loss = 0.0945713
I1209 11:25:53.918715 14823 solver.cpp:244]     Train net output #0: loss = 0.0945713 (* 1 = 0.0945713 loss)
I1209 11:25:53.918725 14823 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I1209 11:25:53.919154 14823 solver.cpp:228] Iteration 3, loss = 0.0646118
I1209 11:25:53.919175 14823 solver.cpp:244]     Train net output #0: loss = 0.0646118 (* 1 = 0.0646118 loss)
I1209 11:25:53.919185 14823 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I1209 11:25:53.919616 14823 solver.cpp:228] Iteration 4, loss = 0.0857749
I1209 11:25:53.919637 14823 solver.cpp:244]     Train net output #0: loss = 0.0857749 (* 1 = 0.0857749 loss)
I1209 11:25:53.919647 14823 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:25:53.919721 14823 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_5.caffemodel
I1209 11:25:53.920758 14823 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_5.solverstate
I1209 11:25:53.921636 14823 solver.cpp:228] Iteration 5, loss = 0.0755682
I1209 11:25:53.921659 14823 solver.cpp:244]     Train net output #0: loss = 0.0755682 (* 1 = 0.0755682 loss)
I1209 11:25:53.921669 14823 sgd_solver.cpp:106] Iteration 5, lr = 0.01
I1209 11:25:53.922114 14823 solver.cpp:228] Iteration 6, loss = 0.0689826
I1209 11:25:53.922137 14823 solver.cpp:244]     Train net output #0: loss = 0.0689826 (* 1 = 0.0689826 loss)
I1209 11:25:53.922147 14823 sgd_solver.cpp:106] Iteration 6, lr = 0.01
I1209 11:25:53.922623 14823 solver.cpp:228] Iteration 7, loss = 0.0652205
I1209 11:25:53.922646 14823 solver.cpp:244]     Train net output #0: loss = 0.0652205 (* 1 = 0.0652205 loss)
I1209 11:25:53.922655 14823 sgd_solver.cpp:106] Iteration 7, lr = 0.01
I1209 11:25:53.923091 14823 solver.cpp:228] Iteration 8, loss = 0.0412991
I1209 11:25:53.923113 14823 solver.cpp:244]     Train net output #0: loss = 0.0412991 (* 1 = 0.0412991 loss)
I1209 11:25:53.923123 14823 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:25:53.923550 14823 solver.cpp:228] Iteration 9, loss = 0.0439358
I1209 11:25:53.923573 14823 solver.cpp:244]     Train net output #0: loss = 0.0439358 (* 1 = 0.0439358 loss)
I1209 11:25:53.923581 14823 sgd_solver.cpp:106] Iteration 9, lr = 0.01
I1209 11:25:53.923653 14823 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_10.caffemodel
I1209 11:25:53.924252 14823 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_10.solverstate
I1209 11:25:53.924726 14823 solver.cpp:337] Iteration 10, Testing net (#0)
I1209 11:25:53.925379 14823 solver.cpp:404]     Test net output #0: loss = 0.0467744 (* 1 = 0.0467744 loss)
I1209 11:25:53.925779 14823 solver.cpp:228] Iteration 10, loss = 0.0422129
I1209 11:25:53.925801 14823 solver.cpp:244]     Train net output #0: loss = 0.0422129 (* 1 = 0.0422129 loss)
I1209 11:25:53.925815 14823 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1209 11:25:53.926266 14823 solver.cpp:228] Iteration 11, loss = 0.0339011
I1209 11:25:53.926287 14823 solver.cpp:244]     Train net output #0: loss = 0.0339011 (* 1 = 0.0339011 loss)
I1209 11:25:53.926296 14823 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1209 11:25:53.926726 14823 solver.cpp:228] Iteration 12, loss = 0.0384588
I1209 11:25:53.926748 14823 solver.cpp:244]     Train net output #0: loss = 0.0384588 (* 1 = 0.0384588 loss)
I1209 11:25:53.926758 14823 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I1209 11:25:53.927187 14823 solver.cpp:228] Iteration 13, loss = 0.024136
I1209 11:25:53.927208 14823 solver.cpp:244]     Train net output #0: loss = 0.024136 (* 1 = 0.024136 loss)
I1209 11:25:53.927217 14823 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1209 11:25:53.927649 14823 solver.cpp:228] Iteration 14, loss = 0.0322204
I1209 11:25:53.927670 14823 solver.cpp:244]     Train net output #0: loss = 0.0322204 (* 1 = 0.0322204 loss)
I1209 11:25:53.927678 14823 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1209 11:25:53.927750 14823 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_15.caffemodel
I1209 11:25:53.928396 14823 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_15.solverstate
I1209 11:25:53.929234 14823 solver.cpp:228] Iteration 15, loss = 0.0258991
I1209 11:25:53.929256 14823 solver.cpp:244]     Train net output #0: loss = 0.0258991 (* 1 = 0.0258991 loss)
I1209 11:25:53.929266 14823 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I1209 11:25:53.929708 14823 solver.cpp:228] Iteration 16, loss = 0.0231253
I1209 11:25:53.929730 14823 solver.cpp:244]     Train net output #0: loss = 0.0231253 (* 1 = 0.0231253 loss)
I1209 11:25:53.929739 14823 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I1209 11:25:53.930199 14823 solver.cpp:228] Iteration 17, loss = 0.0232261
I1209 11:25:53.930222 14823 solver.cpp:244]     Train net output #0: loss = 0.0232261 (* 1 = 0.0232261 loss)
I1209 11:25:53.930232 14823 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I1209 11:25:53.930670 14823 solver.cpp:228] Iteration 18, loss = 0.0154988
I1209 11:25:53.930691 14823 solver.cpp:244]     Train net output #0: loss = 0.0154988 (* 1 = 0.0154988 loss)
I1209 11:25:53.930701 14823 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I1209 11:25:53.931128 14823 solver.cpp:228] Iteration 19, loss = 0.0166041
I1209 11:25:53.931149 14823 solver.cpp:244]     Train net output #0: loss = 0.0166041 (* 1 = 0.0166041 loss)
I1209 11:25:53.931165 14823 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I1209 11:25:53.931238 14823 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_20.caffemodel
I1209 11:25:53.931876 14823 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_20.solverstate
I1209 11:25:53.932343 14823 solver.cpp:337] Iteration 20, Testing net (#0)
I1209 11:25:53.932973 14823 solver.cpp:404]     Test net output #0: loss = 0.017894 (* 1 = 0.017894 loss)
I1209 11:25:53.933362 14823 solver.cpp:228] Iteration 20, loss = 0.0163972
I1209 11:25:53.933382 14823 solver.cpp:244]     Train net output #0: loss = 0.0163972 (* 1 = 0.0163972 loss)
I1209 11:25:53.933393 14823 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I1209 11:25:53.933840 14823 solver.cpp:228] Iteration 21, loss = 0.0135673
I1209 11:25:53.933861 14823 solver.cpp:244]     Train net output #0: loss = 0.0135673 (* 1 = 0.0135673 loss)
I1209 11:25:53.933871 14823 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I1209 11:25:53.934312 14823 solver.cpp:228] Iteration 22, loss = 0.0159421
I1209 11:25:53.934334 14823 solver.cpp:244]     Train net output #0: loss = 0.0159421 (* 1 = 0.0159421 loss)
I1209 11:25:53.934342 14823 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I1209 11:25:53.934780 14823 solver.cpp:228] Iteration 23, loss = 0.0101313
I1209 11:25:53.934800 14823 solver.cpp:244]     Train net output #0: loss = 0.0101313 (* 1 = 0.0101313 loss)
I1209 11:25:53.934810 14823 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I1209 11:25:53.935243 14823 solver.cpp:228] Iteration 24, loss = 0.0146747
I1209 11:25:53.935264 14823 solver.cpp:244]     Train net output #0: loss = 0.0146747 (* 1 = 0.0146747 loss)
I1209 11:25:53.935273 14823 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I1209 11:25:53.935345 14823 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_25.caffemodel
I1209 11:25:53.935917 14823 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_25.solverstate
I1209 11:25:53.936759 14823 solver.cpp:228] Iteration 25, loss = 0.0118844
I1209 11:25:53.936782 14823 solver.cpp:244]     Train net output #0: loss = 0.0118844 (* 1 = 0.0118844 loss)
I1209 11:25:53.936792 14823 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I1209 11:25:53.937232 14823 solver.cpp:228] Iteration 26, loss = 0.0110399
I1209 11:25:53.937253 14823 solver.cpp:244]     Train net output #0: loss = 0.0110399 (* 1 = 0.0110399 loss)
I1209 11:25:53.937263 14823 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I1209 11:25:53.937708 14823 solver.cpp:228] Iteration 27, loss = 0.0120574
I1209 11:25:53.937729 14823 solver.cpp:244]     Train net output #0: loss = 0.0120574 (* 1 = 0.0120574 loss)
I1209 11:25:53.937739 14823 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I1209 11:25:53.938195 14823 solver.cpp:228] Iteration 28, loss = 0.00854655
I1209 11:25:53.938217 14823 solver.cpp:244]     Train net output #0: loss = 0.00854655 (* 1 = 0.00854655 loss)
I1209 11:25:53.938227 14823 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I1209 11:25:53.938662 14823 solver.cpp:228] Iteration 29, loss = 0.0093963
I1209 11:25:53.938683 14823 solver.cpp:244]     Train net output #0: loss = 0.0093963 (* 1 = 0.0093963 loss)
I1209 11:25:53.938693 14823 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I1209 11:25:53.938763 14823 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_30.caffemodel
I1209 11:25:53.939334 14823 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_30.solverstate
I1209 11:25:53.940089 14823 solver.cpp:317] Iteration 30, loss = 0.00964794
I1209 11:25:53.940109 14823 solver.cpp:337] Iteration 30, Testing net (#0)
I1209 11:25:53.940711 14823 solver.cpp:404]     Test net output #0: loss = 0.0103879 (* 1 = 0.0103879 loss)
I1209 11:25:53.940727 14823 solver.cpp:322] Optimization Done.
I1209 11:25:53.940732 14823 caffe.cpp:254] Optimization Done.
