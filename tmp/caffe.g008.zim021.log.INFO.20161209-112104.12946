Log file created at: 2016/12/09 11:21:04
Running on machine: g008
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:21:04.180090 12946 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112103-11d7/solver.prototxt
I1209 11:21:04.181355 12946 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:21:04.181366 12946 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:21:04.182193 12946 caffe.cpp:217] Using GPUs 0
I1209 11:21:04.225556 12946 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:21:04.602602 12946 solver.cpp:48] Initializing solver from parameters: 
test_iter: 2
test_interval: 10
base_lr: 0.01
display: 1
max_iter: 30
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 10
snapshot: 5
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:21:04.603916 12946 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:21:04.604915 12946 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_data
I1209 11:21:04.604933 12946 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_label
I1209 11:21:04.604954 12946 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "train_data"
  type: "Data"
  top: "scaled_data"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.004
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "train_label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scaled_data"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:21:04.605183 12946 layer_factory.hpp:77] Creating layer train_data
I1209 11:21:04.605386 12946 net.cpp:100] Creating Layer train_data
I1209 11:21:04.605433 12946 net.cpp:408] train_data -> scaled_data
I1209 11:21:04.605492 12946 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:21:04.607830 12951 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:21:04.624253 12946 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:21:04.625527 12946 net.cpp:150] Setting up train_data
I1209 11:21:04.625563 12946 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:21:04.625583 12946 net.cpp:165] Memory required for data: 4000
I1209 11:21:04.625607 12946 layer_factory.hpp:77] Creating layer train_label
I1209 11:21:04.625694 12946 net.cpp:100] Creating Layer train_label
I1209 11:21:04.625711 12946 net.cpp:408] train_label -> label
I1209 11:21:04.629487 12953 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:21:04.629649 12946 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:21:04.630100 12946 net.cpp:150] Setting up train_label
I1209 11:21:04.630113 12946 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:21:04.630128 12946 net.cpp:165] Memory required for data: 4080
I1209 11:21:04.630136 12946 layer_factory.hpp:77] Creating layer hidden
I1209 11:21:04.630152 12946 net.cpp:100] Creating Layer hidden
I1209 11:21:04.630162 12946 net.cpp:434] hidden <- scaled_data
I1209 11:21:04.630182 12946 net.cpp:408] hidden -> output
I1209 11:21:04.630415 12946 net.cpp:150] Setting up hidden
I1209 11:21:04.630429 12946 net.cpp:157] Top shape: 10 2 (20)
I1209 11:21:04.630452 12946 net.cpp:165] Memory required for data: 4160
I1209 11:21:04.630486 12946 layer_factory.hpp:77] Creating layer loss
I1209 11:21:04.630506 12946 net.cpp:100] Creating Layer loss
I1209 11:21:04.630511 12946 net.cpp:434] loss <- output
I1209 11:21:04.630522 12946 net.cpp:434] loss <- label
I1209 11:21:04.630532 12946 net.cpp:408] loss -> loss
I1209 11:21:04.630595 12946 net.cpp:150] Setting up loss
I1209 11:21:04.630620 12946 net.cpp:157] Top shape: (1)
I1209 11:21:04.630631 12946 net.cpp:160]     with loss weight 1
I1209 11:21:04.630668 12946 net.cpp:165] Memory required for data: 4164
I1209 11:21:04.630678 12946 net.cpp:226] loss needs backward computation.
I1209 11:21:04.630686 12946 net.cpp:226] hidden needs backward computation.
I1209 11:21:04.630692 12946 net.cpp:228] train_label does not need backward computation.
I1209 11:21:04.630697 12946 net.cpp:228] train_data does not need backward computation.
I1209 11:21:04.630702 12946 net.cpp:270] This network produces output loss
I1209 11:21:04.630712 12946 net.cpp:283] Network initialization done.
I1209 11:21:04.631212 12946 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:21:04.631247 12946 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_data
I1209 11:21:04.631255 12946 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_label
I1209 11:21:04.631266 12946 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "val_data"
  type: "Data"
  top: "scaled_data"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.004
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "val_label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scaled_data"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:21:04.631420 12946 layer_factory.hpp:77] Creating layer val_data
I1209 11:21:04.631522 12946 net.cpp:100] Creating Layer val_data
I1209 11:21:04.631538 12946 net.cpp:408] val_data -> scaled_data
I1209 11:21:04.631569 12946 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:21:04.633877 12955 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:21:04.634110 12946 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:21:04.634359 12946 net.cpp:150] Setting up val_data
I1209 11:21:04.634374 12946 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:21:04.634385 12946 net.cpp:165] Memory required for data: 4000
I1209 11:21:04.634392 12946 layer_factory.hpp:77] Creating layer val_label
I1209 11:21:04.634472 12946 net.cpp:100] Creating Layer val_label
I1209 11:21:04.634487 12946 net.cpp:408] val_label -> label
I1209 11:21:04.637354 12957 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:21:04.637576 12946 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:21:04.637792 12946 net.cpp:150] Setting up val_label
I1209 11:21:04.637823 12946 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:21:04.637835 12946 net.cpp:165] Memory required for data: 4080
I1209 11:21:04.637842 12946 layer_factory.hpp:77] Creating layer hidden
I1209 11:21:04.637854 12946 net.cpp:100] Creating Layer hidden
I1209 11:21:04.637861 12946 net.cpp:434] hidden <- scaled_data
I1209 11:21:04.637874 12946 net.cpp:408] hidden -> output
I1209 11:21:04.638088 12946 net.cpp:150] Setting up hidden
I1209 11:21:04.638113 12946 net.cpp:157] Top shape: 10 2 (20)
I1209 11:21:04.638123 12946 net.cpp:165] Memory required for data: 4160
I1209 11:21:04.638140 12946 layer_factory.hpp:77] Creating layer loss
I1209 11:21:04.638151 12946 net.cpp:100] Creating Layer loss
I1209 11:21:04.638160 12946 net.cpp:434] loss <- output
I1209 11:21:04.638169 12946 net.cpp:434] loss <- label
I1209 11:21:04.638180 12946 net.cpp:408] loss -> loss
I1209 11:21:04.638234 12946 net.cpp:150] Setting up loss
I1209 11:21:04.638244 12946 net.cpp:157] Top shape: (1)
I1209 11:21:04.638253 12946 net.cpp:160]     with loss weight 1
I1209 11:21:04.638279 12946 net.cpp:165] Memory required for data: 4164
I1209 11:21:04.638288 12946 net.cpp:226] loss needs backward computation.
I1209 11:21:04.638293 12946 net.cpp:226] hidden needs backward computation.
I1209 11:21:04.638299 12946 net.cpp:228] val_label does not need backward computation.
I1209 11:21:04.638304 12946 net.cpp:228] val_data does not need backward computation.
I1209 11:21:04.638309 12946 net.cpp:270] This network produces output loss
I1209 11:21:04.638317 12946 net.cpp:283] Network initialization done.
I1209 11:21:04.638358 12946 solver.cpp:60] Solver scaffolding done.
I1209 11:21:04.638491 12946 caffe.cpp:251] Starting Optimization
I1209 11:21:04.638505 12946 solver.cpp:279] Solving 
I1209 11:21:04.638510 12946 solver.cpp:280] Learning Rate Policy: step
I1209 11:21:04.638659 12946 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:21:04.638675 12946 net.cpp:693] Ignoring source layer train_data
I1209 11:21:04.638680 12946 net.cpp:693] Ignoring source layer train_label
I1209 11:21:04.638854 12946 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:21:04.640147 12946 solver.cpp:404]     Test net output #0: loss = 0.0933352 (* 1 = 0.0933352 loss)
I1209 11:21:04.640775 12946 solver.cpp:228] Iteration 0, loss = 0.0855033
I1209 11:21:04.640799 12946 solver.cpp:244]     Train net output #0: loss = 0.0855033 (* 1 = 0.0855033 loss)
I1209 11:21:04.640825 12946 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:21:04.641335 12946 solver.cpp:228] Iteration 1, loss = 0.0785537
I1209 11:21:04.641357 12946 solver.cpp:244]     Train net output #0: loss = 0.0785537 (* 1 = 0.0785537 loss)
I1209 11:21:04.641367 12946 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I1209 11:21:04.641845 12946 solver.cpp:228] Iteration 2, loss = 0.0945713
I1209 11:21:04.641867 12946 solver.cpp:244]     Train net output #0: loss = 0.0945713 (* 1 = 0.0945713 loss)
I1209 11:21:04.641877 12946 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I1209 11:21:04.642334 12946 solver.cpp:228] Iteration 3, loss = 0.0646118
I1209 11:21:04.642355 12946 solver.cpp:244]     Train net output #0: loss = 0.0646118 (* 1 = 0.0646118 loss)
I1209 11:21:04.642365 12946 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I1209 11:21:04.642804 12946 solver.cpp:228] Iteration 4, loss = 0.0857749
I1209 11:21:04.642827 12946 solver.cpp:244]     Train net output #0: loss = 0.0857749 (* 1 = 0.0857749 loss)
I1209 11:21:04.642835 12946 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:21:04.642927 12946 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_5.caffemodel
I1209 11:21:04.643893 12946 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_5.solverstate
I1209 11:21:04.644759 12946 solver.cpp:228] Iteration 5, loss = 0.0755682
I1209 11:21:04.644783 12946 solver.cpp:244]     Train net output #0: loss = 0.0755682 (* 1 = 0.0755682 loss)
I1209 11:21:04.644793 12946 sgd_solver.cpp:106] Iteration 5, lr = 0.01
I1209 11:21:04.645241 12946 solver.cpp:228] Iteration 6, loss = 0.0689826
I1209 11:21:04.645262 12946 solver.cpp:244]     Train net output #0: loss = 0.0689826 (* 1 = 0.0689826 loss)
I1209 11:21:04.645272 12946 sgd_solver.cpp:106] Iteration 6, lr = 0.01
I1209 11:21:04.645720 12946 solver.cpp:228] Iteration 7, loss = 0.0652205
I1209 11:21:04.645741 12946 solver.cpp:244]     Train net output #0: loss = 0.0652205 (* 1 = 0.0652205 loss)
I1209 11:21:04.645750 12946 sgd_solver.cpp:106] Iteration 7, lr = 0.01
I1209 11:21:04.646220 12946 solver.cpp:228] Iteration 8, loss = 0.0412991
I1209 11:21:04.646241 12946 solver.cpp:244]     Train net output #0: loss = 0.0412991 (* 1 = 0.0412991 loss)
I1209 11:21:04.646251 12946 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:21:04.646695 12946 solver.cpp:228] Iteration 9, loss = 0.0439358
I1209 11:21:04.646718 12946 solver.cpp:244]     Train net output #0: loss = 0.0439358 (* 1 = 0.0439358 loss)
I1209 11:21:04.646733 12946 sgd_solver.cpp:106] Iteration 9, lr = 0.01
I1209 11:21:04.646823 12946 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_10.caffemodel
I1209 11:21:04.647585 12946 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_10.solverstate
I1209 11:21:04.648072 12946 solver.cpp:337] Iteration 10, Testing net (#0)
I1209 11:21:04.648085 12946 net.cpp:693] Ignoring source layer train_data
I1209 11:21:04.648090 12946 net.cpp:693] Ignoring source layer train_label
I1209 11:21:04.648684 12946 solver.cpp:404]     Test net output #0: loss = 0.0467744 (* 1 = 0.0467744 loss)
I1209 11:21:04.649061 12946 solver.cpp:228] Iteration 10, loss = 0.0422129
I1209 11:21:04.649083 12946 solver.cpp:244]     Train net output #0: loss = 0.0422129 (* 1 = 0.0422129 loss)
I1209 11:21:04.649096 12946 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1209 11:21:04.649519 12946 solver.cpp:228] Iteration 11, loss = 0.0339011
I1209 11:21:04.649541 12946 solver.cpp:244]     Train net output #0: loss = 0.0339011 (* 1 = 0.0339011 loss)
I1209 11:21:04.649550 12946 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1209 11:21:04.649968 12946 solver.cpp:228] Iteration 12, loss = 0.0384588
I1209 11:21:04.649989 12946 solver.cpp:244]     Train net output #0: loss = 0.0384588 (* 1 = 0.0384588 loss)
I1209 11:21:04.649999 12946 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I1209 11:21:04.650418 12946 solver.cpp:228] Iteration 13, loss = 0.024136
I1209 11:21:04.650440 12946 solver.cpp:244]     Train net output #0: loss = 0.024136 (* 1 = 0.024136 loss)
I1209 11:21:04.650449 12946 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1209 11:21:04.650869 12946 solver.cpp:228] Iteration 14, loss = 0.0322204
I1209 11:21:04.650890 12946 solver.cpp:244]     Train net output #0: loss = 0.0322204 (* 1 = 0.0322204 loss)
I1209 11:21:04.650900 12946 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1209 11:21:04.650988 12946 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_15.caffemodel
I1209 11:21:04.651677 12946 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_15.solverstate
I1209 11:21:04.652575 12946 solver.cpp:228] Iteration 15, loss = 0.0258991
I1209 11:21:04.652600 12946 solver.cpp:244]     Train net output #0: loss = 0.0258991 (* 1 = 0.0258991 loss)
I1209 11:21:04.652608 12946 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I1209 11:21:04.653055 12946 solver.cpp:228] Iteration 16, loss = 0.0231253
I1209 11:21:04.653077 12946 solver.cpp:244]     Train net output #0: loss = 0.0231253 (* 1 = 0.0231253 loss)
I1209 11:21:04.653087 12946 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I1209 11:21:04.653523 12946 solver.cpp:228] Iteration 17, loss = 0.0232261
I1209 11:21:04.653544 12946 solver.cpp:244]     Train net output #0: loss = 0.0232261 (* 1 = 0.0232261 loss)
I1209 11:21:04.653553 12946 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I1209 11:21:04.653982 12946 solver.cpp:228] Iteration 18, loss = 0.0154988
I1209 11:21:04.654005 12946 solver.cpp:244]     Train net output #0: loss = 0.0154988 (* 1 = 0.0154988 loss)
I1209 11:21:04.654014 12946 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I1209 11:21:04.654425 12946 solver.cpp:228] Iteration 19, loss = 0.0166041
I1209 11:21:04.654446 12946 solver.cpp:244]     Train net output #0: loss = 0.0166041 (* 1 = 0.0166041 loss)
I1209 11:21:04.654456 12946 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I1209 11:21:04.654546 12946 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_20.caffemodel
I1209 11:21:04.655241 12946 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_20.solverstate
I1209 11:21:04.655761 12946 solver.cpp:337] Iteration 20, Testing net (#0)
I1209 11:21:04.655776 12946 net.cpp:693] Ignoring source layer train_data
I1209 11:21:04.655781 12946 net.cpp:693] Ignoring source layer train_label
I1209 11:21:04.656370 12946 solver.cpp:404]     Test net output #0: loss = 0.017894 (* 1 = 0.017894 loss)
I1209 11:21:04.656704 12946 solver.cpp:228] Iteration 20, loss = 0.0163972
I1209 11:21:04.656730 12946 solver.cpp:244]     Train net output #0: loss = 0.0163972 (* 1 = 0.0163972 loss)
I1209 11:21:04.656740 12946 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I1209 11:21:04.657192 12946 solver.cpp:228] Iteration 21, loss = 0.0135673
I1209 11:21:04.657214 12946 solver.cpp:244]     Train net output #0: loss = 0.0135673 (* 1 = 0.0135673 loss)
I1209 11:21:04.657224 12946 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I1209 11:21:04.657668 12946 solver.cpp:228] Iteration 22, loss = 0.0159421
I1209 11:21:04.657691 12946 solver.cpp:244]     Train net output #0: loss = 0.0159421 (* 1 = 0.0159421 loss)
I1209 11:21:04.657699 12946 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I1209 11:21:04.658157 12946 solver.cpp:228] Iteration 23, loss = 0.0101313
I1209 11:21:04.658179 12946 solver.cpp:244]     Train net output #0: loss = 0.0101313 (* 1 = 0.0101313 loss)
I1209 11:21:04.658190 12946 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I1209 11:21:04.658639 12946 solver.cpp:228] Iteration 24, loss = 0.0146747
I1209 11:21:04.658661 12946 solver.cpp:244]     Train net output #0: loss = 0.0146747 (* 1 = 0.0146747 loss)
I1209 11:21:04.658670 12946 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I1209 11:21:04.658759 12946 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_25.caffemodel
I1209 11:21:04.659402 12946 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_25.solverstate
I1209 11:21:04.660212 12946 solver.cpp:228] Iteration 25, loss = 0.0118844
I1209 11:21:04.660235 12946 solver.cpp:244]     Train net output #0: loss = 0.0118844 (* 1 = 0.0118844 loss)
I1209 11:21:04.660245 12946 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I1209 11:21:04.660682 12946 solver.cpp:228] Iteration 26, loss = 0.0110399
I1209 11:21:04.660703 12946 solver.cpp:244]     Train net output #0: loss = 0.0110399 (* 1 = 0.0110399 loss)
I1209 11:21:04.660713 12946 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I1209 11:21:04.661170 12946 solver.cpp:228] Iteration 27, loss = 0.0120574
I1209 11:21:04.661192 12946 solver.cpp:244]     Train net output #0: loss = 0.0120574 (* 1 = 0.0120574 loss)
I1209 11:21:04.661201 12946 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I1209 11:21:04.661644 12946 solver.cpp:228] Iteration 28, loss = 0.00854655
I1209 11:21:04.661666 12946 solver.cpp:244]     Train net output #0: loss = 0.00854655 (* 1 = 0.00854655 loss)
I1209 11:21:04.661676 12946 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I1209 11:21:04.662122 12946 solver.cpp:228] Iteration 29, loss = 0.0093963
I1209 11:21:04.662144 12946 solver.cpp:244]     Train net output #0: loss = 0.0093963 (* 1 = 0.0093963 loss)
I1209 11:21:04.662153 12946 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I1209 11:21:04.662243 12946 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_30.caffemodel
I1209 11:21:04.662818 12946 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_30.solverstate
I1209 11:21:04.663579 12946 solver.cpp:317] Iteration 30, loss = 0.00964794
I1209 11:21:04.663599 12946 solver.cpp:337] Iteration 30, Testing net (#0)
I1209 11:21:04.663605 12946 net.cpp:693] Ignoring source layer train_data
I1209 11:21:04.663609 12946 net.cpp:693] Ignoring source layer train_label
I1209 11:21:04.664219 12946 solver.cpp:404]     Test net output #0: loss = 0.0103879 (* 1 = 0.0103879 loss)
I1209 11:21:04.664235 12946 solver.cpp:322] Optimization Done.
I1209 11:21:04.664240 12946 caffe.cpp:254] Optimization Done.
