Log file created at: 2016/12/09 11:20:57
Running on machine: g008
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:20:57.840775 12812 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112055-88ee/solver.prototxt
I1209 11:20:57.842334 12812 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:20:57.842345 12812 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:20:57.843307 12812 caffe.cpp:217] Using GPUs 0
I1209 11:20:57.892014 12812 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:20:58.284268 12812 solver.cpp:48] Initializing solver from parameters: 
test_iter: 7
test_interval: 34
base_lr: 0.01
display: 4
max_iter: 102
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 34
snapshot: 34
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:20:58.285980 12812 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:20:58.287181 12812 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_data
I1209 11:20:58.287200 12812 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_label
I1209 11:20:58.287222 12812 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "train_data"
  type: "Data"
  top: "scaled_data"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.004
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
    batch_size: 3
    backend: LMDB
  }
}
layer {
  name: "train_label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
    batch_size: 3
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scaled_data"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:20:58.287436 12812 layer_factory.hpp:77] Creating layer train_data
I1209 11:20:58.287607 12812 net.cpp:100] Creating Layer train_data
I1209 11:20:58.287633 12812 net.cpp:408] train_data -> scaled_data
I1209 11:20:58.287706 12812 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:20:58.290323 12817 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:20:58.306519 12812 data_layer.cpp:41] output data size: 3,1,10,10
I1209 11:20:58.307770 12812 net.cpp:150] Setting up train_data
I1209 11:20:58.307809 12812 net.cpp:157] Top shape: 3 1 10 10 (300)
I1209 11:20:58.307857 12812 net.cpp:165] Memory required for data: 1200
I1209 11:20:58.307884 12812 layer_factory.hpp:77] Creating layer train_label
I1209 11:20:58.307968 12812 net.cpp:100] Creating Layer train_label
I1209 11:20:58.307986 12812 net.cpp:408] train_label -> label
I1209 11:20:58.312170 12819 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:20:58.312368 12812 data_layer.cpp:41] output data size: 3,1,1,2
I1209 11:20:58.312779 12812 net.cpp:150] Setting up train_label
I1209 11:20:58.312794 12812 net.cpp:157] Top shape: 3 1 1 2 (6)
I1209 11:20:58.312808 12812 net.cpp:165] Memory required for data: 1224
I1209 11:20:58.312815 12812 layer_factory.hpp:77] Creating layer hidden
I1209 11:20:58.312839 12812 net.cpp:100] Creating Layer hidden
I1209 11:20:58.312850 12812 net.cpp:434] hidden <- scaled_data
I1209 11:20:58.312871 12812 net.cpp:408] hidden -> output
I1209 11:20:58.313143 12812 net.cpp:150] Setting up hidden
I1209 11:20:58.313158 12812 net.cpp:157] Top shape: 3 2 (6)
I1209 11:20:58.313169 12812 net.cpp:165] Memory required for data: 1248
I1209 11:20:58.313199 12812 layer_factory.hpp:77] Creating layer loss
I1209 11:20:58.313211 12812 net.cpp:100] Creating Layer loss
I1209 11:20:58.313217 12812 net.cpp:434] loss <- output
I1209 11:20:58.313225 12812 net.cpp:434] loss <- label
I1209 11:20:58.313237 12812 net.cpp:408] loss -> loss
I1209 11:20:58.313298 12812 net.cpp:150] Setting up loss
I1209 11:20:58.313308 12812 net.cpp:157] Top shape: (1)
I1209 11:20:58.313318 12812 net.cpp:160]     with loss weight 1
I1209 11:20:58.313349 12812 net.cpp:165] Memory required for data: 1252
I1209 11:20:58.313357 12812 net.cpp:226] loss needs backward computation.
I1209 11:20:58.313365 12812 net.cpp:226] hidden needs backward computation.
I1209 11:20:58.313371 12812 net.cpp:228] train_label does not need backward computation.
I1209 11:20:58.313376 12812 net.cpp:228] train_data does not need backward computation.
I1209 11:20:58.313380 12812 net.cpp:270] This network produces output loss
I1209 11:20:58.313390 12812 net.cpp:283] Network initialization done.
I1209 11:20:58.313887 12812 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:20:58.313923 12812 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_data
I1209 11:20:58.313930 12812 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_label
I1209 11:20:58.313941 12812 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "val_data"
  type: "Data"
  top: "scaled_data"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.004
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
    batch_size: 3
    backend: LMDB
  }
}
layer {
  name: "val_label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
    batch_size: 3
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scaled_data"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:20:58.314076 12812 layer_factory.hpp:77] Creating layer val_data
I1209 11:20:58.314160 12812 net.cpp:100] Creating Layer val_data
I1209 11:20:58.314175 12812 net.cpp:408] val_data -> scaled_data
I1209 11:20:58.314191 12812 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:20:58.317020 12821 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:20:58.317195 12812 data_layer.cpp:41] output data size: 3,1,10,10
I1209 11:20:58.317395 12812 net.cpp:150] Setting up val_data
I1209 11:20:58.317409 12812 net.cpp:157] Top shape: 3 1 10 10 (300)
I1209 11:20:58.317420 12812 net.cpp:165] Memory required for data: 1200
I1209 11:20:58.317426 12812 layer_factory.hpp:77] Creating layer val_label
I1209 11:20:58.317509 12812 net.cpp:100] Creating Layer val_label
I1209 11:20:58.317524 12812 net.cpp:408] val_label -> label
I1209 11:20:58.320436 12823 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:20:58.320629 12812 data_layer.cpp:41] output data size: 3,1,1,2
I1209 11:20:58.320837 12812 net.cpp:150] Setting up val_label
I1209 11:20:58.320852 12812 net.cpp:157] Top shape: 3 1 1 2 (6)
I1209 11:20:58.320863 12812 net.cpp:165] Memory required for data: 1224
I1209 11:20:58.320873 12812 layer_factory.hpp:77] Creating layer hidden
I1209 11:20:58.320888 12812 net.cpp:100] Creating Layer hidden
I1209 11:20:58.320895 12812 net.cpp:434] hidden <- scaled_data
I1209 11:20:58.320905 12812 net.cpp:408] hidden -> output
I1209 11:20:58.321127 12812 net.cpp:150] Setting up hidden
I1209 11:20:58.321141 12812 net.cpp:157] Top shape: 3 2 (6)
I1209 11:20:58.321149 12812 net.cpp:165] Memory required for data: 1248
I1209 11:20:58.321166 12812 layer_factory.hpp:77] Creating layer loss
I1209 11:20:58.321184 12812 net.cpp:100] Creating Layer loss
I1209 11:20:58.321190 12812 net.cpp:434] loss <- output
I1209 11:20:58.321197 12812 net.cpp:434] loss <- label
I1209 11:20:58.321209 12812 net.cpp:408] loss -> loss
I1209 11:20:58.321261 12812 net.cpp:150] Setting up loss
I1209 11:20:58.321271 12812 net.cpp:157] Top shape: (1)
I1209 11:20:58.321280 12812 net.cpp:160]     with loss weight 1
I1209 11:20:58.321290 12812 net.cpp:165] Memory required for data: 1252
I1209 11:20:58.321295 12812 net.cpp:226] loss needs backward computation.
I1209 11:20:58.321301 12812 net.cpp:226] hidden needs backward computation.
I1209 11:20:58.321307 12812 net.cpp:228] val_label does not need backward computation.
I1209 11:20:58.321312 12812 net.cpp:228] val_data does not need backward computation.
I1209 11:20:58.321316 12812 net.cpp:270] This network produces output loss
I1209 11:20:58.321324 12812 net.cpp:283] Network initialization done.
I1209 11:20:58.321357 12812 solver.cpp:60] Solver scaffolding done.
I1209 11:20:58.321475 12812 caffe.cpp:251] Starting Optimization
I1209 11:20:58.321486 12812 solver.cpp:279] Solving 
I1209 11:20:58.321491 12812 solver.cpp:280] Learning Rate Policy: step
I1209 11:20:58.321630 12812 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:20:58.321652 12812 net.cpp:693] Ignoring source layer train_data
I1209 11:20:58.321658 12812 net.cpp:693] Ignoring source layer train_label
I1209 11:20:58.321820 12812 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:20:58.324584 12812 solver.cpp:404]     Test net output #0: loss = 0.0927805 (* 1 = 0.0927805 loss)
I1209 11:20:58.325203 12812 solver.cpp:228] Iteration 0, loss = 0.0891476
I1209 11:20:58.325232 12812 solver.cpp:244]     Train net output #0: loss = 0.0891476 (* 1 = 0.0891476 loss)
I1209 11:20:58.325263 12812 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:20:58.326952 12812 solver.cpp:228] Iteration 4, loss = 0.040029
I1209 11:20:58.326977 12812 solver.cpp:244]     Train net output #0: loss = 0.040029 (* 1 = 0.040029 loss)
I1209 11:20:58.326987 12812 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:20:58.328573 12812 solver.cpp:228] Iteration 8, loss = 0.106527
I1209 11:20:58.328595 12812 solver.cpp:244]     Train net output #0: loss = 0.106527 (* 1 = 0.106527 loss)
I1209 11:20:58.328605 12812 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:20:58.330166 12812 solver.cpp:228] Iteration 12, loss = 0.0267923
I1209 11:20:58.330189 12812 solver.cpp:244]     Train net output #0: loss = 0.0267923 (* 1 = 0.0267923 loss)
I1209 11:20:58.330199 12812 sgd_solver.cpp:106] Iteration 12, lr = 0.01
I1209 11:20:58.331751 12812 solver.cpp:228] Iteration 16, loss = 0.027374
I1209 11:20:58.331773 12812 solver.cpp:244]     Train net output #0: loss = 0.027374 (* 1 = 0.027374 loss)
I1209 11:20:58.331782 12812 sgd_solver.cpp:106] Iteration 16, lr = 0.01
I1209 11:20:58.333343 12812 solver.cpp:228] Iteration 20, loss = 0.0051302
I1209 11:20:58.333364 12812 solver.cpp:244]     Train net output #0: loss = 0.0051302 (* 1 = 0.0051302 loss)
I1209 11:20:58.333374 12812 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I1209 11:20:58.334939 12812 solver.cpp:228] Iteration 24, loss = 0.00190577
I1209 11:20:58.334962 12812 solver.cpp:244]     Train net output #0: loss = 0.00190577 (* 1 = 0.00190577 loss)
I1209 11:20:58.334972 12812 sgd_solver.cpp:106] Iteration 24, lr = 0.01
I1209 11:20:58.336508 12812 solver.cpp:228] Iteration 28, loss = 0.000963273
I1209 11:20:58.336529 12812 solver.cpp:244]     Train net output #0: loss = 0.000963273 (* 1 = 0.000963273 loss)
I1209 11:20:58.336539 12812 sgd_solver.cpp:106] Iteration 28, lr = 0.01
I1209 11:20:58.338091 12812 solver.cpp:228] Iteration 32, loss = 0.00100994
I1209 11:20:58.338114 12812 solver.cpp:244]     Train net output #0: loss = 0.00100994 (* 1 = 0.00100994 loss)
I1209 11:20:58.338124 12812 sgd_solver.cpp:106] Iteration 32, lr = 0.01
I1209 11:20:58.338593 12812 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_34.caffemodel
I1209 11:20:58.339591 12812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_34.solverstate
I1209 11:20:58.340428 12812 solver.cpp:337] Iteration 34, Testing net (#0)
I1209 11:20:58.340442 12812 net.cpp:693] Ignoring source layer train_data
I1209 11:20:58.340452 12812 net.cpp:693] Ignoring source layer train_label
I1209 11:20:58.342509 12812 solver.cpp:404]     Test net output #0: loss = 0.00120699 (* 1 = 0.00120699 loss)
I1209 11:20:58.343634 12812 solver.cpp:228] Iteration 36, loss = 0.00129922
I1209 11:20:58.343657 12812 solver.cpp:244]     Train net output #0: loss = 0.00129922 (* 1 = 0.00129922 loss)
I1209 11:20:58.343667 12812 sgd_solver.cpp:106] Iteration 36, lr = 0.001
I1209 11:20:58.345227 12812 solver.cpp:228] Iteration 40, loss = 0.000676354
I1209 11:20:58.345248 12812 solver.cpp:244]     Train net output #0: loss = 0.000676353 (* 1 = 0.000676353 loss)
I1209 11:20:58.345258 12812 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I1209 11:20:58.346808 12812 solver.cpp:228] Iteration 44, loss = 0.00313189
I1209 11:20:58.346830 12812 solver.cpp:244]     Train net output #0: loss = 0.00313189 (* 1 = 0.00313189 loss)
I1209 11:20:58.346839 12812 sgd_solver.cpp:106] Iteration 44, lr = 0.001
I1209 11:20:58.348405 12812 solver.cpp:228] Iteration 48, loss = 0.00109859
I1209 11:20:58.348426 12812 solver.cpp:244]     Train net output #0: loss = 0.00109859 (* 1 = 0.00109859 loss)
I1209 11:20:58.348436 12812 sgd_solver.cpp:106] Iteration 48, lr = 0.001
I1209 11:20:58.349970 12812 solver.cpp:228] Iteration 52, loss = 0.00430601
I1209 11:20:58.349992 12812 solver.cpp:244]     Train net output #0: loss = 0.00430601 (* 1 = 0.00430601 loss)
I1209 11:20:58.350003 12812 sgd_solver.cpp:106] Iteration 52, lr = 0.001
I1209 11:20:58.351537 12812 solver.cpp:228] Iteration 56, loss = 0.00460374
I1209 11:20:58.351558 12812 solver.cpp:244]     Train net output #0: loss = 0.00460374 (* 1 = 0.00460374 loss)
I1209 11:20:58.351568 12812 sgd_solver.cpp:106] Iteration 56, lr = 0.001
I1209 11:20:58.353109 12812 solver.cpp:228] Iteration 60, loss = 0.00170915
I1209 11:20:58.353130 12812 solver.cpp:244]     Train net output #0: loss = 0.00170915 (* 1 = 0.00170915 loss)
I1209 11:20:58.353140 12812 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I1209 11:20:58.354717 12812 solver.cpp:228] Iteration 64, loss = 0.00145174
I1209 11:20:58.354737 12812 solver.cpp:244]     Train net output #0: loss = 0.00145174 (* 1 = 0.00145174 loss)
I1209 11:20:58.354748 12812 sgd_solver.cpp:106] Iteration 64, lr = 0.001
I1209 11:20:58.355967 12812 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_68.caffemodel
I1209 11:20:58.356881 12812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_68.solverstate
I1209 11:20:58.357579 12812 solver.cpp:337] Iteration 68, Testing net (#0)
I1209 11:20:58.357594 12812 net.cpp:693] Ignoring source layer train_data
I1209 11:20:58.357597 12812 net.cpp:693] Ignoring source layer train_label
I1209 11:20:58.359616 12812 solver.cpp:404]     Test net output #0: loss = 0.00292929 (* 1 = 0.00292929 loss)
I1209 11:20:58.359951 12812 solver.cpp:228] Iteration 68, loss = 0.00126233
I1209 11:20:58.359972 12812 solver.cpp:244]     Train net output #0: loss = 0.00126233 (* 1 = 0.00126233 loss)
I1209 11:20:58.359982 12812 sgd_solver.cpp:106] Iteration 68, lr = 0.0001
I1209 11:20:58.361551 12812 solver.cpp:228] Iteration 72, loss = 0.00280283
I1209 11:20:58.361572 12812 solver.cpp:244]     Train net output #0: loss = 0.00280283 (* 1 = 0.00280283 loss)
I1209 11:20:58.361582 12812 sgd_solver.cpp:106] Iteration 72, lr = 0.0001
I1209 11:20:58.363175 12812 solver.cpp:228] Iteration 76, loss = 0.00207929
I1209 11:20:58.363198 12812 solver.cpp:244]     Train net output #0: loss = 0.00207929 (* 1 = 0.00207929 loss)
I1209 11:20:58.363209 12812 sgd_solver.cpp:106] Iteration 76, lr = 0.0001
I1209 11:20:58.364758 12812 solver.cpp:228] Iteration 80, loss = 0.00220941
I1209 11:20:58.364779 12812 solver.cpp:244]     Train net output #0: loss = 0.00220941 (* 1 = 0.00220941 loss)
I1209 11:20:58.364789 12812 sgd_solver.cpp:106] Iteration 80, lr = 0.0001
I1209 11:20:58.366331 12812 solver.cpp:228] Iteration 84, loss = 0.00351485
I1209 11:20:58.366353 12812 solver.cpp:244]     Train net output #0: loss = 0.00351485 (* 1 = 0.00351485 loss)
I1209 11:20:58.366374 12812 sgd_solver.cpp:106] Iteration 84, lr = 0.0001
I1209 11:20:58.367928 12812 solver.cpp:228] Iteration 88, loss = 0.00379495
I1209 11:20:58.367950 12812 solver.cpp:244]     Train net output #0: loss = 0.00379495 (* 1 = 0.00379495 loss)
I1209 11:20:58.367959 12812 sgd_solver.cpp:106] Iteration 88, lr = 0.0001
I1209 11:20:58.369524 12812 solver.cpp:228] Iteration 92, loss = 0.00312926
I1209 11:20:58.369546 12812 solver.cpp:244]     Train net output #0: loss = 0.00312926 (* 1 = 0.00312926 loss)
I1209 11:20:58.369555 12812 sgd_solver.cpp:106] Iteration 92, lr = 0.0001
I1209 11:20:58.371150 12812 solver.cpp:228] Iteration 96, loss = 0.00137393
I1209 11:20:58.371172 12812 solver.cpp:244]     Train net output #0: loss = 0.00137393 (* 1 = 0.00137393 loss)
I1209 11:20:58.371181 12812 sgd_solver.cpp:106] Iteration 96, lr = 0.0001
I1209 11:20:58.372722 12812 solver.cpp:228] Iteration 100, loss = 0.00153187
I1209 11:20:58.372745 12812 solver.cpp:244]     Train net output #0: loss = 0.00153187 (* 1 = 0.00153187 loss)
I1209 11:20:58.372753 12812 sgd_solver.cpp:106] Iteration 100, lr = 0.0001
I1209 11:20:58.373219 12812 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_102.caffemodel
I1209 11:20:58.374258 12812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_102.solverstate
I1209 11:20:58.375134 12812 solver.cpp:337] Iteration 102, Testing net (#0)
I1209 11:20:58.375147 12812 net.cpp:693] Ignoring source layer train_data
I1209 11:20:58.375152 12812 net.cpp:693] Ignoring source layer train_label
I1209 11:20:58.377179 12812 solver.cpp:404]     Test net output #0: loss = 0.00253109 (* 1 = 0.00253109 loss)
I1209 11:20:58.377197 12812 solver.cpp:322] Optimization Done.
I1209 11:20:58.377202 12812 caffe.cpp:254] Optimization Done.
