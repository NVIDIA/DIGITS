Log file created at: 2016/12/09 11:26:24
Running on machine: g005
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:26:24.356523  6240 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112619-d0fd/solver.prototxt
I1209 11:26:24.357496  6240 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:26:24.357507  6240 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:26:24.358099  6240 caffe.cpp:217] Using GPUs 0
I1209 11:26:24.404031  6240 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:26:24.813642  6240 solver.cpp:48] Initializing solver from parameters: 
test_iter: 2
test_interval: 10
base_lr: 0.02
display: 1
max_iter: 30
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0002
stepsize: 10
snapshot: 10
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:26:24.814599  6240 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:26:24.815379  6240 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 11:26:24.815399  6240 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I1209 11:26:24.815421  6240 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:26:24.815646  6240 layer_factory.hpp:77] Creating layer data
I1209 11:26:24.815816  6240 net.cpp:100] Creating Layer data
I1209 11:26:24.815857  6240 net.cpp:408] data -> data
I1209 11:26:24.815918  6240 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:26:24.818181  6248 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:26:24.839457  6240 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:26:24.840852  6240 net.cpp:150] Setting up data
I1209 11:26:24.840894  6240 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:26:24.840937  6240 net.cpp:165] Memory required for data: 4000
I1209 11:26:24.840957  6240 layer_factory.hpp:77] Creating layer label
I1209 11:26:24.841034  6240 net.cpp:100] Creating Layer label
I1209 11:26:24.841045  6240 net.cpp:408] label -> label
I1209 11:26:24.844589  6250 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:26:24.844792  6240 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:26:24.845333  6240 net.cpp:150] Setting up label
I1209 11:26:24.845348  6240 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:26:24.845360  6240 net.cpp:165] Memory required for data: 4080
I1209 11:26:24.845366  6240 layer_factory.hpp:77] Creating layer scale
I1209 11:26:24.845393  6240 net.cpp:100] Creating Layer scale
I1209 11:26:24.845402  6240 net.cpp:434] scale <- data
I1209 11:26:24.845420  6240 net.cpp:408] scale -> scale
I1209 11:26:24.845491  6240 net.cpp:150] Setting up scale
I1209 11:26:24.845499  6240 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:26:24.845506  6240 net.cpp:165] Memory required for data: 8080
I1209 11:26:24.845511  6240 layer_factory.hpp:77] Creating layer hidden
I1209 11:26:24.845532  6240 net.cpp:100] Creating Layer hidden
I1209 11:26:24.845543  6240 net.cpp:434] hidden <- scale
I1209 11:26:24.845552  6240 net.cpp:408] hidden -> output
I1209 11:26:24.845700  6240 net.cpp:150] Setting up hidden
I1209 11:26:24.845710  6240 net.cpp:157] Top shape: 10 2 (20)
I1209 11:26:24.845718  6240 net.cpp:165] Memory required for data: 8160
I1209 11:26:24.845748  6240 layer_factory.hpp:77] Creating layer loss
I1209 11:26:24.845759  6240 net.cpp:100] Creating Layer loss
I1209 11:26:24.845764  6240 net.cpp:434] loss <- output
I1209 11:26:24.845769  6240 net.cpp:434] loss <- label
I1209 11:26:24.845777  6240 net.cpp:408] loss -> loss
I1209 11:26:24.845834  6240 net.cpp:150] Setting up loss
I1209 11:26:24.845842  6240 net.cpp:157] Top shape: (1)
I1209 11:26:24.845849  6240 net.cpp:160]     with loss weight 1
I1209 11:26:24.845882  6240 net.cpp:165] Memory required for data: 8164
I1209 11:26:24.845890  6240 net.cpp:226] loss needs backward computation.
I1209 11:26:24.845896  6240 net.cpp:226] hidden needs backward computation.
I1209 11:26:24.845899  6240 net.cpp:228] scale does not need backward computation.
I1209 11:26:24.845904  6240 net.cpp:228] label does not need backward computation.
I1209 11:26:24.845908  6240 net.cpp:228] data does not need backward computation.
I1209 11:26:24.845911  6240 net.cpp:270] This network produces output loss
I1209 11:26:24.845928  6240 net.cpp:283] Network initialization done.
I1209 11:26:24.846248  6240 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:26:24.846284  6240 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 11:26:24.846292  6240 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer label
I1209 11:26:24.846305  6240 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:26:24.846418  6240 layer_factory.hpp:77] Creating layer data
I1209 11:26:24.846518  6240 net.cpp:100] Creating Layer data
I1209 11:26:24.846534  6240 net.cpp:408] data -> data
I1209 11:26:24.846559  6240 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:26:24.848801  6252 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:26:24.849015  6240 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:26:24.849541  6240 net.cpp:150] Setting up data
I1209 11:26:24.849556  6240 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:26:24.849567  6240 net.cpp:165] Memory required for data: 4000
I1209 11:26:24.849575  6240 layer_factory.hpp:77] Creating layer label
I1209 11:26:24.849681  6240 net.cpp:100] Creating Layer label
I1209 11:26:24.849696  6240 net.cpp:408] label -> label
I1209 11:26:24.852625  6254 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:26:24.852752  6240 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:26:24.853184  6240 net.cpp:150] Setting up label
I1209 11:26:24.853198  6240 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:26:24.853205  6240 net.cpp:165] Memory required for data: 4080
I1209 11:26:24.853214  6240 layer_factory.hpp:77] Creating layer scale
I1209 11:26:24.853225  6240 net.cpp:100] Creating Layer scale
I1209 11:26:24.853235  6240 net.cpp:434] scale <- data
I1209 11:26:24.853242  6240 net.cpp:408] scale -> scale
I1209 11:26:24.853288  6240 net.cpp:150] Setting up scale
I1209 11:26:24.853297  6240 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:26:24.853304  6240 net.cpp:165] Memory required for data: 8080
I1209 11:26:24.853309  6240 layer_factory.hpp:77] Creating layer hidden
I1209 11:26:24.853318  6240 net.cpp:100] Creating Layer hidden
I1209 11:26:24.853322  6240 net.cpp:434] hidden <- scale
I1209 11:26:24.853337  6240 net.cpp:408] hidden -> output
I1209 11:26:24.853446  6240 net.cpp:150] Setting up hidden
I1209 11:26:24.853454  6240 net.cpp:157] Top shape: 10 2 (20)
I1209 11:26:24.853461  6240 net.cpp:165] Memory required for data: 8160
I1209 11:26:24.853476  6240 layer_factory.hpp:77] Creating layer loss
I1209 11:26:24.853485  6240 net.cpp:100] Creating Layer loss
I1209 11:26:24.853490  6240 net.cpp:434] loss <- output
I1209 11:26:24.853495  6240 net.cpp:434] loss <- label
I1209 11:26:24.853504  6240 net.cpp:408] loss -> loss
I1209 11:26:24.853543  6240 net.cpp:150] Setting up loss
I1209 11:26:24.853551  6240 net.cpp:157] Top shape: (1)
I1209 11:26:24.853561  6240 net.cpp:160]     with loss weight 1
I1209 11:26:24.853570  6240 net.cpp:165] Memory required for data: 8164
I1209 11:26:24.853575  6240 net.cpp:226] loss needs backward computation.
I1209 11:26:24.853580  6240 net.cpp:226] hidden needs backward computation.
I1209 11:26:24.853585  6240 net.cpp:228] scale does not need backward computation.
I1209 11:26:24.853590  6240 net.cpp:228] label does not need backward computation.
I1209 11:26:24.853593  6240 net.cpp:228] data does not need backward computation.
I1209 11:26:24.853596  6240 net.cpp:270] This network produces output loss
I1209 11:26:24.853603  6240 net.cpp:283] Network initialization done.
I1209 11:26:24.853633  6240 solver.cpp:60] Solver scaffolding done.
I1209 11:26:24.853729  6240 caffe.cpp:251] Starting Optimization
I1209 11:26:24.853737  6240 solver.cpp:279] Solving 
I1209 11:26:24.853741  6240 solver.cpp:280] Learning Rate Policy: step
I1209 11:26:24.853827  6240 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:26:24.854132  6240 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:26:24.855456  6240 solver.cpp:404]     Test net output #0: loss = 0.0933352 (* 1 = 0.0933352 loss)
I1209 11:26:24.856001  6240 solver.cpp:228] Iteration 0, loss = 0.0855033
I1209 11:26:24.856019  6240 solver.cpp:244]     Train net output #0: loss = 0.0855033 (* 1 = 0.0855033 loss)
I1209 11:26:24.856040  6240 sgd_solver.cpp:106] Iteration 0, lr = 0.02
I1209 11:26:24.856470  6240 solver.cpp:228] Iteration 1, loss = 0.0773105
I1209 11:26:24.856488  6240 solver.cpp:244]     Train net output #0: loss = 0.0773105 (* 1 = 0.0773105 loss)
I1209 11:26:24.856497  6240 sgd_solver.cpp:106] Iteration 1, lr = 0.02
I1209 11:26:24.856863  6240 solver.cpp:228] Iteration 2, loss = 0.0903891
I1209 11:26:24.856879  6240 solver.cpp:244]     Train net output #0: loss = 0.0903891 (* 1 = 0.0903891 loss)
I1209 11:26:24.856887  6240 sgd_solver.cpp:106] Iteration 2, lr = 0.02
I1209 11:26:24.857254  6240 solver.cpp:228] Iteration 3, loss = 0.0596274
I1209 11:26:24.857270  6240 solver.cpp:244]     Train net output #0: loss = 0.0596274 (* 1 = 0.0596274 loss)
I1209 11:26:24.857277  6240 sgd_solver.cpp:106] Iteration 3, lr = 0.02
I1209 11:26:24.857636  6240 solver.cpp:228] Iteration 4, loss = 0.0747513
I1209 11:26:24.857653  6240 solver.cpp:244]     Train net output #0: loss = 0.0747513 (* 1 = 0.0747513 loss)
I1209 11:26:24.857661  6240 sgd_solver.cpp:106] Iteration 4, lr = 0.02
I1209 11:26:24.858026  6240 solver.cpp:228] Iteration 5, loss = 0.0605154
I1209 11:26:24.858042  6240 solver.cpp:244]     Train net output #0: loss = 0.0605154 (* 1 = 0.0605154 loss)
I1209 11:26:24.858049  6240 sgd_solver.cpp:106] Iteration 5, lr = 0.02
I1209 11:26:24.858405  6240 solver.cpp:228] Iteration 6, loss = 0.0499631
I1209 11:26:24.858422  6240 solver.cpp:244]     Train net output #0: loss = 0.0499631 (* 1 = 0.0499631 loss)
I1209 11:26:24.858429  6240 sgd_solver.cpp:106] Iteration 6, lr = 0.02
I1209 11:26:24.858813  6240 solver.cpp:228] Iteration 7, loss = 0.0425762
I1209 11:26:24.858829  6240 solver.cpp:244]     Train net output #0: loss = 0.0425762 (* 1 = 0.0425762 loss)
I1209 11:26:24.858837  6240 sgd_solver.cpp:106] Iteration 7, lr = 0.02
I1209 11:26:24.859196  6240 solver.cpp:228] Iteration 8, loss = 0.0254298
I1209 11:26:24.859213  6240 solver.cpp:244]     Train net output #0: loss = 0.0254298 (* 1 = 0.0254298 loss)
I1209 11:26:24.859220  6240 sgd_solver.cpp:106] Iteration 8, lr = 0.02
I1209 11:26:24.859586  6240 solver.cpp:228] Iteration 9, loss = 0.0249706
I1209 11:26:24.859601  6240 solver.cpp:244]     Train net output #0: loss = 0.0249706 (* 1 = 0.0249706 loss)
I1209 11:26:24.859609  6240 sgd_solver.cpp:106] Iteration 9, lr = 0.02
I1209 11:26:24.859669  6240 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_10.caffemodel
I1209 11:26:24.860370  6240 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_10.solverstate
I1209 11:26:24.860805  6240 solver.cpp:337] Iteration 10, Testing net (#0)
I1209 11:26:24.861632  6240 solver.cpp:404]     Test net output #0: loss = 0.0194274 (* 1 = 0.0194274 loss)
I1209 11:26:24.861961  6240 solver.cpp:228] Iteration 10, loss = 0.0171697
I1209 11:26:24.861979  6240 solver.cpp:244]     Train net output #0: loss = 0.0171697 (* 1 = 0.0171697 loss)
I1209 11:26:24.861991  6240 sgd_solver.cpp:106] Iteration 10, lr = 0.002
I1209 11:26:24.862361  6240 solver.cpp:228] Iteration 11, loss = 0.0102686
I1209 11:26:24.862378  6240 solver.cpp:244]     Train net output #0: loss = 0.0102686 (* 1 = 0.0102686 loss)
I1209 11:26:24.862386  6240 sgd_solver.cpp:106] Iteration 11, lr = 0.002
I1209 11:26:24.862745  6240 solver.cpp:228] Iteration 12, loss = 0.00947553
I1209 11:26:24.862762  6240 solver.cpp:244]     Train net output #0: loss = 0.00947553 (* 1 = 0.00947553 loss)
I1209 11:26:24.862771  6240 sgd_solver.cpp:106] Iteration 12, lr = 0.002
I1209 11:26:24.863127  6240 solver.cpp:228] Iteration 13, loss = 0.00424622
I1209 11:26:24.863144  6240 solver.cpp:244]     Train net output #0: loss = 0.00424622 (* 1 = 0.00424622 loss)
I1209 11:26:24.863152  6240 sgd_solver.cpp:106] Iteration 13, lr = 0.002
I1209 11:26:24.863503  6240 solver.cpp:228] Iteration 14, loss = 0.00499988
I1209 11:26:24.863520  6240 solver.cpp:244]     Train net output #0: loss = 0.00499988 (* 1 = 0.00499988 loss)
I1209 11:26:24.863528  6240 sgd_solver.cpp:106] Iteration 14, lr = 0.002
I1209 11:26:24.863883  6240 solver.cpp:228] Iteration 15, loss = 0.00203356
I1209 11:26:24.863899  6240 solver.cpp:244]     Train net output #0: loss = 0.00203356 (* 1 = 0.00203356 loss)
I1209 11:26:24.863907  6240 sgd_solver.cpp:106] Iteration 15, lr = 0.002
I1209 11:26:24.864259  6240 solver.cpp:228] Iteration 16, loss = 0.000977495
I1209 11:26:24.864277  6240 solver.cpp:244]     Train net output #0: loss = 0.000977495 (* 1 = 0.000977495 loss)
I1209 11:26:24.864284  6240 sgd_solver.cpp:106] Iteration 16, lr = 0.002
I1209 11:26:24.864645  6240 solver.cpp:228] Iteration 17, loss = 0.00100083
I1209 11:26:24.864661  6240 solver.cpp:244]     Train net output #0: loss = 0.00100083 (* 1 = 0.00100083 loss)
I1209 11:26:24.864668  6240 sgd_solver.cpp:106] Iteration 17, lr = 0.002
I1209 11:26:24.865034  6240 solver.cpp:228] Iteration 18, loss = 0.000693598
I1209 11:26:24.865051  6240 solver.cpp:244]     Train net output #0: loss = 0.000693598 (* 1 = 0.000693598 loss)
I1209 11:26:24.865058  6240 sgd_solver.cpp:106] Iteration 18, lr = 0.002
I1209 11:26:24.865424  6240 solver.cpp:228] Iteration 19, loss = 0.000578843
I1209 11:26:24.865442  6240 solver.cpp:244]     Train net output #0: loss = 0.000578843 (* 1 = 0.000578843 loss)
I1209 11:26:24.865448  6240 sgd_solver.cpp:106] Iteration 19, lr = 0.002
I1209 11:26:24.865506  6240 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_20.caffemodel
I1209 11:26:24.865985  6240 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_20.solverstate
I1209 11:26:24.866276  6240 solver.cpp:337] Iteration 20, Testing net (#0)
I1209 11:26:24.866984  6240 solver.cpp:404]     Test net output #0: loss = 0.000392077 (* 1 = 0.000392077 loss)
I1209 11:26:24.867316  6240 solver.cpp:228] Iteration 20, loss = 0.00029282
I1209 11:26:24.867333  6240 solver.cpp:244]     Train net output #0: loss = 0.00029282 (* 1 = 0.00029282 loss)
I1209 11:26:24.867341  6240 sgd_solver.cpp:106] Iteration 20, lr = 0.0002
I1209 11:26:24.867703  6240 solver.cpp:228] Iteration 21, loss = 0.000416462
I1209 11:26:24.867720  6240 solver.cpp:244]     Train net output #0: loss = 0.000416462 (* 1 = 0.000416462 loss)
I1209 11:26:24.867727  6240 sgd_solver.cpp:106] Iteration 21, lr = 0.0002
I1209 11:26:24.868086  6240 solver.cpp:228] Iteration 22, loss = 0.00076501
I1209 11:26:24.868103  6240 solver.cpp:244]     Train net output #0: loss = 0.00076501 (* 1 = 0.00076501 loss)
I1209 11:26:24.868110  6240 sgd_solver.cpp:106] Iteration 22, lr = 0.0002
I1209 11:26:24.868474  6240 solver.cpp:228] Iteration 23, loss = 0.00100473
I1209 11:26:24.868489  6240 solver.cpp:244]     Train net output #0: loss = 0.00100473 (* 1 = 0.00100473 loss)
I1209 11:26:24.868497  6240 sgd_solver.cpp:106] Iteration 23, lr = 0.0002
I1209 11:26:24.868857  6240 solver.cpp:228] Iteration 24, loss = 0.00116486
I1209 11:26:24.868875  6240 solver.cpp:244]     Train net output #0: loss = 0.00116486 (* 1 = 0.00116486 loss)
I1209 11:26:24.868882  6240 sgd_solver.cpp:106] Iteration 24, lr = 0.0002
I1209 11:26:24.869240  6240 solver.cpp:228] Iteration 25, loss = 0.00206367
I1209 11:26:24.869257  6240 solver.cpp:244]     Train net output #0: loss = 0.00206367 (* 1 = 0.00206367 loss)
I1209 11:26:24.869266  6240 sgd_solver.cpp:106] Iteration 25, lr = 0.0002
I1209 11:26:24.869621  6240 solver.cpp:228] Iteration 26, loss = 0.00255539
I1209 11:26:24.869637  6240 solver.cpp:244]     Train net output #0: loss = 0.00255539 (* 1 = 0.00255539 loss)
I1209 11:26:24.869644  6240 sgd_solver.cpp:106] Iteration 26, lr = 0.0002
I1209 11:26:24.870012  6240 solver.cpp:228] Iteration 27, loss = 0.00203075
I1209 11:26:24.870029  6240 solver.cpp:244]     Train net output #0: loss = 0.00203075 (* 1 = 0.00203075 loss)
I1209 11:26:24.870038  6240 sgd_solver.cpp:106] Iteration 27, lr = 0.0002
I1209 11:26:24.870398  6240 solver.cpp:228] Iteration 28, loss = 0.000981527
I1209 11:26:24.870414  6240 solver.cpp:244]     Train net output #0: loss = 0.000981527 (* 1 = 0.000981527 loss)
I1209 11:26:24.870422  6240 sgd_solver.cpp:106] Iteration 28, lr = 0.0002
I1209 11:26:24.870805  6240 solver.cpp:228] Iteration 29, loss = 0.00139733
I1209 11:26:24.870822  6240 solver.cpp:244]     Train net output #0: loss = 0.00139733 (* 1 = 0.00139733 loss)
I1209 11:26:24.870829  6240 sgd_solver.cpp:106] Iteration 29, lr = 0.0002
I1209 11:26:24.870887  6240 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_30.caffemodel
I1209 11:26:24.871354  6240 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_30.solverstate
I1209 11:26:24.872084  6240 solver.cpp:317] Iteration 30, loss = 0.00257648
I1209 11:26:24.872099  6240 solver.cpp:337] Iteration 30, Testing net (#0)
I1209 11:26:24.872627  6240 solver.cpp:404]     Test net output #0: loss = 0.00317877 (* 1 = 0.00317877 loss)
I1209 11:26:24.872640  6240 solver.cpp:322] Optimization Done.
I1209 11:26:24.872644  6240 caffe.cpp:254] Optimization Done.
