Log file created at: 2016/12/09 11:25:18
Running on machine: g008
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:25:18.095327 13810 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112517-c2de/solver.prototxt
I1209 11:25:18.096870 13810 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:25:18.096881 13810 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:25:18.097761 13810 caffe.cpp:217] Using GPUs 0
I1209 11:25:18.143126 13810 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:25:18.564687 13810 solver.cpp:48] Initializing solver from parameters: 
test_iter: 2
test_interval: 10
base_lr: 0.01
display: 1
max_iter: 30
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 10
snapshot: 10
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:25:18.566372 13810 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:25:18.567692 13810 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 11:25:18.567711 13810 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I1209 11:25:18.567735 13810 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:25:18.567955 13810 layer_factory.hpp:77] Creating layer data
I1209 11:25:18.568145 13810 net.cpp:100] Creating Layer data
I1209 11:25:18.568164 13810 net.cpp:408] data -> data
I1209 11:25:18.571226 13817 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:25:18.596304 13810 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:25:18.597834 13810 net.cpp:150] Setting up data
I1209 11:25:18.597857 13810 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:18.597875 13810 net.cpp:165] Memory required for data: 4000
I1209 11:25:18.597901 13810 layer_factory.hpp:77] Creating layer label
I1209 11:25:18.598003 13810 net.cpp:100] Creating Layer label
I1209 11:25:18.598019 13810 net.cpp:408] label -> label
I1209 11:25:18.601423 13819 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:25:18.601577 13810 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:25:18.601976 13810 net.cpp:150] Setting up label
I1209 11:25:18.601991 13810 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:25:18.602001 13810 net.cpp:165] Memory required for data: 4080
I1209 11:25:18.602008 13810 layer_factory.hpp:77] Creating layer scale
I1209 11:25:18.602025 13810 net.cpp:100] Creating Layer scale
I1209 11:25:18.602033 13810 net.cpp:434] scale <- data
I1209 11:25:18.602053 13810 net.cpp:408] scale -> scale
I1209 11:25:18.602169 13810 net.cpp:150] Setting up scale
I1209 11:25:18.602182 13810 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:18.602192 13810 net.cpp:165] Memory required for data: 8080
I1209 11:25:18.602198 13810 layer_factory.hpp:77] Creating layer hidden
I1209 11:25:18.602216 13810 net.cpp:100] Creating Layer hidden
I1209 11:25:18.602222 13810 net.cpp:434] hidden <- scale
I1209 11:25:18.602232 13810 net.cpp:408] hidden -> output
I1209 11:25:18.602427 13810 net.cpp:150] Setting up hidden
I1209 11:25:18.602438 13810 net.cpp:157] Top shape: 10 2 (20)
I1209 11:25:18.602447 13810 net.cpp:165] Memory required for data: 8160
I1209 11:25:18.602486 13810 layer_factory.hpp:77] Creating layer loss
I1209 11:25:18.602500 13810 net.cpp:100] Creating Layer loss
I1209 11:25:18.602507 13810 net.cpp:434] loss <- output
I1209 11:25:18.602514 13810 net.cpp:434] loss <- label
I1209 11:25:18.602524 13810 net.cpp:408] loss -> loss
I1209 11:25:18.602586 13810 net.cpp:150] Setting up loss
I1209 11:25:18.602596 13810 net.cpp:157] Top shape: (1)
I1209 11:25:18.602604 13810 net.cpp:160]     with loss weight 1
I1209 11:25:18.602638 13810 net.cpp:165] Memory required for data: 8164
I1209 11:25:18.602645 13810 net.cpp:226] loss needs backward computation.
I1209 11:25:18.602653 13810 net.cpp:226] hidden needs backward computation.
I1209 11:25:18.602659 13810 net.cpp:228] scale does not need backward computation.
I1209 11:25:18.602665 13810 net.cpp:228] label does not need backward computation.
I1209 11:25:18.602670 13810 net.cpp:228] data does not need backward computation.
I1209 11:25:18.602674 13810 net.cpp:270] This network produces output loss
I1209 11:25:18.602684 13810 net.cpp:283] Network initialization done.
I1209 11:25:18.603087 13810 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:25:18.603121 13810 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 11:25:18.603129 13810 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer label
I1209 11:25:18.603142 13810 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:25:18.603278 13810 layer_factory.hpp:77] Creating layer data
I1209 11:25:18.603360 13810 net.cpp:100] Creating Layer data
I1209 11:25:18.603377 13810 net.cpp:408] data -> data
I1209 11:25:18.606220 13821 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:25:18.606436 13810 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:25:18.607246 13810 net.cpp:150] Setting up data
I1209 11:25:18.607269 13810 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:18.607280 13810 net.cpp:165] Memory required for data: 4000
I1209 11:25:18.607287 13810 layer_factory.hpp:77] Creating layer label
I1209 11:25:18.607381 13810 net.cpp:100] Creating Layer label
I1209 11:25:18.607398 13810 net.cpp:408] label -> label
I1209 11:25:18.609978 13823 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:25:18.610170 13810 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:25:18.610682 13810 net.cpp:150] Setting up label
I1209 11:25:18.610697 13810 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:25:18.610707 13810 net.cpp:165] Memory required for data: 4080
I1209 11:25:18.610716 13810 layer_factory.hpp:77] Creating layer scale
I1209 11:25:18.610729 13810 net.cpp:100] Creating Layer scale
I1209 11:25:18.610738 13810 net.cpp:434] scale <- data
I1209 11:25:18.610751 13810 net.cpp:408] scale -> scale
I1209 11:25:18.610796 13810 net.cpp:150] Setting up scale
I1209 11:25:18.610806 13810 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:25:18.610821 13810 net.cpp:165] Memory required for data: 8080
I1209 11:25:18.610827 13810 layer_factory.hpp:77] Creating layer hidden
I1209 11:25:18.610837 13810 net.cpp:100] Creating Layer hidden
I1209 11:25:18.610843 13810 net.cpp:434] hidden <- scale
I1209 11:25:18.610859 13810 net.cpp:408] hidden -> output
I1209 11:25:18.610991 13810 net.cpp:150] Setting up hidden
I1209 11:25:18.611001 13810 net.cpp:157] Top shape: 10 2 (20)
I1209 11:25:18.611009 13810 net.cpp:165] Memory required for data: 8160
I1209 11:25:18.611027 13810 layer_factory.hpp:77] Creating layer loss
I1209 11:25:18.611038 13810 net.cpp:100] Creating Layer loss
I1209 11:25:18.611044 13810 net.cpp:434] loss <- output
I1209 11:25:18.611052 13810 net.cpp:434] loss <- label
I1209 11:25:18.611063 13810 net.cpp:408] loss -> loss
I1209 11:25:18.611115 13810 net.cpp:150] Setting up loss
I1209 11:25:18.611125 13810 net.cpp:157] Top shape: (1)
I1209 11:25:18.611135 13810 net.cpp:160]     with loss weight 1
I1209 11:25:18.611143 13810 net.cpp:165] Memory required for data: 8164
I1209 11:25:18.611150 13810 net.cpp:226] loss needs backward computation.
I1209 11:25:18.611155 13810 net.cpp:226] hidden needs backward computation.
I1209 11:25:18.611161 13810 net.cpp:228] scale does not need backward computation.
I1209 11:25:18.611167 13810 net.cpp:228] label does not need backward computation.
I1209 11:25:18.611172 13810 net.cpp:228] data does not need backward computation.
I1209 11:25:18.611177 13810 net.cpp:270] This network produces output loss
I1209 11:25:18.611189 13810 net.cpp:283] Network initialization done.
I1209 11:25:18.611224 13810 solver.cpp:60] Solver scaffolding done.
I1209 11:25:18.611340 13810 caffe.cpp:251] Starting Optimization
I1209 11:25:18.611351 13810 solver.cpp:279] Solving 
I1209 11:25:18.611356 13810 solver.cpp:280] Learning Rate Policy: step
I1209 11:25:18.611466 13810 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:25:18.611800 13810 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:25:18.613250 13810 solver.cpp:404]     Test net output #0: loss = 0.0933352 (* 1 = 0.0933352 loss)
I1209 11:25:18.613940 13810 solver.cpp:228] Iteration 0, loss = 0.0855033
I1209 11:25:18.613971 13810 solver.cpp:244]     Train net output #0: loss = 0.0855033 (* 1 = 0.0855033 loss)
I1209 11:25:18.614001 13810 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:25:18.614529 13810 solver.cpp:228] Iteration 1, loss = 0.0741212
I1209 11:25:18.614552 13810 solver.cpp:244]     Train net output #0: loss = 0.0741212 (* 1 = 0.0741212 loss)
I1209 11:25:18.614562 13810 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I1209 11:25:18.614998 13810 solver.cpp:228] Iteration 2, loss = 0.0946392
I1209 11:25:18.615020 13810 solver.cpp:244]     Train net output #0: loss = 0.0946392 (* 1 = 0.0946392 loss)
I1209 11:25:18.615030 13810 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I1209 11:25:18.615468 13810 solver.cpp:228] Iteration 3, loss = 0.0849408
I1209 11:25:18.615489 13810 solver.cpp:244]     Train net output #0: loss = 0.0849408 (* 1 = 0.0849408 loss)
I1209 11:25:18.615499 13810 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I1209 11:25:18.615927 13810 solver.cpp:228] Iteration 4, loss = 0.100447
I1209 11:25:18.615948 13810 solver.cpp:244]     Train net output #0: loss = 0.100447 (* 1 = 0.100447 loss)
I1209 11:25:18.615958 13810 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:25:18.616387 13810 solver.cpp:228] Iteration 5, loss = 0.0782987
I1209 11:25:18.616408 13810 solver.cpp:244]     Train net output #0: loss = 0.0782987 (* 1 = 0.0782987 loss)
I1209 11:25:18.616418 13810 sgd_solver.cpp:106] Iteration 5, lr = 0.01
I1209 11:25:18.616857 13810 solver.cpp:228] Iteration 6, loss = 0.0668709
I1209 11:25:18.616878 13810 solver.cpp:244]     Train net output #0: loss = 0.0668709 (* 1 = 0.0668709 loss)
I1209 11:25:18.616886 13810 sgd_solver.cpp:106] Iteration 6, lr = 0.01
I1209 11:25:18.617316 13810 solver.cpp:228] Iteration 7, loss = 0.0747669
I1209 11:25:18.617336 13810 solver.cpp:244]     Train net output #0: loss = 0.0747669 (* 1 = 0.0747669 loss)
I1209 11:25:18.617346 13810 sgd_solver.cpp:106] Iteration 7, lr = 0.01
I1209 11:25:18.617779 13810 solver.cpp:228] Iteration 8, loss = 0.0530129
I1209 11:25:18.617800 13810 solver.cpp:244]     Train net output #0: loss = 0.0530129 (* 1 = 0.0530129 loss)
I1209 11:25:18.617810 13810 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:25:18.618245 13810 solver.cpp:228] Iteration 9, loss = 0.0451137
I1209 11:25:18.618266 13810 solver.cpp:244]     Train net output #0: loss = 0.0451137 (* 1 = 0.0451137 loss)
I1209 11:25:18.618275 13810 sgd_solver.cpp:106] Iteration 9, lr = 0.01
I1209 11:25:18.618351 13810 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_10.caffemodel
I1209 11:25:18.619246 13810 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_10.solverstate
I1209 11:25:18.619874 13810 solver.cpp:337] Iteration 10, Testing net (#0)
I1209 11:25:18.620695 13810 solver.cpp:404]     Test net output #0: loss = 0.0477516 (* 1 = 0.0477516 loss)
I1209 11:25:18.621084 13810 solver.cpp:228] Iteration 10, loss = 0.0569305
I1209 11:25:18.621106 13810 solver.cpp:244]     Train net output #0: loss = 0.0569305 (* 1 = 0.0569305 loss)
I1209 11:25:18.621119 13810 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1209 11:25:18.621554 13810 solver.cpp:228] Iteration 11, loss = 0.0515115
I1209 11:25:18.621575 13810 solver.cpp:244]     Train net output #0: loss = 0.0515115 (* 1 = 0.0515115 loss)
I1209 11:25:18.621585 13810 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1209 11:25:18.622054 13810 solver.cpp:228] Iteration 12, loss = 0.0572844
I1209 11:25:18.622076 13810 solver.cpp:244]     Train net output #0: loss = 0.0572844 (* 1 = 0.0572844 loss)
I1209 11:25:18.622087 13810 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I1209 11:25:18.622524 13810 solver.cpp:228] Iteration 13, loss = 0.0385391
I1209 11:25:18.622545 13810 solver.cpp:244]     Train net output #0: loss = 0.0385391 (* 1 = 0.0385391 loss)
I1209 11:25:18.622553 13810 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1209 11:25:18.622992 13810 solver.cpp:228] Iteration 14, loss = 0.0557141
I1209 11:25:18.623013 13810 solver.cpp:244]     Train net output #0: loss = 0.0557141 (* 1 = 0.0557141 loss)
I1209 11:25:18.623023 13810 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1209 11:25:18.623452 13810 solver.cpp:228] Iteration 15, loss = 0.0556994
I1209 11:25:18.623474 13810 solver.cpp:244]     Train net output #0: loss = 0.0556994 (* 1 = 0.0556994 loss)
I1209 11:25:18.623483 13810 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I1209 11:25:18.623914 13810 solver.cpp:228] Iteration 16, loss = 0.0631248
I1209 11:25:18.623936 13810 solver.cpp:244]     Train net output #0: loss = 0.0631248 (* 1 = 0.0631248 loss)
I1209 11:25:18.623945 13810 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I1209 11:25:18.624379 13810 solver.cpp:228] Iteration 17, loss = 0.0670649
I1209 11:25:18.624402 13810 solver.cpp:244]     Train net output #0: loss = 0.0670649 (* 1 = 0.0670649 loss)
I1209 11:25:18.624410 13810 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I1209 11:25:18.624847 13810 solver.cpp:228] Iteration 18, loss = 0.040826
I1209 11:25:18.624868 13810 solver.cpp:244]     Train net output #0: loss = 0.040826 (* 1 = 0.040826 loss)
I1209 11:25:18.624877 13810 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I1209 11:25:18.625300 13810 solver.cpp:228] Iteration 19, loss = 0.0252037
I1209 11:25:18.625321 13810 solver.cpp:244]     Train net output #0: loss = 0.0252037 (* 1 = 0.0252037 loss)
I1209 11:25:18.625331 13810 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I1209 11:25:18.625403 13810 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_20.caffemodel
I1209 11:25:18.626102 13810 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_20.solverstate
I1209 11:25:18.626598 13810 solver.cpp:337] Iteration 20, Testing net (#0)
I1209 11:25:18.627439 13810 solver.cpp:404]     Test net output #0: loss = 0.0439129 (* 1 = 0.0439129 loss)
I1209 11:25:18.627828 13810 solver.cpp:228] Iteration 20, loss = 0.0462524
I1209 11:25:18.627851 13810 solver.cpp:244]     Train net output #0: loss = 0.0462524 (* 1 = 0.0462524 loss)
I1209 11:25:18.627859 13810 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I1209 11:25:18.628293 13810 solver.cpp:228] Iteration 21, loss = 0.0357765
I1209 11:25:18.628315 13810 solver.cpp:244]     Train net output #0: loss = 0.0357765 (* 1 = 0.0357765 loss)
I1209 11:25:18.628330 13810 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I1209 11:25:18.628757 13810 solver.cpp:228] Iteration 22, loss = 0.0431939
I1209 11:25:18.628778 13810 solver.cpp:244]     Train net output #0: loss = 0.0431939 (* 1 = 0.0431939 loss)
I1209 11:25:18.628788 13810 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I1209 11:25:18.629217 13810 solver.cpp:228] Iteration 23, loss = 0.0324976
I1209 11:25:18.629237 13810 solver.cpp:244]     Train net output #0: loss = 0.0324976 (* 1 = 0.0324976 loss)
I1209 11:25:18.629247 13810 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I1209 11:25:18.629675 13810 solver.cpp:228] Iteration 24, loss = 0.0368729
I1209 11:25:18.629698 13810 solver.cpp:244]     Train net output #0: loss = 0.0368729 (* 1 = 0.0368729 loss)
I1209 11:25:18.629706 13810 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I1209 11:25:18.630153 13810 solver.cpp:228] Iteration 25, loss = 0.0315715
I1209 11:25:18.630175 13810 solver.cpp:244]     Train net output #0: loss = 0.0315715 (* 1 = 0.0315715 loss)
I1209 11:25:18.630185 13810 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I1209 11:25:18.630614 13810 solver.cpp:228] Iteration 26, loss = 0.0329897
I1209 11:25:18.630633 13810 solver.cpp:244]     Train net output #0: loss = 0.0329897 (* 1 = 0.0329897 loss)
I1209 11:25:18.630643 13810 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I1209 11:25:18.631072 13810 solver.cpp:228] Iteration 27, loss = 0.0317242
I1209 11:25:18.631093 13810 solver.cpp:244]     Train net output #0: loss = 0.0317242 (* 1 = 0.0317242 loss)
I1209 11:25:18.631103 13810 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I1209 11:25:18.631534 13810 solver.cpp:228] Iteration 28, loss = 0.0203639
I1209 11:25:18.631556 13810 solver.cpp:244]     Train net output #0: loss = 0.0203639 (* 1 = 0.0203639 loss)
I1209 11:25:18.631566 13810 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I1209 11:25:18.631994 13810 solver.cpp:228] Iteration 29, loss = 0.0156842
I1209 11:25:18.632016 13810 solver.cpp:244]     Train net output #0: loss = 0.0156842 (* 1 = 0.0156842 loss)
I1209 11:25:18.632025 13810 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I1209 11:25:18.632097 13810 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_30.caffemodel
I1209 11:25:18.632776 13810 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_30.solverstate
I1209 11:25:18.633779 13810 solver.cpp:317] Iteration 30, loss = 0.021972
I1209 11:25:18.633797 13810 solver.cpp:337] Iteration 30, Testing net (#0)
I1209 11:25:18.634454 13810 solver.cpp:404]     Test net output #0: loss = 0.0231852 (* 1 = 0.0231852 loss)
I1209 11:25:18.634469 13810 solver.cpp:322] Optimization Done.
I1209 11:25:18.634474 13810 caffe.cpp:254] Optimization Done.
