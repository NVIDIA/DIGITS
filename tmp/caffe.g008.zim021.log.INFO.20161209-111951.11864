Log file created at: 2016/12/09 11:19:51
Running on machine: g008
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:19:51.794055 11864 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-111951-5ab1/solver.prototxt
I1209 11:19:51.795994 11864 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:19:51.796006 11864 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:19:51.796921 11864 caffe.cpp:217] Using GPUs 0
I1209 11:19:51.845594 11864 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:19:52.251394 11864 solver.cpp:48] Initializing solver from parameters: 
test_iter: 2
test_interval: 10
base_lr: 0.01
display: 1
max_iter: 30
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 10
snapshot: 10
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:19:52.252756 11864 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:19:52.253749 11864 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_data
I1209 11:19:52.253778 11864 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_label
I1209 11:19:52.253803 11864 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "train_data"
  type: "Data"
  top: "scaled_data"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.004
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "train_label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scaled_data"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:19:52.254010 11864 layer_factory.hpp:77] Creating layer train_data
I1209 11:19:52.254185 11864 net.cpp:100] Creating Layer train_data
I1209 11:19:52.254206 11864 net.cpp:408] train_data -> scaled_data
I1209 11:19:52.257176 11869 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:19:52.275552 11864 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:19:52.276813 11864 net.cpp:150] Setting up train_data
I1209 11:19:52.276855 11864 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:19:52.276899 11864 net.cpp:165] Memory required for data: 4000
I1209 11:19:52.276928 11864 layer_factory.hpp:77] Creating layer train_label
I1209 11:19:52.277017 11864 net.cpp:100] Creating Layer train_label
I1209 11:19:52.277035 11864 net.cpp:408] train_label -> label
I1209 11:19:52.280740 11871 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:19:52.280941 11864 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:19:52.281352 11864 net.cpp:150] Setting up train_label
I1209 11:19:52.281368 11864 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:19:52.281386 11864 net.cpp:165] Memory required for data: 4080
I1209 11:19:52.281394 11864 layer_factory.hpp:77] Creating layer hidden
I1209 11:19:52.281412 11864 net.cpp:100] Creating Layer hidden
I1209 11:19:52.281422 11864 net.cpp:434] hidden <- scaled_data
I1209 11:19:52.281447 11864 net.cpp:408] hidden -> output
I1209 11:19:52.281648 11864 net.cpp:150] Setting up hidden
I1209 11:19:52.281661 11864 net.cpp:157] Top shape: 10 2 (20)
I1209 11:19:52.281671 11864 net.cpp:165] Memory required for data: 4160
I1209 11:19:52.281702 11864 layer_factory.hpp:77] Creating layer loss
I1209 11:19:52.281725 11864 net.cpp:100] Creating Layer loss
I1209 11:19:52.281731 11864 net.cpp:434] loss <- output
I1209 11:19:52.281738 11864 net.cpp:434] loss <- label
I1209 11:19:52.281749 11864 net.cpp:408] loss -> loss
I1209 11:19:52.281821 11864 net.cpp:150] Setting up loss
I1209 11:19:52.281833 11864 net.cpp:157] Top shape: (1)
I1209 11:19:52.281841 11864 net.cpp:160]     with loss weight 1
I1209 11:19:52.281885 11864 net.cpp:165] Memory required for data: 4164
I1209 11:19:52.281894 11864 net.cpp:226] loss needs backward computation.
I1209 11:19:52.281908 11864 net.cpp:226] hidden needs backward computation.
I1209 11:19:52.281914 11864 net.cpp:228] train_label does not need backward computation.
I1209 11:19:52.281919 11864 net.cpp:228] train_data does not need backward computation.
I1209 11:19:52.281924 11864 net.cpp:270] This network produces output loss
I1209 11:19:52.281934 11864 net.cpp:283] Network initialization done.
I1209 11:19:52.282393 11864 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:19:52.282430 11864 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_data
I1209 11:19:52.282439 11864 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_label
I1209 11:19:52.282449 11864 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "val_data"
  type: "Data"
  top: "scaled_data"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.004
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "val_label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scaled_data"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:19:52.282587 11864 layer_factory.hpp:77] Creating layer val_data
I1209 11:19:52.282693 11864 net.cpp:100] Creating Layer val_data
I1209 11:19:52.282711 11864 net.cpp:408] val_data -> scaled_data
I1209 11:19:52.285312 11873 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:19:52.285527 11864 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:19:52.285742 11864 net.cpp:150] Setting up val_data
I1209 11:19:52.285763 11864 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:19:52.285775 11864 net.cpp:165] Memory required for data: 4000
I1209 11:19:52.285782 11864 layer_factory.hpp:77] Creating layer val_label
I1209 11:19:52.285868 11864 net.cpp:100] Creating Layer val_label
I1209 11:19:52.285882 11864 net.cpp:408] val_label -> label
I1209 11:19:52.288624 11875 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:19:52.288803 11864 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:19:52.289019 11864 net.cpp:150] Setting up val_label
I1209 11:19:52.289036 11864 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:19:52.289046 11864 net.cpp:165] Memory required for data: 4080
I1209 11:19:52.289054 11864 layer_factory.hpp:77] Creating layer hidden
I1209 11:19:52.289067 11864 net.cpp:100] Creating Layer hidden
I1209 11:19:52.289073 11864 net.cpp:434] hidden <- scaled_data
I1209 11:19:52.289090 11864 net.cpp:408] hidden -> output
I1209 11:19:52.289314 11864 net.cpp:150] Setting up hidden
I1209 11:19:52.289325 11864 net.cpp:157] Top shape: 10 2 (20)
I1209 11:19:52.289335 11864 net.cpp:165] Memory required for data: 4160
I1209 11:19:52.289352 11864 layer_factory.hpp:77] Creating layer loss
I1209 11:19:52.289368 11864 net.cpp:100] Creating Layer loss
I1209 11:19:52.289376 11864 net.cpp:434] loss <- output
I1209 11:19:52.289382 11864 net.cpp:434] loss <- label
I1209 11:19:52.289391 11864 net.cpp:408] loss -> loss
I1209 11:19:52.289444 11864 net.cpp:150] Setting up loss
I1209 11:19:52.289454 11864 net.cpp:157] Top shape: (1)
I1209 11:19:52.289463 11864 net.cpp:160]     with loss weight 1
I1209 11:19:52.289471 11864 net.cpp:165] Memory required for data: 4164
I1209 11:19:52.289477 11864 net.cpp:226] loss needs backward computation.
I1209 11:19:52.289484 11864 net.cpp:226] hidden needs backward computation.
I1209 11:19:52.289494 11864 net.cpp:228] val_label does not need backward computation.
I1209 11:19:52.289499 11864 net.cpp:228] val_data does not need backward computation.
I1209 11:19:52.289504 11864 net.cpp:270] This network produces output loss
I1209 11:19:52.289511 11864 net.cpp:283] Network initialization done.
I1209 11:19:52.289544 11864 solver.cpp:60] Solver scaffolding done.
I1209 11:19:52.289655 11864 caffe.cpp:251] Starting Optimization
I1209 11:19:52.289676 11864 solver.cpp:279] Solving 
I1209 11:19:52.289683 11864 solver.cpp:280] Learning Rate Policy: step
I1209 11:19:52.289810 11864 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:19:52.289834 11864 net.cpp:693] Ignoring source layer train_data
I1209 11:19:52.289839 11864 net.cpp:693] Ignoring source layer train_label
I1209 11:19:52.289991 11864 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:19:52.291404 11864 solver.cpp:404]     Test net output #0: loss = 0.0933352 (* 1 = 0.0933352 loss)
I1209 11:19:52.292060 11864 solver.cpp:228] Iteration 0, loss = 0.0855033
I1209 11:19:52.292088 11864 solver.cpp:244]     Train net output #0: loss = 0.0855033 (* 1 = 0.0855033 loss)
I1209 11:19:52.292116 11864 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:19:52.292598 11864 solver.cpp:228] Iteration 1, loss = 0.0741212
I1209 11:19:52.292621 11864 solver.cpp:244]     Train net output #0: loss = 0.0741212 (* 1 = 0.0741212 loss)
I1209 11:19:52.292631 11864 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I1209 11:19:52.293046 11864 solver.cpp:228] Iteration 2, loss = 0.0946392
I1209 11:19:52.293067 11864 solver.cpp:244]     Train net output #0: loss = 0.0946392 (* 1 = 0.0946392 loss)
I1209 11:19:52.293077 11864 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I1209 11:19:52.293489 11864 solver.cpp:228] Iteration 3, loss = 0.0849408
I1209 11:19:52.293509 11864 solver.cpp:244]     Train net output #0: loss = 0.0849408 (* 1 = 0.0849408 loss)
I1209 11:19:52.293519 11864 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I1209 11:19:52.293947 11864 solver.cpp:228] Iteration 4, loss = 0.100447
I1209 11:19:52.293968 11864 solver.cpp:244]     Train net output #0: loss = 0.100447 (* 1 = 0.100447 loss)
I1209 11:19:52.293978 11864 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:19:52.294380 11864 solver.cpp:228] Iteration 5, loss = 0.0782987
I1209 11:19:52.294401 11864 solver.cpp:244]     Train net output #0: loss = 0.0782987 (* 1 = 0.0782987 loss)
I1209 11:19:52.294411 11864 sgd_solver.cpp:106] Iteration 5, lr = 0.01
I1209 11:19:52.294803 11864 solver.cpp:228] Iteration 6, loss = 0.0668709
I1209 11:19:52.294823 11864 solver.cpp:244]     Train net output #0: loss = 0.0668709 (* 1 = 0.0668709 loss)
I1209 11:19:52.294833 11864 sgd_solver.cpp:106] Iteration 6, lr = 0.01
I1209 11:19:52.295238 11864 solver.cpp:228] Iteration 7, loss = 0.0747669
I1209 11:19:52.295258 11864 solver.cpp:244]     Train net output #0: loss = 0.0747669 (* 1 = 0.0747669 loss)
I1209 11:19:52.295267 11864 sgd_solver.cpp:106] Iteration 7, lr = 0.01
I1209 11:19:52.295677 11864 solver.cpp:228] Iteration 8, loss = 0.0530129
I1209 11:19:52.295698 11864 solver.cpp:244]     Train net output #0: loss = 0.0530129 (* 1 = 0.0530129 loss)
I1209 11:19:52.295707 11864 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:19:52.296119 11864 solver.cpp:228] Iteration 9, loss = 0.0451137
I1209 11:19:52.296139 11864 solver.cpp:244]     Train net output #0: loss = 0.0451137 (* 1 = 0.0451137 loss)
I1209 11:19:52.296149 11864 sgd_solver.cpp:106] Iteration 9, lr = 0.01
I1209 11:19:52.296226 11864 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_10.caffemodel
I1209 11:19:52.297116 11864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_10.solverstate
I1209 11:19:52.297617 11864 solver.cpp:337] Iteration 10, Testing net (#0)
I1209 11:19:52.297632 11864 net.cpp:693] Ignoring source layer train_data
I1209 11:19:52.297637 11864 net.cpp:693] Ignoring source layer train_label
I1209 11:19:52.298233 11864 solver.cpp:404]     Test net output #0: loss = 0.0477516 (* 1 = 0.0477516 loss)
I1209 11:19:52.298578 11864 solver.cpp:228] Iteration 10, loss = 0.0569305
I1209 11:19:52.298599 11864 solver.cpp:244]     Train net output #0: loss = 0.0569305 (* 1 = 0.0569305 loss)
I1209 11:19:52.298612 11864 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1209 11:19:52.299026 11864 solver.cpp:228] Iteration 11, loss = 0.0515115
I1209 11:19:52.299047 11864 solver.cpp:244]     Train net output #0: loss = 0.0515115 (* 1 = 0.0515115 loss)
I1209 11:19:52.299057 11864 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1209 11:19:52.299450 11864 solver.cpp:228] Iteration 12, loss = 0.0572844
I1209 11:19:52.299471 11864 solver.cpp:244]     Train net output #0: loss = 0.0572844 (* 1 = 0.0572844 loss)
I1209 11:19:52.299480 11864 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I1209 11:19:52.299885 11864 solver.cpp:228] Iteration 13, loss = 0.0385391
I1209 11:19:52.299906 11864 solver.cpp:244]     Train net output #0: loss = 0.0385391 (* 1 = 0.0385391 loss)
I1209 11:19:52.299916 11864 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1209 11:19:52.300329 11864 solver.cpp:228] Iteration 14, loss = 0.0557141
I1209 11:19:52.300350 11864 solver.cpp:244]     Train net output #0: loss = 0.0557141 (* 1 = 0.0557141 loss)
I1209 11:19:52.300360 11864 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1209 11:19:52.300758 11864 solver.cpp:228] Iteration 15, loss = 0.0556994
I1209 11:19:52.300778 11864 solver.cpp:244]     Train net output #0: loss = 0.0556994 (* 1 = 0.0556994 loss)
I1209 11:19:52.300788 11864 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I1209 11:19:52.301183 11864 solver.cpp:228] Iteration 16, loss = 0.0631248
I1209 11:19:52.301204 11864 solver.cpp:244]     Train net output #0: loss = 0.0631248 (* 1 = 0.0631248 loss)
I1209 11:19:52.301213 11864 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I1209 11:19:52.301616 11864 solver.cpp:228] Iteration 17, loss = 0.0670649
I1209 11:19:52.301637 11864 solver.cpp:244]     Train net output #0: loss = 0.0670649 (* 1 = 0.0670649 loss)
I1209 11:19:52.301646 11864 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I1209 11:19:52.302055 11864 solver.cpp:228] Iteration 18, loss = 0.040826
I1209 11:19:52.302076 11864 solver.cpp:244]     Train net output #0: loss = 0.040826 (* 1 = 0.040826 loss)
I1209 11:19:52.302086 11864 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I1209 11:19:52.302470 11864 solver.cpp:228] Iteration 19, loss = 0.0252037
I1209 11:19:52.302490 11864 solver.cpp:244]     Train net output #0: loss = 0.0252037 (* 1 = 0.0252037 loss)
I1209 11:19:52.302500 11864 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I1209 11:19:52.302572 11864 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_20.caffemodel
I1209 11:19:52.303163 11864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_20.solverstate
I1209 11:19:52.303627 11864 solver.cpp:337] Iteration 20, Testing net (#0)
I1209 11:19:52.303640 11864 net.cpp:693] Ignoring source layer train_data
I1209 11:19:52.303644 11864 net.cpp:693] Ignoring source layer train_label
I1209 11:19:52.304214 11864 solver.cpp:404]     Test net output #0: loss = 0.0439129 (* 1 = 0.0439129 loss)
I1209 11:19:52.304567 11864 solver.cpp:228] Iteration 20, loss = 0.0462524
I1209 11:19:52.304589 11864 solver.cpp:244]     Train net output #0: loss = 0.0462524 (* 1 = 0.0462524 loss)
I1209 11:19:52.304599 11864 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I1209 11:19:52.305013 11864 solver.cpp:228] Iteration 21, loss = 0.0357765
I1209 11:19:52.305034 11864 solver.cpp:244]     Train net output #0: loss = 0.0357765 (* 1 = 0.0357765 loss)
I1209 11:19:52.305043 11864 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I1209 11:19:52.305460 11864 solver.cpp:228] Iteration 22, loss = 0.0431939
I1209 11:19:52.305481 11864 solver.cpp:244]     Train net output #0: loss = 0.0431939 (* 1 = 0.0431939 loss)
I1209 11:19:52.305490 11864 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I1209 11:19:52.305924 11864 solver.cpp:228] Iteration 23, loss = 0.0324976
I1209 11:19:52.305946 11864 solver.cpp:244]     Train net output #0: loss = 0.0324976 (* 1 = 0.0324976 loss)
I1209 11:19:52.305960 11864 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I1209 11:19:52.306372 11864 solver.cpp:228] Iteration 24, loss = 0.0368729
I1209 11:19:52.306393 11864 solver.cpp:244]     Train net output #0: loss = 0.0368729 (* 1 = 0.0368729 loss)
I1209 11:19:52.306402 11864 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I1209 11:19:52.306792 11864 solver.cpp:228] Iteration 25, loss = 0.0315715
I1209 11:19:52.306813 11864 solver.cpp:244]     Train net output #0: loss = 0.0315715 (* 1 = 0.0315715 loss)
I1209 11:19:52.306821 11864 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I1209 11:19:52.307209 11864 solver.cpp:228] Iteration 26, loss = 0.0329897
I1209 11:19:52.307229 11864 solver.cpp:244]     Train net output #0: loss = 0.0329897 (* 1 = 0.0329897 loss)
I1209 11:19:52.307238 11864 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I1209 11:19:52.307646 11864 solver.cpp:228] Iteration 27, loss = 0.0317242
I1209 11:19:52.307667 11864 solver.cpp:244]     Train net output #0: loss = 0.0317242 (* 1 = 0.0317242 loss)
I1209 11:19:52.307677 11864 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I1209 11:19:52.308068 11864 solver.cpp:228] Iteration 28, loss = 0.0203639
I1209 11:19:52.308089 11864 solver.cpp:244]     Train net output #0: loss = 0.0203639 (* 1 = 0.0203639 loss)
I1209 11:19:52.308099 11864 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I1209 11:19:52.308487 11864 solver.cpp:228] Iteration 29, loss = 0.0156842
I1209 11:19:52.308508 11864 solver.cpp:244]     Train net output #0: loss = 0.0156842 (* 1 = 0.0156842 loss)
I1209 11:19:52.308517 11864 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I1209 11:19:52.308589 11864 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_30.caffemodel
I1209 11:19:52.309162 11864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_30.solverstate
I1209 11:19:52.309924 11864 solver.cpp:317] Iteration 30, loss = 0.021972
I1209 11:19:52.309944 11864 solver.cpp:337] Iteration 30, Testing net (#0)
I1209 11:19:52.309950 11864 net.cpp:693] Ignoring source layer train_data
I1209 11:19:52.309954 11864 net.cpp:693] Ignoring source layer train_label
I1209 11:19:52.310547 11864 solver.cpp:404]     Test net output #0: loss = 0.0231852 (* 1 = 0.0231852 loss)
I1209 11:19:52.310564 11864 solver.cpp:322] Optimization Done.
I1209 11:19:52.310569 11864 caffe.cpp:254] Optimization Done.
