salloc: Granted job allocation 4016423
srun: Job step created
I1209 11:22:49.945632 13240 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112247-4aa4/solver.prototxt
I1209 11:22:49.947232 13240 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:22:49.947243 13240 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:22:49.948277 13240 caffe.cpp:217] Using GPUs 0
I1209 11:22:49.993716 13240 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:22:50.417026 13240 solver.cpp:48] Initializing solver from parameters:
test_iter: 10
test_interval: 90
base_lr: 0.01
display: 11
max_iter: 270
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 90
snapshot: 90
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
level: 0
stage: ""
}
type: "SGD"
I1209 11:22:50.418556 13240 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:22:50.419535 13240 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 11:22:50.419553 13240 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I1209 11:22:50.419574 13240 net.cpp:58] Initializing net from parameters:
state {
phase: TRAIN
level: 0
stage: ""
}
layer {
name: "data"
type: "Data"
top: "data"
include {
phase: TRAIN
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112241-5a96/train_db/features"
batch_size: 1
backend: LMDB
}
}
layer {
name: "label"
type: "Data"
top: "label"
include {
phase: TRAIN
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112241-5a96/train_db/labels"
batch_size: 1
backend: LMDB
}
}
layer {
name: "identity"
type: "Power"
bottom: "data"
top: "output"
}
layer {
name: "loss"
type: "EuclideanLoss"
bottom: "output"
bottom: "label"
top: "loss"
}
I1209 11:22:50.419765 13240 layer_factory.hpp:77] Creating layer data
I1209 11:22:50.419935 13240 net.cpp:100] Creating Layer data
I1209 11:22:50.419955 13240 net.cpp:408] data -> data
I1209 11:22:50.423882 13245 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112241-5a96/train_db/features
I1209 11:22:50.449182 13240 data_layer.cpp:41] output data size: 1,1,26,10
I1209 11:22:50.450656 13240 net.cpp:150] Setting up data
I1209 11:22:50.450678 13240 net.cpp:157] Top shape: 1 1 26 10 (260)
I1209 11:22:50.450724 13240 net.cpp:165] Memory required for data: 1040
I1209 11:22:50.450763 13240 layer_factory.hpp:77] Creating layer label
I1209 11:22:50.450851 13240 net.cpp:100] Creating Layer label
I1209 11:22:50.450880 13240 net.cpp:408] label -> label
I1209 11:22:50.455234 13247 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112241-5a96/train_db/labels
I1209 11:22:50.455430 13240 data_layer.cpp:41] output data size: 1,1,26,10
I1209 11:22:50.455971 13240 net.cpp:150] Setting up label
I1209 11:22:50.455986 13240 net.cpp:157] Top shape: 1 1 26 10 (260)
I1209 11:22:50.455996 13240 net.cpp:165] Memory required for data: 2080
I1209 11:22:50.456003 13240 layer_factory.hpp:77] Creating layer identity
I1209 11:22:50.456025 13240 net.cpp:100] Creating Layer identity
I1209 11:22:50.456035 13240 net.cpp:434] identity <- data
I1209 11:22:50.456055 13240 net.cpp:408] identity -> output
I1209 11:22:50.456117 13240 net.cpp:150] Setting up identity
I1209 11:22:50.456128 13240 net.cpp:157] Top shape: 1 1 26 10 (260)
I1209 11:22:50.456136 13240 net.cpp:165] Memory required for data: 3120
I1209 11:22:50.456142 13240 layer_factory.hpp:77] Creating layer loss
I1209 11:22:50.456152 13240 net.cpp:100] Creating Layer loss
I1209 11:22:50.456158 13240 net.cpp:434] loss <- output
I1209 11:22:50.456166 13240 net.cpp:434] loss <- label
I1209 11:22:50.456176 13240 net.cpp:408] loss -> loss
I1209 11:22:50.456255 13240 net.cpp:150] Setting up loss
I1209 11:22:50.456265 13240 net.cpp:157] Top shape: (1)
I1209 11:22:50.456274 13240 net.cpp:160]     with loss weight 1
I1209 11:22:50.456313 13240 net.cpp:165] Memory required for data: 3124
I1209 11:22:50.456326 13240 net.cpp:228] loss does not need backward computation.
I1209 11:22:50.456334 13240 net.cpp:228] identity does not need backward computation.
I1209 11:22:50.456339 13240 net.cpp:228] label does not need backward computation.
I1209 11:22:50.456344 13240 net.cpp:228] data does not need backward computation.
I1209 11:22:50.456349 13240 net.cpp:270] This network produces output loss
I1209 11:22:50.456359 13240 net.cpp:283] Network initialization done.
I1209 11:22:50.456765 13240 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:22:50.456802 13240 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 11:22:50.456810 13240 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer label
I1209 11:22:50.456820 13240 net.cpp:58] Initializing net from parameters:
state {
phase: TEST
}
layer {
name: "data"
type: "Data"
top: "data"
include {
phase: TEST
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112241-5a96/val_db/features"
batch_size: 1
backend: LMDB
}
}
layer {
name: "label"
type: "Data"
top: "label"
include {
phase: TEST
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112241-5a96/val_db/labels"
batch_size: 1
backend: LMDB
}
}
layer {
name: "identity"
type: "Power"
bottom: "data"
top: "output"
}
layer {
name: "loss"
type: "EuclideanLoss"
bottom: "output"
bottom: "label"
top: "loss"
}
I1209 11:22:50.456926 13240 layer_factory.hpp:77] Creating layer data
I1209 11:22:50.457017 13240 net.cpp:100] Creating Layer data
I1209 11:22:50.457033 13240 net.cpp:408] data -> data
I1209 11:22:50.460021 13249 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112241-5a96/val_db/features
I1209 11:22:50.460253 13240 data_layer.cpp:41] output data size: 1,1,16,28
I1209 11:22:50.460880 13240 net.cpp:150] Setting up data
I1209 11:22:50.460894 13240 net.cpp:157] Top shape: 1 1 16 28 (448)
I1209 11:22:50.460904 13240 net.cpp:165] Memory required for data: 1792
I1209 11:22:50.460911 13240 layer_factory.hpp:77] Creating layer label
I1209 11:22:50.460978 13240 net.cpp:100] Creating Layer label
I1209 11:22:50.460994 13240 net.cpp:408] label -> label
I1209 11:22:50.464299 13251 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112241-5a96/val_db/labels
I1209 11:22:50.464485 13240 data_layer.cpp:41] output data size: 1,1,16,28
I1209 11:22:50.464946 13240 net.cpp:150] Setting up label
I1209 11:22:50.464962 13240 net.cpp:157] Top shape: 1 1 16 28 (448)
I1209 11:22:50.464972 13240 net.cpp:165] Memory required for data: 3584
I1209 11:22:50.464982 13240 layer_factory.hpp:77] Creating layer identity
I1209 11:22:50.464993 13240 net.cpp:100] Creating Layer identity
I1209 11:22:50.464999 13240 net.cpp:434] identity <- data
I1209 11:22:50.465008 13240 net.cpp:408] identity -> output
I1209 11:22:50.465112 13240 net.cpp:150] Setting up identity
I1209 11:22:50.465126 13240 net.cpp:157] Top shape: 1 1 16 28 (448)
I1209 11:22:50.465152 13240 net.cpp:165] Memory required for data: 5376
I1209 11:22:50.465158 13240 layer_factory.hpp:77] Creating layer loss
I1209 11:22:50.465168 13240 net.cpp:100] Creating Layer loss
I1209 11:22:50.465174 13240 net.cpp:434] loss <- output
I1209 11:22:50.465181 13240 net.cpp:434] loss <- label
I1209 11:22:50.465189 13240 net.cpp:408] loss -> loss
I1209 11:22:50.465245 13240 net.cpp:150] Setting up loss
I1209 11:22:50.465255 13240 net.cpp:157] Top shape: (1)
I1209 11:22:50.465263 13240 net.cpp:160]     with loss weight 1
I1209 11:22:50.465272 13240 net.cpp:165] Memory required for data: 5380
I1209 11:22:50.465278 13240 net.cpp:228] loss does not need backward computation.
I1209 11:22:50.465286 13240 net.cpp:228] identity does not need backward computation.
I1209 11:22:50.465291 13240 net.cpp:228] label does not need backward computation.
I1209 11:22:50.465301 13240 net.cpp:228] data does not need backward computation.
I1209 11:22:50.465304 13240 net.cpp:270] This network produces output loss
I1209 11:22:50.465312 13240 net.cpp:283] Network initialization done.
I1209 11:22:50.465350 13240 solver.cpp:60] Solver scaffolding done.
I1209 11:22:50.465369 13240 caffe.cpp:251] Starting Optimization
I1209 11:22:50.465379 13240 solver.cpp:279] Solving
I1209 11:22:50.465384 13240 solver.cpp:280] Learning Rate Policy: step
I1209 11:22:50.465405 13240 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:22:50.465780 13240 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:22:50.471135 13240 solver.cpp:404]     Test net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.471602 13240 solver.cpp:228] Iteration 0, loss = 0
I1209 11:22:50.471626 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.471655 13240 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:22:50.475733 13240 solver.cpp:228] Iteration 11, loss = 0
I1209 11:22:50.475756 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.475766 13240 sgd_solver.cpp:106] Iteration 11, lr = 0.01
I1209 11:22:50.479408 13240 solver.cpp:228] Iteration 22, loss = 0
I1209 11:22:50.479429 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.479439 13240 sgd_solver.cpp:106] Iteration 22, lr = 0.01
I1209 11:22:50.482851 13240 solver.cpp:228] Iteration 33, loss = 0
I1209 11:22:50.482874 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.482884 13240 sgd_solver.cpp:106] Iteration 33, lr = 0.01
I1209 11:22:50.485946 13240 solver.cpp:228] Iteration 44, loss = 0
I1209 11:22:50.485967 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.485976 13240 sgd_solver.cpp:106] Iteration 44, lr = 0.01
I1209 11:22:50.488591 13240 solver.cpp:228] Iteration 55, loss = 0
I1209 11:22:50.488611 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.488620 13240 sgd_solver.cpp:106] Iteration 55, lr = 0.01
I1209 11:22:50.492056 13240 solver.cpp:228] Iteration 66, loss = 0
I1209 11:22:50.492077 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.492087 13240 sgd_solver.cpp:106] Iteration 66, lr = 0.01
I1209 11:22:50.495064 13240 solver.cpp:228] Iteration 77, loss = 0
I1209 11:22:50.495086 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.495095 13240 sgd_solver.cpp:106] Iteration 77, lr = 0.01
I1209 11:22:50.497661 13240 solver.cpp:228] Iteration 88, loss = 0
I1209 11:22:50.497681 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.497690 13240 sgd_solver.cpp:106] Iteration 88, lr = 0.01
I1209 11:22:50.497980 13240 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_90.caffemodel
I1209 11:22:50.498939 13240 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_90.solverstate
I1209 11:22:50.499583 13240 solver.cpp:337] Iteration 90, Testing net (#0)
I1209 11:22:50.502573 13240 solver.cpp:404]     Test net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.504855 13240 solver.cpp:228] Iteration 99, loss = 0
I1209 11:22:50.504875 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.504885 13240 sgd_solver.cpp:106] Iteration 99, lr = 0.001
I1209 11:22:50.507468 13240 solver.cpp:228] Iteration 110, loss = 0
I1209 11:22:50.507489 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.507499 13240 sgd_solver.cpp:106] Iteration 110, lr = 0.001
I1209 11:22:50.510066 13240 solver.cpp:228] Iteration 121, loss = 0
I1209 11:22:50.510087 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.510097 13240 sgd_solver.cpp:106] Iteration 121, lr = 0.001
I1209 11:22:50.512717 13240 solver.cpp:228] Iteration 132, loss = 0
I1209 11:22:50.512737 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.512745 13240 sgd_solver.cpp:106] Iteration 132, lr = 0.001
I1209 11:22:50.515424 13240 solver.cpp:228] Iteration 143, loss = 0
I1209 11:22:50.515444 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.515455 13240 sgd_solver.cpp:106] Iteration 143, lr = 0.001
I1209 11:22:50.518076 13240 solver.cpp:228] Iteration 154, loss = 0
I1209 11:22:50.518097 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.518106 13240 sgd_solver.cpp:106] Iteration 154, lr = 0.001
I1209 11:22:50.520756 13240 solver.cpp:228] Iteration 165, loss = 0
I1209 11:22:50.520774 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.520784 13240 sgd_solver.cpp:106] Iteration 165, lr = 0.001
I1209 11:22:50.523428 13240 solver.cpp:228] Iteration 176, loss = 0
I1209 11:22:50.523449 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.523458 13240 sgd_solver.cpp:106] Iteration 176, lr = 0.001
I1209 11:22:50.524227 13240 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_180.caffemodel
I1209 11:22:50.524906 13240 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_180.solverstate
I1209 11:22:50.525437 13240 solver.cpp:337] Iteration 180, Testing net (#0)
I1209 11:22:50.528467 13240 solver.cpp:404]     Test net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.530382 13240 solver.cpp:228] Iteration 187, loss = 0
I1209 11:22:50.530403 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.530412 13240 sgd_solver.cpp:106] Iteration 187, lr = 0.0001
I1209 11:22:50.533004 13240 solver.cpp:228] Iteration 198, loss = 0
I1209 11:22:50.533025 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.533035 13240 sgd_solver.cpp:106] Iteration 198, lr = 0.0001
I1209 11:22:50.535681 13240 solver.cpp:228] Iteration 209, loss = 0
I1209 11:22:50.535702 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.535712 13240 sgd_solver.cpp:106] Iteration 209, lr = 0.0001
I1209 11:22:50.538373 13240 solver.cpp:228] Iteration 220, loss = 0
I1209 11:22:50.538393 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.538403 13240 sgd_solver.cpp:106] Iteration 220, lr = 0.0001
I1209 11:22:50.541038 13240 solver.cpp:228] Iteration 231, loss = 0
I1209 11:22:50.541057 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.541067 13240 sgd_solver.cpp:106] Iteration 231, lr = 0.0001
I1209 11:22:50.543714 13240 solver.cpp:228] Iteration 242, loss = 0
I1209 11:22:50.543735 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.543743 13240 sgd_solver.cpp:106] Iteration 242, lr = 0.0001
I1209 11:22:50.546447 13240 solver.cpp:228] Iteration 253, loss = 0
I1209 11:22:50.546468 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.546478 13240 sgd_solver.cpp:106] Iteration 253, lr = 0.0001
I1209 11:22:50.549105 13240 solver.cpp:228] Iteration 264, loss = 0
I1209 11:22:50.549125 13240 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.549134 13240 sgd_solver.cpp:106] Iteration 264, lr = 0.0001
I1209 11:22:50.550374 13240 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_270.caffemodel
I1209 11:22:50.551213 13240 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_270.solverstate
I1209 11:22:50.552325 13240 solver.cpp:337] Iteration 270, Testing net (#0)
I1209 11:22:50.554955 13240 solver.cpp:404]     Test net output #0: loss = 0 (* 1 = 0 loss)
I1209 11:22:50.554972 13240 solver.cpp:322] Optimization Done.
I1209 11:22:50.554977 13240 caffe.cpp:254] Optimization Done.
salloc: Relinquishing job allocation 4016423
salloc: Job allocation 4016423 has been revoked.
