salloc: Granted job allocation 4016384
srun: Job step created
I1209 11:19:13.762917 11404 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-111913-a8c3/solver.prototxt
I1209 11:19:13.764329 11404 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:19:13.764339 11404 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:19:13.765274 11404 caffe.cpp:217] Using GPUs 0
I1209 11:19:13.812418 11404 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:19:14.222620 11404 solver.cpp:48] Initializing solver from parameters:
test_iter: 2
test_interval: 10
base_lr: 0.01
display: 1
max_iter: 30
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 10
snapshot: 10
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
level: 0
stage: ""
}
type: "SGD"
I1209 11:19:14.224108 11404 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:19:14.225175 11404 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_data
I1209 11:19:14.225191 11404 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_label
I1209 11:19:14.225214 11404 net.cpp:58] Initializing net from parameters:
state {
phase: TRAIN
level: 0
stage: ""
}
layer {
name: "train_data"
type: "Data"
top: "scaled_data"
include {
phase: TRAIN
}
transform_param {
scale: 0.004
mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
batch_size: 10
backend: LMDB
}
}
layer {
name: "train_label"
type: "Data"
top: "label"
include {
phase: TRAIN
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
batch_size: 10
backend: LMDB
}
}
layer {
name: "hidden"
type: "InnerProduct"
bottom: "scaled_data"
top: "output"
inner_product_param {
num_output: 2
}
}
layer {
name: "loss"
type: "EuclideanLoss"
bottom: "output"
bottom: "label"
top: "loss"
}
I1209 11:19:14.225422 11404 layer_factory.hpp:77] Creating layer train_data
I1209 11:19:14.225600 11404 net.cpp:100] Creating Layer train_data
I1209 11:19:14.225639 11404 net.cpp:408] train_data -> scaled_data
I1209 11:19:14.225703 11404 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:19:14.228456 11473 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:19:14.246768 11404 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:19:14.248360 11404 net.cpp:150] Setting up train_data
I1209 11:19:14.248396 11404 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:19:14.248430 11404 net.cpp:165] Memory required for data: 4000
I1209 11:19:14.248455 11404 layer_factory.hpp:77] Creating layer train_label
I1209 11:19:14.248554 11404 net.cpp:100] Creating Layer train_label
I1209 11:19:14.248574 11404 net.cpp:408] train_label -> label
I1209 11:19:14.252841 11475 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:19:14.253029 11404 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:19:14.253463 11404 net.cpp:150] Setting up train_label
I1209 11:19:14.253478 11404 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:19:14.253497 11404 net.cpp:165] Memory required for data: 4080
I1209 11:19:14.253504 11404 layer_factory.hpp:77] Creating layer hidden
I1209 11:19:14.253523 11404 net.cpp:100] Creating Layer hidden
I1209 11:19:14.253533 11404 net.cpp:434] hidden <- scaled_data
I1209 11:19:14.253556 11404 net.cpp:408] hidden -> output
I1209 11:19:14.253806 11404 net.cpp:150] Setting up hidden
I1209 11:19:14.253820 11404 net.cpp:157] Top shape: 10 2 (20)
I1209 11:19:14.253839 11404 net.cpp:165] Memory required for data: 4160
I1209 11:19:14.253870 11404 layer_factory.hpp:77] Creating layer loss
I1209 11:19:14.253883 11404 net.cpp:100] Creating Layer loss
I1209 11:19:14.253890 11404 net.cpp:434] loss <- output
I1209 11:19:14.253904 11404 net.cpp:434] loss <- label
I1209 11:19:14.253913 11404 net.cpp:408] loss -> loss
I1209 11:19:14.253976 11404 net.cpp:150] Setting up loss
I1209 11:19:14.253986 11404 net.cpp:157] Top shape: (1)
I1209 11:19:14.253994 11404 net.cpp:160]     with loss weight 1
I1209 11:19:14.254026 11404 net.cpp:165] Memory required for data: 4164
I1209 11:19:14.254035 11404 net.cpp:226] loss needs backward computation.
I1209 11:19:14.254043 11404 net.cpp:226] hidden needs backward computation.
I1209 11:19:14.254048 11404 net.cpp:228] train_label does not need backward computation.
I1209 11:19:14.254055 11404 net.cpp:228] train_data does not need backward computation.
I1209 11:19:14.254060 11404 net.cpp:270] This network produces output loss
I1209 11:19:14.254068 11404 net.cpp:283] Network initialization done.
I1209 11:19:14.254508 11404 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:19:14.254545 11404 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_data
I1209 11:19:14.254552 11404 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_label
I1209 11:19:14.254564 11404 net.cpp:58] Initializing net from parameters:
state {
phase: TEST
}
layer {
name: "val_data"
type: "Data"
top: "scaled_data"
include {
phase: TEST
}
transform_param {
scale: 0.004
mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
batch_size: 10
backend: LMDB
}
}
layer {
name: "val_label"
type: "Data"
top: "label"
include {
phase: TEST
}
data_param {
source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
batch_size: 10
backend: LMDB
}
}
layer {
name: "hidden"
type: "InnerProduct"
bottom: "scaled_data"
top: "output"
inner_product_param {
num_output: 2
}
}
layer {
name: "loss"
type: "EuclideanLoss"
bottom: "output"
bottom: "label"
top: "loss"
}
I1209 11:19:14.254698 11404 layer_factory.hpp:77] Creating layer val_data
I1209 11:19:14.254792 11404 net.cpp:100] Creating Layer val_data
I1209 11:19:14.254808 11404 net.cpp:408] val_data -> scaled_data
I1209 11:19:14.254824 11404 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:19:14.257550 11477 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:19:14.257727 11404 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:19:14.258266 11404 net.cpp:150] Setting up val_data
I1209 11:19:14.258288 11404 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:19:14.258301 11404 net.cpp:165] Memory required for data: 4000
I1209 11:19:14.258307 11404 layer_factory.hpp:77] Creating layer val_label
I1209 11:19:14.258396 11404 net.cpp:100] Creating Layer val_label
I1209 11:19:14.258411 11404 net.cpp:408] val_label -> label
I1209 11:19:14.261607 11479 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:19:14.261795 11404 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:19:14.262307 11404 net.cpp:150] Setting up val_label
I1209 11:19:14.262322 11404 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:19:14.262332 11404 net.cpp:165] Memory required for data: 4080
I1209 11:19:14.262341 11404 layer_factory.hpp:77] Creating layer hidden
I1209 11:19:14.262353 11404 net.cpp:100] Creating Layer hidden
I1209 11:19:14.262359 11404 net.cpp:434] hidden <- scaled_data
I1209 11:19:14.262372 11404 net.cpp:408] hidden -> output
I1209 11:19:14.262595 11404 net.cpp:150] Setting up hidden
I1209 11:19:14.262609 11404 net.cpp:157] Top shape: 10 2 (20)
I1209 11:19:14.262619 11404 net.cpp:165] Memory required for data: 4160
I1209 11:19:14.262635 11404 layer_factory.hpp:77] Creating layer loss
I1209 11:19:14.262645 11404 net.cpp:100] Creating Layer loss
I1209 11:19:14.262655 11404 net.cpp:434] loss <- output
I1209 11:19:14.262661 11404 net.cpp:434] loss <- label
I1209 11:19:14.262672 11404 net.cpp:408] loss -> loss
I1209 11:19:14.262725 11404 net.cpp:150] Setting up loss
I1209 11:19:14.262735 11404 net.cpp:157] Top shape: (1)
I1209 11:19:14.262743 11404 net.cpp:160]     with loss weight 1
I1209 11:19:14.262753 11404 net.cpp:165] Memory required for data: 4164
I1209 11:19:14.262759 11404 net.cpp:226] loss needs backward computation.
I1209 11:19:14.262765 11404 net.cpp:226] hidden needs backward computation.
I1209 11:19:14.262771 11404 net.cpp:228] val_label does not need backward computation.
I1209 11:19:14.262776 11404 net.cpp:228] val_data does not need backward computation.
I1209 11:19:14.262780 11404 net.cpp:270] This network produces output loss
I1209 11:19:14.262789 11404 net.cpp:283] Network initialization done.
I1209 11:19:14.262821 11404 solver.cpp:60] Solver scaffolding done.
I1209 11:19:14.262934 11404 caffe.cpp:251] Starting Optimization
I1209 11:19:14.262948 11404 solver.cpp:279] Solving
I1209 11:19:14.262953 11404 solver.cpp:280] Learning Rate Policy: step
I1209 11:19:14.263088 11404 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:19:14.263104 11404 net.cpp:693] Ignoring source layer train_data
I1209 11:19:14.263109 11404 net.cpp:693] Ignoring source layer train_label
I1209 11:19:14.263258 11404 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:19:14.264601 11404 solver.cpp:404]     Test net output #0: loss = 0.0933352 (* 1 = 0.0933352 loss)
I1209 11:19:14.281620 11404 solver.cpp:228] Iteration 0, loss = 0.0855033
I1209 11:19:14.281659 11404 solver.cpp:244]     Train net output #0: loss = 0.0855033 (* 1 = 0.0855033 loss)
I1209 11:19:14.281687 11404 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:19:14.282418 11404 solver.cpp:228] Iteration 1, loss = 0.0785537
I1209 11:19:14.282449 11404 solver.cpp:244]     Train net output #0: loss = 0.0785537 (* 1 = 0.0785537 loss)
I1209 11:19:14.282460 11404 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I1209 11:19:14.282881 11404 solver.cpp:228] Iteration 2, loss = 0.0945713
I1209 11:19:14.282904 11404 solver.cpp:244]     Train net output #0: loss = 0.0945713 (* 1 = 0.0945713 loss)
I1209 11:19:14.282914 11404 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I1209 11:19:14.283325 11404 solver.cpp:228] Iteration 3, loss = 0.0646118
I1209 11:19:14.283347 11404 solver.cpp:244]     Train net output #0: loss = 0.0646118 (* 1 = 0.0646118 loss)
I1209 11:19:14.283357 11404 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I1209 11:19:14.283805 11404 solver.cpp:228] Iteration 4, loss = 0.0857749
I1209 11:19:14.283828 11404 solver.cpp:244]     Train net output #0: loss = 0.0857749 (* 1 = 0.0857749 loss)
I1209 11:19:14.283836 11404 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:19:14.284246 11404 solver.cpp:228] Iteration 5, loss = 0.0755682
I1209 11:19:14.284267 11404 solver.cpp:244]     Train net output #0: loss = 0.0755682 (* 1 = 0.0755682 loss)
I1209 11:19:14.284277 11404 sgd_solver.cpp:106] Iteration 5, lr = 0.01
I1209 11:19:14.284672 11404 solver.cpp:228] Iteration 6, loss = 0.0689826
I1209 11:19:14.284693 11404 solver.cpp:244]     Train net output #0: loss = 0.0689826 (* 1 = 0.0689826 loss)
I1209 11:19:14.284701 11404 sgd_solver.cpp:106] Iteration 6, lr = 0.01
I1209 11:19:14.285099 11404 solver.cpp:228] Iteration 7, loss = 0.0652205
I1209 11:19:14.285120 11404 solver.cpp:244]     Train net output #0: loss = 0.0652205 (* 1 = 0.0652205 loss)
I1209 11:19:14.285130 11404 sgd_solver.cpp:106] Iteration 7, lr = 0.01
I1209 11:19:14.285526 11404 solver.cpp:228] Iteration 8, loss = 0.0412991
I1209 11:19:14.285547 11404 solver.cpp:244]     Train net output #0: loss = 0.0412991 (* 1 = 0.0412991 loss)
I1209 11:19:14.285557 11404 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:19:14.285985 11404 solver.cpp:228] Iteration 9, loss = 0.0439358
I1209 11:19:14.286007 11404 solver.cpp:244]     Train net output #0: loss = 0.0439358 (* 1 = 0.0439358 loss)
I1209 11:19:14.286017 11404 sgd_solver.cpp:106] Iteration 9, lr = 0.01
I1209 11:19:14.286095 11404 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_10.caffemodel
I1209 11:19:14.287479 11404 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_10.solverstate
I1209 11:19:14.288110 11404 solver.cpp:337] Iteration 10, Testing net (#0)
I1209 11:19:14.288122 11404 net.cpp:693] Ignoring source layer train_data
I1209 11:19:14.288127 11404 net.cpp:693] Ignoring source layer train_label
I1209 11:19:14.288866 11404 solver.cpp:404]     Test net output #0: loss = 0.0467744 (* 1 = 0.0467744 loss)
I1209 11:19:14.289222 11404 solver.cpp:228] Iteration 10, loss = 0.0422129
I1209 11:19:14.289243 11404 solver.cpp:244]     Train net output #0: loss = 0.0422129 (* 1 = 0.0422129 loss)
I1209 11:19:14.289258 11404 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1209 11:19:14.289656 11404 solver.cpp:228] Iteration 11, loss = 0.0339011
I1209 11:19:14.289677 11404 solver.cpp:244]     Train net output #0: loss = 0.0339011 (* 1 = 0.0339011 loss)
I1209 11:19:14.289686 11404 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1209 11:19:14.290098 11404 solver.cpp:228] Iteration 12, loss = 0.0384588
I1209 11:19:14.290120 11404 solver.cpp:244]     Train net output #0: loss = 0.0384588 (* 1 = 0.0384588 loss)
I1209 11:19:14.290130 11404 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I1209 11:19:14.290542 11404 solver.cpp:228] Iteration 13, loss = 0.024136
I1209 11:19:14.290565 11404 solver.cpp:244]     Train net output #0: loss = 0.024136 (* 1 = 0.024136 loss)
I1209 11:19:14.290573 11404 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1209 11:19:14.290989 11404 solver.cpp:228] Iteration 14, loss = 0.0322204
I1209 11:19:14.291010 11404 solver.cpp:244]     Train net output #0: loss = 0.0322204 (* 1 = 0.0322204 loss)
I1209 11:19:14.291019 11404 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1209 11:19:14.291404 11404 solver.cpp:228] Iteration 15, loss = 0.0258991
I1209 11:19:14.291426 11404 solver.cpp:244]     Train net output #0: loss = 0.0258991 (* 1 = 0.0258991 loss)
I1209 11:19:14.291436 11404 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I1209 11:19:14.291856 11404 solver.cpp:228] Iteration 16, loss = 0.0231253
I1209 11:19:14.291877 11404 solver.cpp:244]     Train net output #0: loss = 0.0231253 (* 1 = 0.0231253 loss)
I1209 11:19:14.291887 11404 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I1209 11:19:14.292292 11404 solver.cpp:228] Iteration 17, loss = 0.0232261
I1209 11:19:14.292313 11404 solver.cpp:244]     Train net output #0: loss = 0.0232261 (* 1 = 0.0232261 loss)
I1209 11:19:14.292323 11404 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I1209 11:19:14.292718 11404 solver.cpp:228] Iteration 18, loss = 0.0154988
I1209 11:19:14.292739 11404 solver.cpp:244]     Train net output #0: loss = 0.0154988 (* 1 = 0.0154988 loss)
I1209 11:19:14.292748 11404 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I1209 11:19:14.293123 11404 solver.cpp:228] Iteration 19, loss = 0.0166041
I1209 11:19:14.293144 11404 solver.cpp:244]     Train net output #0: loss = 0.0166041 (* 1 = 0.0166041 loss)
I1209 11:19:14.293154 11404 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I1209 11:19:14.293226 11404 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_20.caffemodel
I1209 11:19:14.294055 11404 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_20.solverstate
I1209 11:19:14.294698 11404 solver.cpp:337] Iteration 20, Testing net (#0)
I1209 11:19:14.294711 11404 net.cpp:693] Ignoring source layer train_data
I1209 11:19:14.294716 11404 net.cpp:693] Ignoring source layer train_label
I1209 11:19:14.295317 11404 solver.cpp:404]     Test net output #0: loss = 0.017894 (* 1 = 0.017894 loss)
I1209 11:19:14.295694 11404 solver.cpp:228] Iteration 20, loss = 0.0163972
I1209 11:19:14.295716 11404 solver.cpp:244]     Train net output #0: loss = 0.0163972 (* 1 = 0.0163972 loss)
I1209 11:19:14.295725 11404 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I1209 11:19:14.296152 11404 solver.cpp:228] Iteration 21, loss = 0.0135673
I1209 11:19:14.296174 11404 solver.cpp:244]     Train net output #0: loss = 0.0135673 (* 1 = 0.0135673 loss)
I1209 11:19:14.296182 11404 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I1209 11:19:14.296586 11404 solver.cpp:228] Iteration 22, loss = 0.0159421
I1209 11:19:14.296607 11404 solver.cpp:244]     Train net output #0: loss = 0.0159421 (* 1 = 0.0159421 loss)
I1209 11:19:14.296617 11404 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I1209 11:19:14.297036 11404 solver.cpp:228] Iteration 23, loss = 0.0101313
I1209 11:19:14.297058 11404 solver.cpp:244]     Train net output #0: loss = 0.0101313 (* 1 = 0.0101313 loss)
I1209 11:19:14.297067 11404 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I1209 11:19:14.297487 11404 solver.cpp:228] Iteration 24, loss = 0.0146747
I1209 11:19:14.297508 11404 solver.cpp:244]     Train net output #0: loss = 0.0146747 (* 1 = 0.0146747 loss)
I1209 11:19:14.297518 11404 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I1209 11:19:14.297924 11404 solver.cpp:228] Iteration 25, loss = 0.0118844
I1209 11:19:14.297946 11404 solver.cpp:244]     Train net output #0: loss = 0.0118844 (* 1 = 0.0118844 loss)
I1209 11:19:14.297956 11404 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I1209 11:19:14.298357 11404 solver.cpp:228] Iteration 26, loss = 0.0110399
I1209 11:19:14.298377 11404 solver.cpp:244]     Train net output #0: loss = 0.0110399 (* 1 = 0.0110399 loss)
I1209 11:19:14.298387 11404 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I1209 11:19:14.298796 11404 solver.cpp:228] Iteration 27, loss = 0.0120574
I1209 11:19:14.298817 11404 solver.cpp:244]     Train net output #0: loss = 0.0120574 (* 1 = 0.0120574 loss)
I1209 11:19:14.298826 11404 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I1209 11:19:14.299222 11404 solver.cpp:228] Iteration 28, loss = 0.00854655
I1209 11:19:14.299243 11404 solver.cpp:244]     Train net output #0: loss = 0.00854655 (* 1 = 0.00854655 loss)
I1209 11:19:14.299253 11404 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I1209 11:19:14.299660 11404 solver.cpp:228] Iteration 29, loss = 0.0093963
I1209 11:19:14.299681 11404 solver.cpp:244]     Train net output #0: loss = 0.0093963 (* 1 = 0.0093963 loss)
I1209 11:19:14.299691 11404 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I1209 11:19:14.299763 11404 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_30.caffemodel
I1209 11:19:14.300767 11404 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_30.solverstate
I1209 11:19:14.302111 11404 solver.cpp:317] Iteration 30, loss = 0.00964794
I1209 11:19:14.302130 11404 solver.cpp:337] Iteration 30, Testing net (#0)
I1209 11:19:14.302136 11404 net.cpp:693] Ignoring source layer train_data
I1209 11:19:14.302140 11404 net.cpp:693] Ignoring source layer train_label
I1209 11:19:14.302763 11404 solver.cpp:404]     Test net output #0: loss = 0.0103879 (* 1 = 0.0103879 loss)
I1209 11:19:14.302779 11404 solver.cpp:322] Optimization Done.
I1209 11:19:14.302785 11404 caffe.cpp:254] Optimization Done.
salloc: Relinquishing job allocation 4016384
