Log file created at: 2016/12/09 11:26:22
Running on machine: g011
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:26:22.268276 13396 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112619-8e57/solver.prototxt
I1209 11:26:22.269202 13396 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:26:22.269213 13396 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:26:22.269860 13396 caffe.cpp:217] Using GPUs 0
I1209 11:26:22.317961 13396 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:26:22.723763 13396 solver.cpp:48] Initializing solver from parameters: 
test_iter: 3
test_interval: 13
base_lr: 0.01
display: 1
max_iter: 39
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 13
snapshot: 13
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:26:22.724701 13396 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:26:22.725435 13396 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 11:26:22.725456 13396 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I1209 11:26:22.725479 13396 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
    batch_size: 8
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
    batch_size: 8
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:26:22.725706 13396 layer_factory.hpp:77] Creating layer data
I1209 11:26:22.725869 13396 net.cpp:100] Creating Layer data
I1209 11:26:22.725899 13396 net.cpp:408] data -> data
I1209 11:26:22.725961 13396 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:26:22.727933 13404 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:26:22.749256 13396 data_layer.cpp:41] output data size: 8,1,10,10
I1209 11:26:22.750705 13396 net.cpp:150] Setting up data
I1209 11:26:22.750726 13396 net.cpp:157] Top shape: 8 1 10 10 (800)
I1209 11:26:22.750824 13396 net.cpp:165] Memory required for data: 3200
I1209 11:26:22.750876 13396 layer_factory.hpp:77] Creating layer label
I1209 11:26:22.750970 13396 net.cpp:100] Creating Layer label
I1209 11:26:22.750993 13396 net.cpp:408] label -> label
I1209 11:26:22.754743 13406 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:26:22.754912 13396 data_layer.cpp:41] output data size: 8,1,1,2
I1209 11:26:22.755156 13396 net.cpp:150] Setting up label
I1209 11:26:22.755170 13396 net.cpp:157] Top shape: 8 1 1 2 (16)
I1209 11:26:22.755188 13396 net.cpp:165] Memory required for data: 3264
I1209 11:26:22.755195 13396 layer_factory.hpp:77] Creating layer scale
I1209 11:26:22.755213 13396 net.cpp:100] Creating Layer scale
I1209 11:26:22.755233 13396 net.cpp:434] scale <- data
I1209 11:26:22.755256 13396 net.cpp:408] scale -> scale
I1209 11:26:22.755339 13396 net.cpp:150] Setting up scale
I1209 11:26:22.755347 13396 net.cpp:157] Top shape: 8 1 10 10 (800)
I1209 11:26:22.755354 13396 net.cpp:165] Memory required for data: 6464
I1209 11:26:22.755359 13396 layer_factory.hpp:77] Creating layer hidden
I1209 11:26:22.755391 13396 net.cpp:100] Creating Layer hidden
I1209 11:26:22.755398 13396 net.cpp:434] hidden <- scale
I1209 11:26:22.755410 13396 net.cpp:408] hidden -> output
I1209 11:26:22.755635 13396 net.cpp:150] Setting up hidden
I1209 11:26:22.755646 13396 net.cpp:157] Top shape: 8 2 (16)
I1209 11:26:22.755656 13396 net.cpp:165] Memory required for data: 6528
I1209 11:26:22.755682 13396 layer_factory.hpp:77] Creating layer loss
I1209 11:26:22.755713 13396 net.cpp:100] Creating Layer loss
I1209 11:26:22.755718 13396 net.cpp:434] loss <- output
I1209 11:26:22.755724 13396 net.cpp:434] loss <- label
I1209 11:26:22.755733 13396 net.cpp:408] loss -> loss
I1209 11:26:22.755797 13396 net.cpp:150] Setting up loss
I1209 11:26:22.755820 13396 net.cpp:157] Top shape: (1)
I1209 11:26:22.755827 13396 net.cpp:160]     with loss weight 1
I1209 11:26:22.755857 13396 net.cpp:165] Memory required for data: 6532
I1209 11:26:22.755866 13396 net.cpp:226] loss needs backward computation.
I1209 11:26:22.755892 13396 net.cpp:226] hidden needs backward computation.
I1209 11:26:22.755897 13396 net.cpp:228] scale does not need backward computation.
I1209 11:26:22.755903 13396 net.cpp:228] label does not need backward computation.
I1209 11:26:22.755905 13396 net.cpp:228] data does not need backward computation.
I1209 11:26:22.755909 13396 net.cpp:270] This network produces output loss
I1209 11:26:22.755918 13396 net.cpp:283] Network initialization done.
I1209 11:26:22.756448 13396 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:26:22.756491 13396 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 11:26:22.756500 13396 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer label
I1209 11:26:22.756523 13396 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
    batch_size: 8
    backend: LMDB
  }
}
layer {
  name: "label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
    batch_size: 8
    backend: LMDB
  }
}
layer {
  name: "scale"
  type: "Power"
  bottom: "data"
  top: "scale"
  power_param {
    scale: 0.004
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scale"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:26:22.756660 13396 layer_factory.hpp:77] Creating layer data
I1209 11:26:22.756772 13396 net.cpp:100] Creating Layer data
I1209 11:26:22.756799 13396 net.cpp:408] data -> data
I1209 11:26:22.756816 13396 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:26:22.758886 13408 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:26:22.759104 13396 data_layer.cpp:41] output data size: 8,1,10,10
I1209 11:26:22.759348 13396 net.cpp:150] Setting up data
I1209 11:26:22.759362 13396 net.cpp:157] Top shape: 8 1 10 10 (800)
I1209 11:26:22.759373 13396 net.cpp:165] Memory required for data: 3200
I1209 11:26:22.759382 13396 layer_factory.hpp:77] Creating layer label
I1209 11:26:22.759492 13396 net.cpp:100] Creating Layer label
I1209 11:26:22.759523 13396 net.cpp:408] label -> label
I1209 11:26:22.762401 13410 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:26:22.762570 13396 data_layer.cpp:41] output data size: 8,1,1,2
I1209 11:26:22.762809 13396 net.cpp:150] Setting up label
I1209 11:26:22.762825 13396 net.cpp:157] Top shape: 8 1 1 2 (16)
I1209 11:26:22.762836 13396 net.cpp:165] Memory required for data: 3264
I1209 11:26:22.762854 13396 layer_factory.hpp:77] Creating layer scale
I1209 11:26:22.762866 13396 net.cpp:100] Creating Layer scale
I1209 11:26:22.762873 13396 net.cpp:434] scale <- data
I1209 11:26:22.762887 13396 net.cpp:408] scale -> scale
I1209 11:26:22.762955 13396 net.cpp:150] Setting up scale
I1209 11:26:22.762966 13396 net.cpp:157] Top shape: 8 1 10 10 (800)
I1209 11:26:22.762975 13396 net.cpp:165] Memory required for data: 6464
I1209 11:26:22.762981 13396 layer_factory.hpp:77] Creating layer hidden
I1209 11:26:22.763005 13396 net.cpp:100] Creating Layer hidden
I1209 11:26:22.763010 13396 net.cpp:434] hidden <- scale
I1209 11:26:22.763016 13396 net.cpp:408] hidden -> output
I1209 11:26:22.763154 13396 net.cpp:150] Setting up hidden
I1209 11:26:22.763164 13396 net.cpp:157] Top shape: 8 2 (16)
I1209 11:26:22.763173 13396 net.cpp:165] Memory required for data: 6528
I1209 11:26:22.763190 13396 layer_factory.hpp:77] Creating layer loss
I1209 11:26:22.763211 13396 net.cpp:100] Creating Layer loss
I1209 11:26:22.763216 13396 net.cpp:434] loss <- output
I1209 11:26:22.763221 13396 net.cpp:434] loss <- label
I1209 11:26:22.763227 13396 net.cpp:408] loss -> loss
I1209 11:26:22.763278 13396 net.cpp:150] Setting up loss
I1209 11:26:22.763286 13396 net.cpp:157] Top shape: (1)
I1209 11:26:22.763293 13396 net.cpp:160]     with loss weight 1
I1209 11:26:22.763300 13396 net.cpp:165] Memory required for data: 6532
I1209 11:26:22.763305 13396 net.cpp:226] loss needs backward computation.
I1209 11:26:22.763310 13396 net.cpp:226] hidden needs backward computation.
I1209 11:26:22.763314 13396 net.cpp:228] scale does not need backward computation.
I1209 11:26:22.763319 13396 net.cpp:228] label does not need backward computation.
I1209 11:26:22.763324 13396 net.cpp:228] data does not need backward computation.
I1209 11:26:22.763326 13396 net.cpp:270] This network produces output loss
I1209 11:26:22.763334 13396 net.cpp:283] Network initialization done.
I1209 11:26:22.763362 13396 solver.cpp:60] Solver scaffolding done.
I1209 11:26:22.763499 13396 caffe.cpp:251] Starting Optimization
I1209 11:26:22.763511 13396 solver.cpp:279] Solving 
I1209 11:26:22.763516 13396 solver.cpp:280] Learning Rate Policy: step
I1209 11:26:22.763656 13396 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:26:22.763818 13396 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 11:26:22.765519 13396 solver.cpp:404]     Test net output #0: loss = 0.091347 (* 1 = 0.091347 loss)
I1209 11:26:22.766072 13396 solver.cpp:228] Iteration 0, loss = 0.0860582
I1209 11:26:22.766090 13396 solver.cpp:244]     Train net output #0: loss = 0.0860582 (* 1 = 0.0860582 loss)
I1209 11:26:22.766113 13396 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:26:22.766558 13396 solver.cpp:228] Iteration 1, loss = 0.0806937
I1209 11:26:22.766582 13396 solver.cpp:244]     Train net output #0: loss = 0.0806937 (* 1 = 0.0806937 loss)
I1209 11:26:22.766590 13396 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I1209 11:26:22.766963 13396 solver.cpp:228] Iteration 2, loss = 0.0655285
I1209 11:26:22.766981 13396 solver.cpp:244]     Train net output #0: loss = 0.0655285 (* 1 = 0.0655285 loss)
I1209 11:26:22.766989 13396 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I1209 11:26:22.767359 13396 solver.cpp:228] Iteration 3, loss = 0.103372
I1209 11:26:22.767375 13396 solver.cpp:244]     Train net output #0: loss = 0.103372 (* 1 = 0.103372 loss)
I1209 11:26:22.767383 13396 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I1209 11:26:22.767751 13396 solver.cpp:228] Iteration 4, loss = 0.0604189
I1209 11:26:22.767770 13396 solver.cpp:244]     Train net output #0: loss = 0.0604189 (* 1 = 0.0604189 loss)
I1209 11:26:22.767777 13396 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:26:22.768141 13396 solver.cpp:228] Iteration 5, loss = 0.067049
I1209 11:26:22.768158 13396 solver.cpp:244]     Train net output #0: loss = 0.067049 (* 1 = 0.067049 loss)
I1209 11:26:22.768165 13396 sgd_solver.cpp:106] Iteration 5, lr = 0.01
I1209 11:26:22.768523 13396 solver.cpp:228] Iteration 6, loss = 0.087293
I1209 11:26:22.768542 13396 solver.cpp:244]     Train net output #0: loss = 0.087293 (* 1 = 0.087293 loss)
I1209 11:26:22.768548 13396 sgd_solver.cpp:106] Iteration 6, lr = 0.01
I1209 11:26:22.768915 13396 solver.cpp:228] Iteration 7, loss = 0.0630942
I1209 11:26:22.768932 13396 solver.cpp:244]     Train net output #0: loss = 0.0630942 (* 1 = 0.0630942 loss)
I1209 11:26:22.768940 13396 sgd_solver.cpp:106] Iteration 7, lr = 0.01
I1209 11:26:22.769306 13396 solver.cpp:228] Iteration 8, loss = 0.0561201
I1209 11:26:22.769323 13396 solver.cpp:244]     Train net output #0: loss = 0.0561201 (* 1 = 0.0561201 loss)
I1209 11:26:22.769331 13396 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:26:22.769703 13396 solver.cpp:228] Iteration 9, loss = 0.0533348
I1209 11:26:22.769721 13396 solver.cpp:244]     Train net output #0: loss = 0.0533348 (* 1 = 0.0533348 loss)
I1209 11:26:22.769728 13396 sgd_solver.cpp:106] Iteration 9, lr = 0.01
I1209 11:26:22.770092 13396 solver.cpp:228] Iteration 10, loss = 0.0335617
I1209 11:26:22.770110 13396 solver.cpp:244]     Train net output #0: loss = 0.0335617 (* 1 = 0.0335617 loss)
I1209 11:26:22.770117 13396 sgd_solver.cpp:106] Iteration 10, lr = 0.01
I1209 11:26:22.770479 13396 solver.cpp:228] Iteration 11, loss = 0.0302733
I1209 11:26:22.770498 13396 solver.cpp:244]     Train net output #0: loss = 0.0302733 (* 1 = 0.0302733 loss)
I1209 11:26:22.770504 13396 sgd_solver.cpp:106] Iteration 11, lr = 0.01
I1209 11:26:22.770876 13396 solver.cpp:228] Iteration 12, loss = 0.0373556
I1209 11:26:22.770895 13396 solver.cpp:244]     Train net output #0: loss = 0.0373556 (* 1 = 0.0373556 loss)
I1209 11:26:22.770902 13396 sgd_solver.cpp:106] Iteration 12, lr = 0.01
I1209 11:26:22.770963 13396 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_13.caffemodel
I1209 11:26:22.771648 13396 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_13.solverstate
I1209 11:26:22.772065 13396 solver.cpp:337] Iteration 13, Testing net (#0)
I1209 11:26:22.772986 13396 solver.cpp:404]     Test net output #0: loss = 0.0299809 (* 1 = 0.0299809 loss)
I1209 11:26:22.773303 13396 solver.cpp:228] Iteration 13, loss = 0.0263213
I1209 11:26:22.773321 13396 solver.cpp:244]     Train net output #0: loss = 0.0263213 (* 1 = 0.0263213 loss)
I1209 11:26:22.773332 13396 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1209 11:26:22.773695 13396 solver.cpp:228] Iteration 14, loss = 0.020056
I1209 11:26:22.773713 13396 solver.cpp:244]     Train net output #0: loss = 0.020056 (* 1 = 0.020056 loss)
I1209 11:26:22.773720 13396 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1209 11:26:22.774085 13396 solver.cpp:228] Iteration 15, loss = 0.0238794
I1209 11:26:22.774102 13396 solver.cpp:244]     Train net output #0: loss = 0.0238794 (* 1 = 0.0238794 loss)
I1209 11:26:22.774109 13396 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I1209 11:26:22.774464 13396 solver.cpp:228] Iteration 16, loss = 0.0160555
I1209 11:26:22.774482 13396 solver.cpp:244]     Train net output #0: loss = 0.0160555 (* 1 = 0.0160555 loss)
I1209 11:26:22.774489 13396 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I1209 11:26:22.774871 13396 solver.cpp:228] Iteration 17, loss = 0.0149526
I1209 11:26:22.774889 13396 solver.cpp:244]     Train net output #0: loss = 0.0149526 (* 1 = 0.0149526 loss)
I1209 11:26:22.774897 13396 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I1209 11:26:22.775257 13396 solver.cpp:228] Iteration 18, loss = 0.0156139
I1209 11:26:22.775274 13396 solver.cpp:244]     Train net output #0: loss = 0.0156139 (* 1 = 0.0156139 loss)
I1209 11:26:22.775281 13396 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I1209 11:26:22.775645 13396 solver.cpp:228] Iteration 19, loss = 0.0116506
I1209 11:26:22.775662 13396 solver.cpp:244]     Train net output #0: loss = 0.0116506 (* 1 = 0.0116506 loss)
I1209 11:26:22.775670 13396 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I1209 11:26:22.776026 13396 solver.cpp:228] Iteration 20, loss = 0.0106087
I1209 11:26:22.776042 13396 solver.cpp:244]     Train net output #0: loss = 0.0106087 (* 1 = 0.0106087 loss)
I1209 11:26:22.776051 13396 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I1209 11:26:22.776401 13396 solver.cpp:228] Iteration 21, loss = 0.00870339
I1209 11:26:22.776422 13396 solver.cpp:244]     Train net output #0: loss = 0.00870339 (* 1 = 0.00870339 loss)
I1209 11:26:22.776430 13396 sgd_solver.cpp:106] Iteration 21, lr = 0.001
I1209 11:26:22.776785 13396 solver.cpp:228] Iteration 22, loss = 0.0110701
I1209 11:26:22.776803 13396 solver.cpp:244]     Train net output #0: loss = 0.0110701 (* 1 = 0.0110701 loss)
I1209 11:26:22.776809 13396 sgd_solver.cpp:106] Iteration 22, lr = 0.001
I1209 11:26:22.777173 13396 solver.cpp:228] Iteration 23, loss = 0.00448019
I1209 11:26:22.777190 13396 solver.cpp:244]     Train net output #0: loss = 0.00448019 (* 1 = 0.00448019 loss)
I1209 11:26:22.777197 13396 sgd_solver.cpp:106] Iteration 23, lr = 0.001
I1209 11:26:22.777565 13396 solver.cpp:228] Iteration 24, loss = 0.0072219
I1209 11:26:22.777582 13396 solver.cpp:244]     Train net output #0: loss = 0.0072219 (* 1 = 0.0072219 loss)
I1209 11:26:22.777590 13396 sgd_solver.cpp:106] Iteration 24, lr = 0.001
I1209 11:26:22.777953 13396 solver.cpp:228] Iteration 25, loss = 0.00609665
I1209 11:26:22.777971 13396 solver.cpp:244]     Train net output #0: loss = 0.00609665 (* 1 = 0.00609665 loss)
I1209 11:26:22.777978 13396 sgd_solver.cpp:106] Iteration 25, lr = 0.001
I1209 11:26:22.778036 13396 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_26.caffemodel
I1209 11:26:22.778651 13396 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_26.solverstate
I1209 11:26:22.779129 13396 solver.cpp:337] Iteration 26, Testing net (#0)
I1209 11:26:22.780012 13396 solver.cpp:404]     Test net output #0: loss = 0.00583719 (* 1 = 0.00583719 loss)
I1209 11:26:22.780331 13396 solver.cpp:228] Iteration 26, loss = 0.00495737
I1209 11:26:22.780349 13396 solver.cpp:244]     Train net output #0: loss = 0.00495737 (* 1 = 0.00495737 loss)
I1209 11:26:22.780357 13396 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I1209 11:26:22.780717 13396 solver.cpp:228] Iteration 27, loss = 0.00389185
I1209 11:26:22.780735 13396 solver.cpp:244]     Train net output #0: loss = 0.00389185 (* 1 = 0.00389185 loss)
I1209 11:26:22.780742 13396 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I1209 11:26:22.781096 13396 solver.cpp:228] Iteration 28, loss = 0.00507683
I1209 11:26:22.781113 13396 solver.cpp:244]     Train net output #0: loss = 0.00507683 (* 1 = 0.00507683 loss)
I1209 11:26:22.781121 13396 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I1209 11:26:22.781478 13396 solver.cpp:228] Iteration 29, loss = 0.00295438
I1209 11:26:22.781496 13396 solver.cpp:244]     Train net output #0: loss = 0.00295438 (* 1 = 0.00295438 loss)
I1209 11:26:22.781502 13396 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I1209 11:26:22.781869 13396 solver.cpp:228] Iteration 30, loss = 0.00424832
I1209 11:26:22.781888 13396 solver.cpp:244]     Train net output #0: loss = 0.00424832 (* 1 = 0.00424832 loss)
I1209 11:26:22.781894 13396 sgd_solver.cpp:106] Iteration 30, lr = 0.0001
I1209 11:26:22.782253 13396 solver.cpp:228] Iteration 31, loss = 0.00379888
I1209 11:26:22.782269 13396 solver.cpp:244]     Train net output #0: loss = 0.00379888 (* 1 = 0.00379888 loss)
I1209 11:26:22.782276 13396 sgd_solver.cpp:106] Iteration 31, lr = 0.0001
I1209 11:26:22.782655 13396 solver.cpp:228] Iteration 32, loss = 0.00304394
I1209 11:26:22.782671 13396 solver.cpp:244]     Train net output #0: loss = 0.00304394 (* 1 = 0.00304394 loss)
I1209 11:26:22.782680 13396 sgd_solver.cpp:106] Iteration 32, lr = 0.0001
I1209 11:26:22.783046 13396 solver.cpp:228] Iteration 33, loss = 0.00245558
I1209 11:26:22.783062 13396 solver.cpp:244]     Train net output #0: loss = 0.00245558 (* 1 = 0.00245558 loss)
I1209 11:26:22.783069 13396 sgd_solver.cpp:106] Iteration 33, lr = 0.0001
I1209 11:26:22.783439 13396 solver.cpp:228] Iteration 34, loss = 0.00330667
I1209 11:26:22.783457 13396 solver.cpp:244]     Train net output #0: loss = 0.00330667 (* 1 = 0.00330667 loss)
I1209 11:26:22.783463 13396 sgd_solver.cpp:106] Iteration 34, lr = 0.0001
I1209 11:26:22.783831 13396 solver.cpp:228] Iteration 35, loss = 0.00250884
I1209 11:26:22.783849 13396 solver.cpp:244]     Train net output #0: loss = 0.00250884 (* 1 = 0.00250884 loss)
I1209 11:26:22.783860 13396 sgd_solver.cpp:106] Iteration 35, lr = 0.0001
I1209 11:26:22.784245 13396 solver.cpp:228] Iteration 36, loss = 0.00237303
I1209 11:26:22.784262 13396 solver.cpp:244]     Train net output #0: loss = 0.00237303 (* 1 = 0.00237303 loss)
I1209 11:26:22.784271 13396 sgd_solver.cpp:106] Iteration 36, lr = 0.0001
I1209 11:26:22.784632 13396 solver.cpp:228] Iteration 37, loss = 0.00296588
I1209 11:26:22.784649 13396 solver.cpp:244]     Train net output #0: loss = 0.00296588 (* 1 = 0.00296588 loss)
I1209 11:26:22.784657 13396 sgd_solver.cpp:106] Iteration 37, lr = 0.0001
I1209 11:26:22.785020 13396 solver.cpp:228] Iteration 38, loss = 0.00219076
I1209 11:26:22.785037 13396 solver.cpp:244]     Train net output #0: loss = 0.00219076 (* 1 = 0.00219076 loss)
I1209 11:26:22.785044 13396 sgd_solver.cpp:106] Iteration 38, lr = 0.0001
I1209 11:26:22.785102 13396 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_39.caffemodel
I1209 11:26:22.785670 13396 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_39.solverstate
I1209 11:26:22.786334 13396 solver.cpp:317] Iteration 39, loss = 0.00176182
I1209 11:26:22.786350 13396 solver.cpp:337] Iteration 39, Testing net (#0)
I1209 11:26:22.787173 13396 solver.cpp:404]     Test net output #0: loss = 0.00255805 (* 1 = 0.00255805 loss)
I1209 11:26:22.787186 13396 solver.cpp:322] Optimization Done.
I1209 11:26:22.787190 13396 caffe.cpp:254] Optimization Done.
