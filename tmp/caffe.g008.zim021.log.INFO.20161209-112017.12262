Log file created at: 2016/12/09 11:20:17
Running on machine: g008
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 11:20:17.565426 12262 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112014-9cec/solver.prototxt
I1209 11:20:17.566617 12262 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 11:20:17.566627 12262 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 11:20:17.567422 12262 caffe.cpp:217] Using GPUs 0
I1209 11:20:17.616103 12262 caffe.cpp:222] GPU 0: Tesla K20m
I1209 11:20:18.042299 12262 solver.cpp:48] Initializing solver from parameters: 
test_iter: 2
test_interval: 10
base_lr: 0.01
display: 1
max_iter: 30
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 10
snapshot: 10
snapshot_prefix: "snapshot"
solver_mode: GPU
device_id: 0
random_seed: 3405691582
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1209 11:20:18.043603 12262 solver.cpp:91] Creating training net from net file: train_val.prototxt
I1209 11:20:18.044601 12262 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_data
I1209 11:20:18.044617 12262 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer val_label
I1209 11:20:18.044638 12262 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "train_data"
  type: "Data"
  top: "scaled_data"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.004
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "train_label"
  type: "Data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scaled_data"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:20:18.044847 12262 layer_factory.hpp:77] Creating layer train_data
I1209 11:20:18.045022 12262 net.cpp:100] Creating Layer train_data
I1209 11:20:18.045040 12262 net.cpp:408] train_data -> scaled_data
I1209 11:20:18.045092 12262 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:20:18.047354 12270 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_images
I1209 11:20:18.069345 12262 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:20:18.070904 12262 net.cpp:150] Setting up train_data
I1209 11:20:18.070943 12262 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:20:18.070977 12262 net.cpp:165] Memory required for data: 4000
I1209 11:20:18.071013 12262 layer_factory.hpp:77] Creating layer train_label
I1209 11:20:18.071099 12262 net.cpp:100] Creating Layer train_label
I1209 11:20:18.071115 12262 net.cpp:408] train_label -> label
I1209 11:20:18.074728 12272 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_labels
I1209 11:20:18.074931 12262 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:20:18.075403 12262 net.cpp:150] Setting up train_label
I1209 11:20:18.075419 12262 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:20:18.075430 12262 net.cpp:165] Memory required for data: 4080
I1209 11:20:18.075438 12262 layer_factory.hpp:77] Creating layer hidden
I1209 11:20:18.075459 12262 net.cpp:100] Creating Layer hidden
I1209 11:20:18.075469 12262 net.cpp:434] hidden <- scaled_data
I1209 11:20:18.075489 12262 net.cpp:408] hidden -> output
I1209 11:20:18.075696 12262 net.cpp:150] Setting up hidden
I1209 11:20:18.075708 12262 net.cpp:157] Top shape: 10 2 (20)
I1209 11:20:18.075717 12262 net.cpp:165] Memory required for data: 4160
I1209 11:20:18.075748 12262 layer_factory.hpp:77] Creating layer loss
I1209 11:20:18.075765 12262 net.cpp:100] Creating Layer loss
I1209 11:20:18.075772 12262 net.cpp:434] loss <- output
I1209 11:20:18.075781 12262 net.cpp:434] loss <- label
I1209 11:20:18.075791 12262 net.cpp:408] loss -> loss
I1209 11:20:18.075848 12262 net.cpp:150] Setting up loss
I1209 11:20:18.075858 12262 net.cpp:157] Top shape: (1)
I1209 11:20:18.075871 12262 net.cpp:160]     with loss weight 1
I1209 11:20:18.075904 12262 net.cpp:165] Memory required for data: 4164
I1209 11:20:18.075913 12262 net.cpp:226] loss needs backward computation.
I1209 11:20:18.075922 12262 net.cpp:226] hidden needs backward computation.
I1209 11:20:18.075927 12262 net.cpp:228] train_label does not need backward computation.
I1209 11:20:18.075932 12262 net.cpp:228] train_data does not need backward computation.
I1209 11:20:18.075937 12262 net.cpp:270] This network produces output loss
I1209 11:20:18.075947 12262 net.cpp:283] Network initialization done.
I1209 11:20:18.076488 12262 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I1209 11:20:18.076527 12262 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_data
I1209 11:20:18.076536 12262 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer train_label
I1209 11:20:18.076547 12262 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "val_data"
  type: "Data"
  top: "scaled_data"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.004
    mean_file: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto"
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "val_label"
  type: "Data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "hidden"
  type: "InnerProduct"
  bottom: "scaled_data"
  top: "output"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "output"
  bottom: "label"
  top: "loss"
}
I1209 11:20:18.076685 12262 layer_factory.hpp:77] Creating layer val_data
I1209 11:20:18.076786 12262 net.cpp:100] Creating Layer val_data
I1209 11:20:18.076818 12262 net.cpp:408] val_data -> scaled_data
I1209 11:20:18.076836 12262 data_transformer.cpp:25] Loading mean file from: /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/train_mean.binaryproto
I1209 11:20:18.079236 12274 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_images
I1209 11:20:18.079452 12262 data_layer.cpp:41] output data size: 10,1,10,10
I1209 11:20:18.079891 12262 net.cpp:150] Setting up val_data
I1209 11:20:18.079906 12262 net.cpp:157] Top shape: 10 1 10 10 (1000)
I1209 11:20:18.079917 12262 net.cpp:165] Memory required for data: 4000
I1209 11:20:18.079926 12262 layer_factory.hpp:77] Creating layer val_label
I1209 11:20:18.079999 12262 net.cpp:100] Creating Layer val_label
I1209 11:20:18.080015 12262 net.cpp:408] val_label -> label
I1209 11:20:18.082815 12276 db_lmdb.cpp:35] Opened lmdb /data/zim021/digits-proj/DIGITS/tmp/tmpc2ZVI7/val_labels
I1209 11:20:18.083000 12262 data_layer.cpp:41] output data size: 10,1,1,2
I1209 11:20:18.083511 12262 net.cpp:150] Setting up val_label
I1209 11:20:18.083526 12262 net.cpp:157] Top shape: 10 1 1 2 (20)
I1209 11:20:18.083537 12262 net.cpp:165] Memory required for data: 4080
I1209 11:20:18.083546 12262 layer_factory.hpp:77] Creating layer hidden
I1209 11:20:18.083562 12262 net.cpp:100] Creating Layer hidden
I1209 11:20:18.083570 12262 net.cpp:434] hidden <- scaled_data
I1209 11:20:18.083580 12262 net.cpp:408] hidden -> output
I1209 11:20:18.083796 12262 net.cpp:150] Setting up hidden
I1209 11:20:18.083811 12262 net.cpp:157] Top shape: 10 2 (20)
I1209 11:20:18.083823 12262 net.cpp:165] Memory required for data: 4160
I1209 11:20:18.083844 12262 layer_factory.hpp:77] Creating layer loss
I1209 11:20:18.083855 12262 net.cpp:100] Creating Layer loss
I1209 11:20:18.083860 12262 net.cpp:434] loss <- output
I1209 11:20:18.083868 12262 net.cpp:434] loss <- label
I1209 11:20:18.083879 12262 net.cpp:408] loss -> loss
I1209 11:20:18.083931 12262 net.cpp:150] Setting up loss
I1209 11:20:18.083941 12262 net.cpp:157] Top shape: (1)
I1209 11:20:18.083950 12262 net.cpp:160]     with loss weight 1
I1209 11:20:18.083959 12262 net.cpp:165] Memory required for data: 4164
I1209 11:20:18.083966 12262 net.cpp:226] loss needs backward computation.
I1209 11:20:18.083971 12262 net.cpp:226] hidden needs backward computation.
I1209 11:20:18.083978 12262 net.cpp:228] val_label does not need backward computation.
I1209 11:20:18.083983 12262 net.cpp:228] val_data does not need backward computation.
I1209 11:20:18.083987 12262 net.cpp:270] This network produces output loss
I1209 11:20:18.083995 12262 net.cpp:283] Network initialization done.
I1209 11:20:18.084028 12262 solver.cpp:60] Solver scaffolding done.
I1209 11:20:18.084153 12262 caffe.cpp:155] Finetuning from /data/zim021/digits-proj/DIGITS/tmp/tmpyzXVAu/20161209-112009-1755/snapshot_iter_30.caffemodel
I1209 11:20:18.084852 12262 net.cpp:761] Ignoring source layer train_data
I1209 11:20:18.084870 12262 net.cpp:761] Ignoring source layer train_label
I1209 11:20:18.084890 12262 caffe.cpp:251] Starting Optimization
I1209 11:20:18.084903 12262 solver.cpp:279] Solving 
I1209 11:20:18.084908 12262 solver.cpp:280] Learning Rate Policy: step
I1209 11:20:18.085355 12262 solver.cpp:337] Iteration 0, Testing net (#0)
I1209 11:20:18.085369 12262 net.cpp:693] Ignoring source layer train_data
I1209 11:20:18.085376 12262 net.cpp:693] Ignoring source layer train_label
I1209 11:20:18.086447 12262 solver.cpp:404]     Test net output #0: loss = 0.0103879 (* 1 = 0.0103879 loss)
I1209 11:20:18.087093 12262 solver.cpp:228] Iteration 0, loss = 0.00964794
I1209 11:20:18.087121 12262 solver.cpp:244]     Train net output #0: loss = 0.00964794 (* 1 = 0.00964794 loss)
I1209 11:20:18.087147 12262 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 11:20:18.087620 12262 solver.cpp:228] Iteration 1, loss = 0.00840117
I1209 11:20:18.087647 12262 solver.cpp:244]     Train net output #0: loss = 0.00840117 (* 1 = 0.00840117 loss)
I1209 11:20:18.087659 12262 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I1209 11:20:18.088043 12262 solver.cpp:228] Iteration 2, loss = 0.010222
I1209 11:20:18.088064 12262 solver.cpp:244]     Train net output #0: loss = 0.010222 (* 1 = 0.010222 loss)
I1209 11:20:18.088074 12262 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I1209 11:20:18.088459 12262 solver.cpp:228] Iteration 3, loss = 0.00659591
I1209 11:20:18.088480 12262 solver.cpp:244]     Train net output #0: loss = 0.00659591 (* 1 = 0.00659591 loss)
I1209 11:20:18.088490 12262 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I1209 11:20:18.088876 12262 solver.cpp:228] Iteration 4, loss = 0.00981481
I1209 11:20:18.088896 12262 solver.cpp:244]     Train net output #0: loss = 0.00981481 (* 1 = 0.00981481 loss)
I1209 11:20:18.088906 12262 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I1209 11:20:18.089289 12262 solver.cpp:228] Iteration 5, loss = 0.00769824
I1209 11:20:18.089310 12262 solver.cpp:244]     Train net output #0: loss = 0.00769824 (* 1 = 0.00769824 loss)
I1209 11:20:18.089319 12262 sgd_solver.cpp:106] Iteration 5, lr = 0.01
I1209 11:20:18.089700 12262 solver.cpp:228] Iteration 6, loss = 0.0069255
I1209 11:20:18.089721 12262 solver.cpp:244]     Train net output #0: loss = 0.0069255 (* 1 = 0.0069255 loss)
I1209 11:20:18.089731 12262 sgd_solver.cpp:106] Iteration 6, lr = 0.01
I1209 11:20:18.090128 12262 solver.cpp:228] Iteration 7, loss = 0.00741194
I1209 11:20:18.090150 12262 solver.cpp:244]     Train net output #0: loss = 0.00741194 (* 1 = 0.00741194 loss)
I1209 11:20:18.090160 12262 sgd_solver.cpp:106] Iteration 7, lr = 0.01
I1209 11:20:18.090543 12262 solver.cpp:228] Iteration 8, loss = 0.00524921
I1209 11:20:18.090564 12262 solver.cpp:244]     Train net output #0: loss = 0.00524921 (* 1 = 0.00524921 loss)
I1209 11:20:18.090574 12262 sgd_solver.cpp:106] Iteration 8, lr = 0.01
I1209 11:20:18.090957 12262 solver.cpp:228] Iteration 9, loss = 0.00565211
I1209 11:20:18.090978 12262 solver.cpp:244]     Train net output #0: loss = 0.00565211 (* 1 = 0.00565211 loss)
I1209 11:20:18.090996 12262 sgd_solver.cpp:106] Iteration 9, lr = 0.01
I1209 11:20:18.091073 12262 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_10.caffemodel
I1209 11:20:18.091892 12262 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_10.solverstate
I1209 11:20:18.092401 12262 solver.cpp:337] Iteration 10, Testing net (#0)
I1209 11:20:18.092414 12262 net.cpp:693] Ignoring source layer train_data
I1209 11:20:18.092419 12262 net.cpp:693] Ignoring source layer train_label
I1209 11:20:18.093140 12262 solver.cpp:404]     Test net output #0: loss = 0.00531192 (* 1 = 0.00531192 loss)
I1209 11:20:18.093497 12262 solver.cpp:228] Iteration 10, loss = 0.00485462
I1209 11:20:18.093519 12262 solver.cpp:244]     Train net output #0: loss = 0.00485462 (* 1 = 0.00485462 loss)
I1209 11:20:18.093533 12262 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1209 11:20:18.093936 12262 solver.cpp:228] Iteration 11, loss = 0.00370071
I1209 11:20:18.093957 12262 solver.cpp:244]     Train net output #0: loss = 0.00370071 (* 1 = 0.00370071 loss)
I1209 11:20:18.093968 12262 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1209 11:20:18.094372 12262 solver.cpp:228] Iteration 12, loss = 0.00424462
I1209 11:20:18.094391 12262 solver.cpp:244]     Train net output #0: loss = 0.00424462 (* 1 = 0.00424462 loss)
I1209 11:20:18.094401 12262 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I1209 11:20:18.094816 12262 solver.cpp:228] Iteration 13, loss = 0.00251463
I1209 11:20:18.094836 12262 solver.cpp:244]     Train net output #0: loss = 0.00251463 (* 1 = 0.00251463 loss)
I1209 11:20:18.094846 12262 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1209 11:20:18.095252 12262 solver.cpp:228] Iteration 14, loss = 0.00376641
I1209 11:20:18.095273 12262 solver.cpp:244]     Train net output #0: loss = 0.00376641 (* 1 = 0.00376641 loss)
I1209 11:20:18.095283 12262 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1209 11:20:18.095698 12262 solver.cpp:228] Iteration 15, loss = 0.00269244
I1209 11:20:18.095719 12262 solver.cpp:244]     Train net output #0: loss = 0.00269244 (* 1 = 0.00269244 loss)
I1209 11:20:18.095729 12262 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I1209 11:20:18.096145 12262 solver.cpp:228] Iteration 16, loss = 0.00236764
I1209 11:20:18.096165 12262 solver.cpp:244]     Train net output #0: loss = 0.00236764 (* 1 = 0.00236764 loss)
I1209 11:20:18.096175 12262 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I1209 11:20:18.096583 12262 solver.cpp:228] Iteration 17, loss = 0.00268838
I1209 11:20:18.096603 12262 solver.cpp:244]     Train net output #0: loss = 0.00268838 (* 1 = 0.00268838 loss)
I1209 11:20:18.096613 12262 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I1209 11:20:18.097025 12262 solver.cpp:228] Iteration 18, loss = 0.00197448
I1209 11:20:18.097045 12262 solver.cpp:244]     Train net output #0: loss = 0.00197448 (* 1 = 0.00197448 loss)
I1209 11:20:18.097055 12262 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I1209 11:20:18.097443 12262 solver.cpp:228] Iteration 19, loss = 0.00213392
I1209 11:20:18.097463 12262 solver.cpp:244]     Train net output #0: loss = 0.00213392 (* 1 = 0.00213392 loss)
I1209 11:20:18.097473 12262 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I1209 11:20:18.097545 12262 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_20.caffemodel
I1209 11:20:18.098245 12262 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_20.solverstate
I1209 11:20:18.098793 12262 solver.cpp:337] Iteration 20, Testing net (#0)
I1209 11:20:18.098806 12262 net.cpp:693] Ignoring source layer train_data
I1209 11:20:18.098811 12262 net.cpp:693] Ignoring source layer train_label
I1209 11:20:18.099824 12262 solver.cpp:404]     Test net output #0: loss = 0.0020513 (* 1 = 0.0020513 loss)
I1209 11:20:18.100209 12262 solver.cpp:228] Iteration 20, loss = 0.00190715
I1209 11:20:18.100231 12262 solver.cpp:244]     Train net output #0: loss = 0.00190715 (* 1 = 0.00190715 loss)
I1209 11:20:18.100241 12262 sgd_solver.cpp:106] Iteration 20, lr = 0.0001
I1209 11:20:18.100630 12262 solver.cpp:228] Iteration 21, loss = 0.00149779
I1209 11:20:18.100652 12262 solver.cpp:244]     Train net output #0: loss = 0.00149779 (* 1 = 0.00149779 loss)
I1209 11:20:18.100662 12262 sgd_solver.cpp:106] Iteration 21, lr = 0.0001
I1209 11:20:18.101043 12262 solver.cpp:228] Iteration 22, loss = 0.00178292
I1209 11:20:18.101065 12262 solver.cpp:244]     Train net output #0: loss = 0.00178292 (* 1 = 0.00178292 loss)
I1209 11:20:18.101075 12262 sgd_solver.cpp:106] Iteration 22, lr = 0.0001
I1209 11:20:18.101452 12262 solver.cpp:228] Iteration 23, loss = 0.00106488
I1209 11:20:18.101474 12262 solver.cpp:244]     Train net output #0: loss = 0.00106488 (* 1 = 0.00106488 loss)
I1209 11:20:18.101483 12262 sgd_solver.cpp:106] Iteration 23, lr = 0.0001
I1209 11:20:18.101899 12262 solver.cpp:228] Iteration 24, loss = 0.00173454
I1209 11:20:18.101920 12262 solver.cpp:244]     Train net output #0: loss = 0.00173454 (* 1 = 0.00173454 loss)
I1209 11:20:18.101930 12262 sgd_solver.cpp:106] Iteration 24, lr = 0.0001
I1209 11:20:18.102327 12262 solver.cpp:228] Iteration 25, loss = 0.00125056
I1209 11:20:18.102349 12262 solver.cpp:244]     Train net output #0: loss = 0.00125056 (* 1 = 0.00125056 loss)
I1209 11:20:18.102358 12262 sgd_solver.cpp:106] Iteration 25, lr = 0.0001
I1209 11:20:18.102744 12262 solver.cpp:228] Iteration 26, loss = 0.00114636
I1209 11:20:18.102766 12262 solver.cpp:244]     Train net output #0: loss = 0.00114636 (* 1 = 0.00114636 loss)
I1209 11:20:18.102774 12262 sgd_solver.cpp:106] Iteration 26, lr = 0.0001
I1209 11:20:18.103168 12262 solver.cpp:228] Iteration 27, loss = 0.00141347
I1209 11:20:18.103188 12262 solver.cpp:244]     Train net output #0: loss = 0.00141347 (* 1 = 0.00141347 loss)
I1209 11:20:18.103199 12262 sgd_solver.cpp:106] Iteration 27, lr = 0.0001
I1209 11:20:18.103595 12262 solver.cpp:228] Iteration 28, loss = 0.00108869
I1209 11:20:18.103615 12262 solver.cpp:244]     Train net output #0: loss = 0.00108869 (* 1 = 0.00108869 loss)
I1209 11:20:18.103624 12262 sgd_solver.cpp:106] Iteration 28, lr = 0.0001
I1209 11:20:18.104027 12262 solver.cpp:228] Iteration 29, loss = 0.00120544
I1209 11:20:18.104048 12262 solver.cpp:244]     Train net output #0: loss = 0.00120544 (* 1 = 0.00120544 loss)
I1209 11:20:18.104058 12262 sgd_solver.cpp:106] Iteration 29, lr = 0.0001
I1209 11:20:18.104130 12262 solver.cpp:454] Snapshotting to binary proto file snapshot_iter_30.caffemodel
I1209 11:20:18.104827 12262 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshot_iter_30.solverstate
I1209 11:20:18.105830 12262 solver.cpp:317] Iteration 30, loss = 0.00113113
I1209 11:20:18.105854 12262 solver.cpp:337] Iteration 30, Testing net (#0)
I1209 11:20:18.105859 12262 net.cpp:693] Ignoring source layer train_data
I1209 11:20:18.105865 12262 net.cpp:693] Ignoring source layer train_label
I1209 11:20:18.106472 12262 solver.cpp:404]     Test net output #0: loss = 0.00119825 (* 1 = 0.00119825 loss)
I1209 11:20:18.106488 12262 solver.cpp:322] Optimization Done.
I1209 11:20:18.106494 12262 caffe.cpp:254] Optimization Done.
